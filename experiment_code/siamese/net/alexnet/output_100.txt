I0318 05:16:41.372972  3501 caffe.cpp:185] Using GPUs 0
I0318 05:16:41.382088  3501 caffe.cpp:190] GPU 0: GRID K520
I0318 05:16:41.526863  3501 solver.cpp:48] Initializing solver from parameters: 
test_iter: 250
test_interval: 250
base_lr: 0.01
display: 20
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "./caffe_alexnet_train"
solver_mode: GPU
device_id: 0
net: "./siamese_alexnet_train.prototxt"
I0318 05:16:41.527045  3501 solver.cpp:91] Creating training net from net file: ./siamese_alexnet_train.prototxt
I0318 05:16:41.528584  3501 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0318 05:16:41.528648  3501 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer siamese_accuracy
I0318 05:16:41.529060  3501 net.cpp:49] Initializing net from parameters: 
name: "SiameseAlexNet"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/ubuntu/fyp/data/data-fyp/siamese_train_leveldb"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 3
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    name: "conv3_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    name: "conv4_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    name: "conv5_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    name: "fc6_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    name: "fc7_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "fc8"
  bottom: "fc8_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 100
  }
}
I0318 05:16:41.529305  3501 layer_factory.hpp:77] Creating layer pair_data
I0318 05:16:41.529836  3501 net.cpp:91] Creating Layer pair_data
I0318 05:16:41.529860  3501 net.cpp:399] pair_data -> pair_data
I0318 05:16:41.529932  3501 net.cpp:399] pair_data -> sim
I0318 05:16:41.537737  3505 db_leveldb.cpp:18] Opened leveldb /home/ubuntu/fyp/data/data-fyp/siamese_train_leveldb
I0318 05:16:41.551177  3501 data_layer.cpp:41] output data size: 64,6,227,227
I0318 05:16:41.707916  3501 net.cpp:141] Setting up pair_data
I0318 05:16:41.708022  3501 net.cpp:148] Top shape: 64 6 227 227 (19787136)
I0318 05:16:41.708039  3501 net.cpp:148] Top shape: 64 (64)
I0318 05:16:41.708043  3501 net.cpp:156] Memory required for data: 79148800
I0318 05:16:41.708061  3501 layer_factory.hpp:77] Creating layer slice_pair
I0318 05:16:41.708091  3501 net.cpp:91] Creating Layer slice_pair
I0318 05:16:41.708107  3501 net.cpp:425] slice_pair <- pair_data
I0318 05:16:41.708128  3501 net.cpp:399] slice_pair -> data
I0318 05:16:41.708150  3501 net.cpp:399] slice_pair -> data_p
I0318 05:16:41.708703  3501 net.cpp:141] Setting up slice_pair
I0318 05:16:41.708721  3501 net.cpp:148] Top shape: 64 3 227 227 (9893568)
I0318 05:16:41.708729  3501 net.cpp:148] Top shape: 64 3 227 227 (9893568)
I0318 05:16:41.708734  3501 net.cpp:156] Memory required for data: 158297344
I0318 05:16:41.708739  3501 layer_factory.hpp:77] Creating layer conv1
I0318 05:16:41.708771  3501 net.cpp:91] Creating Layer conv1
I0318 05:16:41.708784  3501 net.cpp:425] conv1 <- data
I0318 05:16:41.708822  3501 net.cpp:399] conv1 -> conv1
I0318 05:16:41.954499  3501 net.cpp:141] Setting up conv1
I0318 05:16:41.954555  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:41.954568  3501 net.cpp:156] Memory required for data: 232639744
I0318 05:16:41.954610  3501 layer_factory.hpp:77] Creating layer relu1
I0318 05:16:41.954638  3501 net.cpp:91] Creating Layer relu1
I0318 05:16:41.954651  3501 net.cpp:425] relu1 <- conv1
I0318 05:16:41.954669  3501 net.cpp:386] relu1 -> conv1 (in-place)
I0318 05:16:41.954874  3501 net.cpp:141] Setting up relu1
I0318 05:16:41.954900  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:41.954913  3501 net.cpp:156] Memory required for data: 306982144
I0318 05:16:41.954924  3501 layer_factory.hpp:77] Creating layer norm1
I0318 05:16:41.954955  3501 net.cpp:91] Creating Layer norm1
I0318 05:16:41.954977  3501 net.cpp:425] norm1 <- conv1
I0318 05:16:41.955008  3501 net.cpp:399] norm1 -> norm1
I0318 05:16:41.955314  3501 net.cpp:141] Setting up norm1
I0318 05:16:41.955340  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:41.955351  3501 net.cpp:156] Memory required for data: 381324544
I0318 05:16:41.955363  3501 layer_factory.hpp:77] Creating layer pool1
I0318 05:16:41.955380  3501 net.cpp:91] Creating Layer pool1
I0318 05:16:41.955392  3501 net.cpp:425] pool1 <- norm1
I0318 05:16:41.955406  3501 net.cpp:399] pool1 -> pool1
I0318 05:16:41.955492  3501 net.cpp:141] Setting up pool1
I0318 05:16:41.955515  3501 net.cpp:148] Top shape: 64 96 27 27 (4478976)
I0318 05:16:41.955525  3501 net.cpp:156] Memory required for data: 399240448
I0318 05:16:41.955536  3501 layer_factory.hpp:77] Creating layer conv2
I0318 05:16:41.955565  3501 net.cpp:91] Creating Layer conv2
I0318 05:16:41.955579  3501 net.cpp:425] conv2 <- pool1
I0318 05:16:41.955596  3501 net.cpp:399] conv2 -> conv2
I0318 05:16:41.967770  3501 net.cpp:141] Setting up conv2
I0318 05:16:41.967802  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:41.967811  3501 net.cpp:156] Memory required for data: 447016192
I0318 05:16:41.967831  3501 layer_factory.hpp:77] Creating layer relu2
I0318 05:16:41.967847  3501 net.cpp:91] Creating Layer relu2
I0318 05:16:41.967859  3501 net.cpp:425] relu2 <- conv2
I0318 05:16:41.967872  3501 net.cpp:386] relu2 -> conv2 (in-place)
I0318 05:16:41.968150  3501 net.cpp:141] Setting up relu2
I0318 05:16:41.968176  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:41.968186  3501 net.cpp:156] Memory required for data: 494791936
I0318 05:16:41.968199  3501 layer_factory.hpp:77] Creating layer norm2
I0318 05:16:41.968224  3501 net.cpp:91] Creating Layer norm2
I0318 05:16:41.968235  3501 net.cpp:425] norm2 <- conv2
I0318 05:16:41.968253  3501 net.cpp:399] norm2 -> norm2
I0318 05:16:41.968566  3501 net.cpp:141] Setting up norm2
I0318 05:16:41.968595  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:41.968605  3501 net.cpp:156] Memory required for data: 542567680
I0318 05:16:41.968616  3501 layer_factory.hpp:77] Creating layer pool2
I0318 05:16:41.968636  3501 net.cpp:91] Creating Layer pool2
I0318 05:16:41.968647  3501 net.cpp:425] pool2 <- norm2
I0318 05:16:41.968662  3501 net.cpp:399] pool2 -> pool2
I0318 05:16:41.968737  3501 net.cpp:141] Setting up pool2
I0318 05:16:41.968760  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:41.968770  3501 net.cpp:156] Memory required for data: 553643264
I0318 05:16:41.968780  3501 layer_factory.hpp:77] Creating layer conv3
I0318 05:16:41.968809  3501 net.cpp:91] Creating Layer conv3
I0318 05:16:41.968828  3501 net.cpp:425] conv3 <- pool2
I0318 05:16:41.968847  3501 net.cpp:399] conv3 -> conv3
I0318 05:16:41.999155  3501 net.cpp:141] Setting up conv3
I0318 05:16:41.999181  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:41.999191  3501 net.cpp:156] Memory required for data: 570256640
I0318 05:16:41.999212  3501 layer_factory.hpp:77] Creating layer relu3
I0318 05:16:41.999231  3501 net.cpp:91] Creating Layer relu3
I0318 05:16:41.999243  3501 net.cpp:425] relu3 <- conv3
I0318 05:16:41.999280  3501 net.cpp:386] relu3 -> conv3 (in-place)
I0318 05:16:41.999558  3501 net.cpp:141] Setting up relu3
I0318 05:16:41.999584  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:41.999594  3501 net.cpp:156] Memory required for data: 586870016
I0318 05:16:41.999604  3501 layer_factory.hpp:77] Creating layer conv4
I0318 05:16:41.999631  3501 net.cpp:91] Creating Layer conv4
I0318 05:16:41.999644  3501 net.cpp:425] conv4 <- conv3
I0318 05:16:41.999665  3501 net.cpp:399] conv4 -> conv4
I0318 05:16:42.023401  3501 net.cpp:141] Setting up conv4
I0318 05:16:42.023430  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:42.023442  3501 net.cpp:156] Memory required for data: 603483392
I0318 05:16:42.023458  3501 layer_factory.hpp:77] Creating layer relu4
I0318 05:16:42.023479  3501 net.cpp:91] Creating Layer relu4
I0318 05:16:42.023490  3501 net.cpp:425] relu4 <- conv4
I0318 05:16:42.023504  3501 net.cpp:386] relu4 -> conv4 (in-place)
I0318 05:16:42.023769  3501 net.cpp:141] Setting up relu4
I0318 05:16:42.023794  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:42.023805  3501 net.cpp:156] Memory required for data: 620096768
I0318 05:16:42.023815  3501 layer_factory.hpp:77] Creating layer conv5
I0318 05:16:42.023843  3501 net.cpp:91] Creating Layer conv5
I0318 05:16:42.023854  3501 net.cpp:425] conv5 <- conv4
I0318 05:16:42.023876  3501 net.cpp:399] conv5 -> conv5
I0318 05:16:42.042989  3501 net.cpp:141] Setting up conv5
I0318 05:16:42.043015  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:42.043025  3501 net.cpp:156] Memory required for data: 631172352
I0318 05:16:42.043046  3501 layer_factory.hpp:77] Creating layer relu5
I0318 05:16:42.043062  3501 net.cpp:91] Creating Layer relu5
I0318 05:16:42.043073  3501 net.cpp:425] relu5 <- conv5
I0318 05:16:42.043092  3501 net.cpp:386] relu5 -> conv5 (in-place)
I0318 05:16:42.043283  3501 net.cpp:141] Setting up relu5
I0318 05:16:42.043308  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:42.043318  3501 net.cpp:156] Memory required for data: 642247936
I0318 05:16:42.043329  3501 layer_factory.hpp:77] Creating layer pool5
I0318 05:16:42.043351  3501 net.cpp:91] Creating Layer pool5
I0318 05:16:42.043364  3501 net.cpp:425] pool5 <- conv5
I0318 05:16:42.043378  3501 net.cpp:399] pool5 -> pool5
I0318 05:16:42.043457  3501 net.cpp:141] Setting up pool5
I0318 05:16:42.043478  3501 net.cpp:148] Top shape: 64 256 6 6 (589824)
I0318 05:16:42.043488  3501 net.cpp:156] Memory required for data: 644607232
I0318 05:16:42.043499  3501 layer_factory.hpp:77] Creating layer fc6
I0318 05:16:42.043529  3501 net.cpp:91] Creating Layer fc6
I0318 05:16:42.043547  3501 net.cpp:425] fc6 <- pool5
I0318 05:16:42.043565  3501 net.cpp:399] fc6 -> fc6
I0318 05:16:43.313637  3501 net.cpp:141] Setting up fc6
I0318 05:16:43.313694  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:43.313709  3501 net.cpp:156] Memory required for data: 645655808
I0318 05:16:43.313740  3501 layer_factory.hpp:77] Creating layer relu6
I0318 05:16:43.313779  3501 net.cpp:91] Creating Layer relu6
I0318 05:16:43.313792  3501 net.cpp:425] relu6 <- fc6
I0318 05:16:43.313810  3501 net.cpp:386] relu6 -> fc6 (in-place)
I0318 05:16:43.314272  3501 net.cpp:141] Setting up relu6
I0318 05:16:43.314297  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:43.314308  3501 net.cpp:156] Memory required for data: 646704384
I0318 05:16:43.314319  3501 layer_factory.hpp:77] Creating layer drop6
I0318 05:16:43.314347  3501 net.cpp:91] Creating Layer drop6
I0318 05:16:43.314371  3501 net.cpp:425] drop6 <- fc6
I0318 05:16:43.314388  3501 net.cpp:386] drop6 -> fc6 (in-place)
I0318 05:16:43.314479  3501 net.cpp:141] Setting up drop6
I0318 05:16:43.314502  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:43.314512  3501 net.cpp:156] Memory required for data: 647752960
I0318 05:16:43.314522  3501 layer_factory.hpp:77] Creating layer fc7
I0318 05:16:43.314548  3501 net.cpp:91] Creating Layer fc7
I0318 05:16:43.314558  3501 net.cpp:425] fc7 <- fc6
I0318 05:16:43.314613  3501 net.cpp:399] fc7 -> fc7
I0318 05:16:43.878944  3501 net.cpp:141] Setting up fc7
I0318 05:16:43.879001  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:43.879014  3501 net.cpp:156] Memory required for data: 648801536
I0318 05:16:43.879040  3501 layer_factory.hpp:77] Creating layer fc8
I0318 05:16:43.879071  3501 net.cpp:91] Creating Layer fc8
I0318 05:16:43.879084  3501 net.cpp:425] fc8 <- fc7
I0318 05:16:43.879104  3501 net.cpp:399] fc8 -> fc8
I0318 05:16:44.016692  3501 net.cpp:141] Setting up fc8
I0318 05:16:44.016751  3501 net.cpp:148] Top shape: 64 1000 (64000)
I0318 05:16:44.016762  3501 net.cpp:156] Memory required for data: 649057536
I0318 05:16:44.016788  3501 layer_factory.hpp:77] Creating layer conv1_p
I0318 05:16:44.016829  3501 net.cpp:91] Creating Layer conv1_p
I0318 05:16:44.016842  3501 net.cpp:425] conv1_p <- data_p
I0318 05:16:44.016865  3501 net.cpp:399] conv1_p -> conv1_p
I0318 05:16:44.019158  3501 net.cpp:141] Setting up conv1_p
I0318 05:16:44.019186  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:44.019196  3501 net.cpp:156] Memory required for data: 723399936
I0318 05:16:44.019217  3501 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0318 05:16:44.019232  3501 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0318 05:16:44.019243  3501 layer_factory.hpp:77] Creating layer relu1_p
I0318 05:16:44.019259  3501 net.cpp:91] Creating Layer relu1_p
I0318 05:16:44.019270  3501 net.cpp:425] relu1_p <- conv1_p
I0318 05:16:44.019289  3501 net.cpp:386] relu1_p -> conv1_p (in-place)
I0318 05:16:44.019594  3501 net.cpp:141] Setting up relu1_p
I0318 05:16:44.019619  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:44.019631  3501 net.cpp:156] Memory required for data: 797742336
I0318 05:16:44.019642  3501 layer_factory.hpp:77] Creating layer norm1_p
I0318 05:16:44.019666  3501 net.cpp:91] Creating Layer norm1_p
I0318 05:16:44.019677  3501 net.cpp:425] norm1_p <- conv1_p
I0318 05:16:44.019692  3501 net.cpp:399] norm1_p -> norm1_p
I0318 05:16:44.020011  3501 net.cpp:141] Setting up norm1_p
I0318 05:16:44.020037  3501 net.cpp:148] Top shape: 64 96 55 55 (18585600)
I0318 05:16:44.020047  3501 net.cpp:156] Memory required for data: 872084736
I0318 05:16:44.020058  3501 layer_factory.hpp:77] Creating layer pool1_p
I0318 05:16:44.020076  3501 net.cpp:91] Creating Layer pool1_p
I0318 05:16:44.020087  3501 net.cpp:425] pool1_p <- norm1_p
I0318 05:16:44.020107  3501 net.cpp:399] pool1_p -> pool1_p
I0318 05:16:44.020177  3501 net.cpp:141] Setting up pool1_p
I0318 05:16:44.020200  3501 net.cpp:148] Top shape: 64 96 27 27 (4478976)
I0318 05:16:44.020210  3501 net.cpp:156] Memory required for data: 890000640
I0318 05:16:44.020221  3501 layer_factory.hpp:77] Creating layer conv2_p
I0318 05:16:44.020251  3501 net.cpp:91] Creating Layer conv2_p
I0318 05:16:44.020267  3501 net.cpp:425] conv2_p <- pool1_p
I0318 05:16:44.020285  3501 net.cpp:399] conv2_p -> conv2_p
I0318 05:16:44.032173  3501 net.cpp:141] Setting up conv2_p
I0318 05:16:44.032202  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:44.032212  3501 net.cpp:156] Memory required for data: 937776384
I0318 05:16:44.032224  3501 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0318 05:16:44.032238  3501 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0318 05:16:44.032248  3501 layer_factory.hpp:77] Creating layer relu2_p
I0318 05:16:44.032266  3501 net.cpp:91] Creating Layer relu2_p
I0318 05:16:44.032277  3501 net.cpp:425] relu2_p <- conv2_p
I0318 05:16:44.032291  3501 net.cpp:386] relu2_p -> conv2_p (in-place)
I0318 05:16:44.032588  3501 net.cpp:141] Setting up relu2_p
I0318 05:16:44.032614  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:44.032624  3501 net.cpp:156] Memory required for data: 985552128
I0318 05:16:44.032635  3501 layer_factory.hpp:77] Creating layer norm2_p
I0318 05:16:44.032655  3501 net.cpp:91] Creating Layer norm2_p
I0318 05:16:44.032699  3501 net.cpp:425] norm2_p <- conv2_p
I0318 05:16:44.032717  3501 net.cpp:399] norm2_p -> norm2_p
I0318 05:16:44.032953  3501 net.cpp:141] Setting up norm2_p
I0318 05:16:44.032979  3501 net.cpp:148] Top shape: 64 256 27 27 (11943936)
I0318 05:16:44.032989  3501 net.cpp:156] Memory required for data: 1033327872
I0318 05:16:44.032999  3501 layer_factory.hpp:77] Creating layer pool2_p
I0318 05:16:44.033020  3501 net.cpp:91] Creating Layer pool2_p
I0318 05:16:44.033030  3501 net.cpp:425] pool2_p <- norm2_p
I0318 05:16:44.033046  3501 net.cpp:399] pool2_p -> pool2_p
I0318 05:16:44.033125  3501 net.cpp:141] Setting up pool2_p
I0318 05:16:44.033148  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:44.033157  3501 net.cpp:156] Memory required for data: 1044403456
I0318 05:16:44.033167  3501 layer_factory.hpp:77] Creating layer conv3_p
I0318 05:16:44.033193  3501 net.cpp:91] Creating Layer conv3_p
I0318 05:16:44.033205  3501 net.cpp:425] conv3_p <- pool2_p
I0318 05:16:44.033226  3501 net.cpp:399] conv3_p -> conv3_p
I0318 05:16:44.063819  3501 net.cpp:141] Setting up conv3_p
I0318 05:16:44.063849  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:44.063860  3501 net.cpp:156] Memory required for data: 1061016832
I0318 05:16:44.063872  3501 net.cpp:484] Sharing parameters 'conv3_w' owned by layer 'conv3', param index 0
I0318 05:16:44.063886  3501 net.cpp:484] Sharing parameters 'conv3_b' owned by layer 'conv3', param index 1
I0318 05:16:44.063897  3501 layer_factory.hpp:77] Creating layer relu3_p
I0318 05:16:44.063911  3501 net.cpp:91] Creating Layer relu3_p
I0318 05:16:44.063921  3501 net.cpp:425] relu3_p <- conv3_p
I0318 05:16:44.063940  3501 net.cpp:386] relu3_p -> conv3_p (in-place)
I0318 05:16:44.064227  3501 net.cpp:141] Setting up relu3_p
I0318 05:16:44.064256  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:44.064267  3501 net.cpp:156] Memory required for data: 1077630208
I0318 05:16:44.064278  3501 layer_factory.hpp:77] Creating layer conv4_p
I0318 05:16:44.064327  3501 net.cpp:91] Creating Layer conv4_p
I0318 05:16:44.064342  3501 net.cpp:425] conv4_p <- conv3_p
I0318 05:16:44.064365  3501 net.cpp:399] conv4_p -> conv4_p
I0318 05:16:44.088129  3501 net.cpp:141] Setting up conv4_p
I0318 05:16:44.088155  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:44.088166  3501 net.cpp:156] Memory required for data: 1094243584
I0318 05:16:44.088178  3501 net.cpp:484] Sharing parameters 'conv4_w' owned by layer 'conv4', param index 0
I0318 05:16:44.088191  3501 net.cpp:484] Sharing parameters 'conv4_b' owned by layer 'conv4', param index 1
I0318 05:16:44.088202  3501 layer_factory.hpp:77] Creating layer relu4_p
I0318 05:16:44.088217  3501 net.cpp:91] Creating Layer relu4_p
I0318 05:16:44.088227  3501 net.cpp:425] relu4_p <- conv4_p
I0318 05:16:44.088245  3501 net.cpp:386] relu4_p -> conv4_p (in-place)
I0318 05:16:44.088546  3501 net.cpp:141] Setting up relu4_p
I0318 05:16:44.088575  3501 net.cpp:148] Top shape: 64 384 13 13 (4153344)
I0318 05:16:44.088585  3501 net.cpp:156] Memory required for data: 1110856960
I0318 05:16:44.088598  3501 layer_factory.hpp:77] Creating layer conv5_p
I0318 05:16:44.088624  3501 net.cpp:91] Creating Layer conv5_p
I0318 05:16:44.088644  3501 net.cpp:425] conv5_p <- conv4_p
I0318 05:16:44.088663  3501 net.cpp:399] conv5_p -> conv5_p
I0318 05:16:44.105103  3501 net.cpp:141] Setting up conv5_p
I0318 05:16:44.105129  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:44.105139  3501 net.cpp:156] Memory required for data: 1121932544
I0318 05:16:44.105151  3501 net.cpp:484] Sharing parameters 'conv5_w' owned by layer 'conv5', param index 0
I0318 05:16:44.105165  3501 net.cpp:484] Sharing parameters 'conv5_b' owned by layer 'conv5', param index 1
I0318 05:16:44.105176  3501 layer_factory.hpp:77] Creating layer relu5_p
I0318 05:16:44.105197  3501 net.cpp:91] Creating Layer relu5_p
I0318 05:16:44.105209  3501 net.cpp:425] relu5_p <- conv5_p
I0318 05:16:44.105227  3501 net.cpp:386] relu5_p -> conv5_p (in-place)
I0318 05:16:44.105518  3501 net.cpp:141] Setting up relu5_p
I0318 05:16:44.105554  3501 net.cpp:148] Top shape: 64 256 13 13 (2768896)
I0318 05:16:44.105568  3501 net.cpp:156] Memory required for data: 1133008128
I0318 05:16:44.105579  3501 layer_factory.hpp:77] Creating layer pool5_p
I0318 05:16:44.105595  3501 net.cpp:91] Creating Layer pool5_p
I0318 05:16:44.105607  3501 net.cpp:425] pool5_p <- conv5_p
I0318 05:16:44.105621  3501 net.cpp:399] pool5_p -> pool5_p
I0318 05:16:44.105705  3501 net.cpp:141] Setting up pool5_p
I0318 05:16:44.105728  3501 net.cpp:148] Top shape: 64 256 6 6 (589824)
I0318 05:16:44.105738  3501 net.cpp:156] Memory required for data: 1135367424
I0318 05:16:44.105749  3501 layer_factory.hpp:77] Creating layer fc6_p
I0318 05:16:44.105773  3501 net.cpp:91] Creating Layer fc6_p
I0318 05:16:44.105785  3501 net.cpp:425] fc6_p <- pool5_p
I0318 05:16:44.105801  3501 net.cpp:399] fc6_p -> fc6_p
I0318 05:16:45.373831  3501 net.cpp:141] Setting up fc6_p
I0318 05:16:45.373888  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:45.373899  3501 net.cpp:156] Memory required for data: 1136416000
I0318 05:16:45.373915  3501 net.cpp:484] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0318 05:16:45.373931  3501 net.cpp:484] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0318 05:16:45.373942  3501 layer_factory.hpp:77] Creating layer relu6_p
I0318 05:16:45.373965  3501 net.cpp:91] Creating Layer relu6_p
I0318 05:16:45.373978  3501 net.cpp:425] relu6_p <- fc6_p
I0318 05:16:45.373996  3501 net.cpp:386] relu6_p -> fc6_p (in-place)
I0318 05:16:45.374258  3501 net.cpp:141] Setting up relu6_p
I0318 05:16:45.374284  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:45.374295  3501 net.cpp:156] Memory required for data: 1137464576
I0318 05:16:45.374307  3501 layer_factory.hpp:77] Creating layer drop6_p
I0318 05:16:45.374325  3501 net.cpp:91] Creating Layer drop6_p
I0318 05:16:45.374336  3501 net.cpp:425] drop6_p <- fc6_p
I0318 05:16:45.374351  3501 net.cpp:386] drop6_p -> fc6_p (in-place)
I0318 05:16:45.374423  3501 net.cpp:141] Setting up drop6_p
I0318 05:16:45.374445  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:45.374455  3501 net.cpp:156] Memory required for data: 1138513152
I0318 05:16:45.374465  3501 layer_factory.hpp:77] Creating layer fc7_p
I0318 05:16:45.374492  3501 net.cpp:91] Creating Layer fc7_p
I0318 05:16:45.374505  3501 net.cpp:425] fc7_p <- fc6_p
I0318 05:16:45.374521  3501 net.cpp:399] fc7_p -> fc7_p
I0318 05:16:45.941449  3501 net.cpp:141] Setting up fc7_p
I0318 05:16:45.941509  3501 net.cpp:148] Top shape: 64 4096 (262144)
I0318 05:16:45.941520  3501 net.cpp:156] Memory required for data: 1139561728
I0318 05:16:45.941537  3501 net.cpp:484] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0318 05:16:45.941555  3501 net.cpp:484] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0318 05:16:45.941566  3501 layer_factory.hpp:77] Creating layer fc8_p
I0318 05:16:45.941601  3501 net.cpp:91] Creating Layer fc8_p
I0318 05:16:45.941614  3501 net.cpp:425] fc8_p <- fc7_p
I0318 05:16:45.941638  3501 net.cpp:399] fc8_p -> fc8_p
I0318 05:16:46.078456  3501 net.cpp:141] Setting up fc8_p
I0318 05:16:46.078501  3501 net.cpp:148] Top shape: 64 1000 (64000)
I0318 05:16:46.078513  3501 net.cpp:156] Memory required for data: 1139817728
I0318 05:16:46.078529  3501 net.cpp:484] Sharing parameters 'fc8_w' owned by layer 'fc8', param index 0
I0318 05:16:46.078543  3501 net.cpp:484] Sharing parameters 'fc8_b' owned by layer 'fc8', param index 1
I0318 05:16:46.078554  3501 layer_factory.hpp:77] Creating layer loss
I0318 05:16:46.078583  3501 net.cpp:91] Creating Layer loss
I0318 05:16:46.078598  3501 net.cpp:425] loss <- fc8
I0318 05:16:46.078613  3501 net.cpp:425] loss <- fc8_p
I0318 05:16:46.078627  3501 net.cpp:425] loss <- sim
I0318 05:16:46.078660  3501 net.cpp:399] loss -> loss
I0318 05:16:46.078856  3501 net.cpp:141] Setting up loss
I0318 05:16:46.078881  3501 net.cpp:148] Top shape: (1)
I0318 05:16:46.078891  3501 net.cpp:151]     with loss weight 1
I0318 05:16:46.078972  3501 net.cpp:156] Memory required for data: 1139817732
I0318 05:16:46.078986  3501 net.cpp:217] loss needs backward computation.
I0318 05:16:46.078999  3501 net.cpp:217] fc8_p needs backward computation.
I0318 05:16:46.079010  3501 net.cpp:219] fc7_p does not need backward computation.
I0318 05:16:46.079021  3501 net.cpp:219] drop6_p does not need backward computation.
I0318 05:16:46.079030  3501 net.cpp:219] relu6_p does not need backward computation.
I0318 05:16:46.079041  3501 net.cpp:219] fc6_p does not need backward computation.
I0318 05:16:46.079051  3501 net.cpp:219] pool5_p does not need backward computation.
I0318 05:16:46.079061  3501 net.cpp:219] relu5_p does not need backward computation.
I0318 05:16:46.079071  3501 net.cpp:219] conv5_p does not need backward computation.
I0318 05:16:46.079082  3501 net.cpp:219] relu4_p does not need backward computation.
I0318 05:16:46.079090  3501 net.cpp:219] conv4_p does not need backward computation.
I0318 05:16:46.079102  3501 net.cpp:219] relu3_p does not need backward computation.
I0318 05:16:46.079111  3501 net.cpp:219] conv3_p does not need backward computation.
I0318 05:16:46.079123  3501 net.cpp:219] pool2_p does not need backward computation.
I0318 05:16:46.079133  3501 net.cpp:219] norm2_p does not need backward computation.
I0318 05:16:46.079144  3501 net.cpp:219] relu2_p does not need backward computation.
I0318 05:16:46.079154  3501 net.cpp:219] conv2_p does not need backward computation.
I0318 05:16:46.079164  3501 net.cpp:219] pool1_p does not need backward computation.
I0318 05:16:46.079175  3501 net.cpp:219] norm1_p does not need backward computation.
I0318 05:16:46.079185  3501 net.cpp:219] relu1_p does not need backward computation.
I0318 05:16:46.079195  3501 net.cpp:219] conv1_p does not need backward computation.
I0318 05:16:46.079206  3501 net.cpp:217] fc8 needs backward computation.
I0318 05:16:46.079221  3501 net.cpp:219] fc7 does not need backward computation.
I0318 05:16:46.079231  3501 net.cpp:219] drop6 does not need backward computation.
I0318 05:16:46.079242  3501 net.cpp:219] relu6 does not need backward computation.
I0318 05:16:46.079252  3501 net.cpp:219] fc6 does not need backward computation.
I0318 05:16:46.079262  3501 net.cpp:219] pool5 does not need backward computation.
I0318 05:16:46.079272  3501 net.cpp:219] relu5 does not need backward computation.
I0318 05:16:46.079282  3501 net.cpp:219] conv5 does not need backward computation.
I0318 05:16:46.079293  3501 net.cpp:219] relu4 does not need backward computation.
I0318 05:16:46.079303  3501 net.cpp:219] conv4 does not need backward computation.
I0318 05:16:46.079313  3501 net.cpp:219] relu3 does not need backward computation.
I0318 05:16:46.079322  3501 net.cpp:219] conv3 does not need backward computation.
I0318 05:16:46.079334  3501 net.cpp:219] pool2 does not need backward computation.
I0318 05:16:46.079344  3501 net.cpp:219] norm2 does not need backward computation.
I0318 05:16:46.079355  3501 net.cpp:219] relu2 does not need backward computation.
I0318 05:16:46.079365  3501 net.cpp:219] conv2 does not need backward computation.
I0318 05:16:46.079375  3501 net.cpp:219] pool1 does not need backward computation.
I0318 05:16:46.079392  3501 net.cpp:219] norm1 does not need backward computation.
I0318 05:16:46.079403  3501 net.cpp:219] relu1 does not need backward computation.
I0318 05:16:46.079413  3501 net.cpp:219] conv1 does not need backward computation.
I0318 05:16:46.079424  3501 net.cpp:219] slice_pair does not need backward computation.
I0318 05:16:46.079435  3501 net.cpp:219] pair_data does not need backward computation.
I0318 05:16:46.079444  3501 net.cpp:261] This network produces output loss
I0318 05:16:46.116555  3501 net.cpp:274] Network initialization done.
I0318 05:16:46.118134  3501 solver.cpp:181] Creating test net (#0) specified by net file: ./siamese_alexnet_train.prototxt
I0318 05:16:46.118254  3501 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0318 05:16:46.118716  3501 net.cpp:49] Initializing net from parameters: 
name: "SiameseAlexNet"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/ubuntu/fyp/data/data-fyp/siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 3
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    name: "conv3_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    name: "conv4_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    name: "conv5_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    name: "fc6_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    name: "fc7_w"
    lr_mult: 0
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "siamese_accuracy"
  type: "SiameseAccuracy"
  bottom: "fc8"
  bottom: "fc8_p"
  bottom: "sim"
  top: "siamese_accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "fc8"
  bottom: "fc8_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 100
  }
}
I0318 05:16:46.119122  3501 layer_factory.hpp:77] Creating layer pair_data
I0318 05:16:46.119340  3501 net.cpp:91] Creating Layer pair_data
I0318 05:16:46.119359  3501 net.cpp:399] pair_data -> pair_data
I0318 05:16:46.119388  3501 net.cpp:399] pair_data -> sim
I0318 05:16:46.126886  3507 db_leveldb.cpp:18] Opened leveldb /home/ubuntu/fyp/data/data-fyp/siamese_test_leveldb
I0318 05:16:46.129024  3501 data_layer.cpp:41] output data size: 100,6,227,227
I0318 05:16:46.368677  3501 net.cpp:141] Setting up pair_data
I0318 05:16:46.368727  3501 net.cpp:148] Top shape: 100 6 227 227 (30917400)
I0318 05:16:46.368736  3501 net.cpp:148] Top shape: 100 (100)
I0318 05:16:46.368741  3501 net.cpp:156] Memory required for data: 123670000
I0318 05:16:46.368752  3501 layer_factory.hpp:77] Creating layer sim_pair_data_1_split
I0318 05:16:46.368775  3501 net.cpp:91] Creating Layer sim_pair_data_1_split
I0318 05:16:46.368782  3501 net.cpp:425] sim_pair_data_1_split <- sim
I0318 05:16:46.368798  3501 net.cpp:399] sim_pair_data_1_split -> sim_pair_data_1_split_0
I0318 05:16:46.368814  3501 net.cpp:399] sim_pair_data_1_split -> sim_pair_data_1_split_1
I0318 05:16:46.368923  3501 net.cpp:141] Setting up sim_pair_data_1_split
I0318 05:16:46.368939  3501 net.cpp:148] Top shape: 100 (100)
I0318 05:16:46.368947  3501 net.cpp:148] Top shape: 100 (100)
I0318 05:16:46.368950  3501 net.cpp:156] Memory required for data: 123670800
I0318 05:16:46.368957  3501 layer_factory.hpp:77] Creating layer slice_pair
I0318 05:16:46.368968  3501 net.cpp:91] Creating Layer slice_pair
I0318 05:16:46.368974  3501 net.cpp:425] slice_pair <- pair_data
I0318 05:16:46.368985  3501 net.cpp:399] slice_pair -> data
I0318 05:16:46.368996  3501 net.cpp:399] slice_pair -> data_p
I0318 05:16:46.369058  3501 net.cpp:141] Setting up slice_pair
I0318 05:16:46.369074  3501 net.cpp:148] Top shape: 100 3 227 227 (15458700)
I0318 05:16:46.369081  3501 net.cpp:148] Top shape: 100 3 227 227 (15458700)
I0318 05:16:46.369086  3501 net.cpp:156] Memory required for data: 247340400
I0318 05:16:46.369091  3501 layer_factory.hpp:77] Creating layer conv1
I0318 05:16:46.369113  3501 net.cpp:91] Creating Layer conv1
I0318 05:16:46.369125  3501 net.cpp:425] conv1 <- data
I0318 05:16:46.369173  3501 net.cpp:399] conv1 -> conv1
I0318 05:16:46.386037  3501 net.cpp:141] Setting up conv1
I0318 05:16:46.386085  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:46.386091  3501 net.cpp:156] Memory required for data: 363500400
I0318 05:16:46.386112  3501 layer_factory.hpp:77] Creating layer relu1
I0318 05:16:46.386132  3501 net.cpp:91] Creating Layer relu1
I0318 05:16:46.386139  3501 net.cpp:425] relu1 <- conv1
I0318 05:16:46.386148  3501 net.cpp:386] relu1 -> conv1 (in-place)
I0318 05:16:46.386428  3501 net.cpp:141] Setting up relu1
I0318 05:16:46.386451  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:46.386456  3501 net.cpp:156] Memory required for data: 479660400
I0318 05:16:46.386461  3501 layer_factory.hpp:77] Creating layer norm1
I0318 05:16:46.386473  3501 net.cpp:91] Creating Layer norm1
I0318 05:16:46.386478  3501 net.cpp:425] norm1 <- conv1
I0318 05:16:46.386490  3501 net.cpp:399] norm1 -> norm1
I0318 05:16:46.386704  3501 net.cpp:141] Setting up norm1
I0318 05:16:46.386724  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:46.386729  3501 net.cpp:156] Memory required for data: 595820400
I0318 05:16:46.386734  3501 layer_factory.hpp:77] Creating layer pool1
I0318 05:16:46.386744  3501 net.cpp:91] Creating Layer pool1
I0318 05:16:46.386750  3501 net.cpp:425] pool1 <- norm1
I0318 05:16:46.386761  3501 net.cpp:399] pool1 -> pool1
I0318 05:16:46.386816  3501 net.cpp:141] Setting up pool1
I0318 05:16:46.386840  3501 net.cpp:148] Top shape: 100 96 27 27 (6998400)
I0318 05:16:46.386847  3501 net.cpp:156] Memory required for data: 623814000
I0318 05:16:46.386852  3501 layer_factory.hpp:77] Creating layer conv2
I0318 05:16:46.386878  3501 net.cpp:91] Creating Layer conv2
I0318 05:16:46.386891  3501 net.cpp:425] conv2 <- pool1
I0318 05:16:46.386901  3501 net.cpp:399] conv2 -> conv2
I0318 05:16:46.399448  3501 net.cpp:141] Setting up conv2
I0318 05:16:46.399497  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:46.399503  3501 net.cpp:156] Memory required for data: 698463600
I0318 05:16:46.399524  3501 layer_factory.hpp:77] Creating layer relu2
I0318 05:16:46.399549  3501 net.cpp:91] Creating Layer relu2
I0318 05:16:46.399557  3501 net.cpp:425] relu2 <- conv2
I0318 05:16:46.399569  3501 net.cpp:386] relu2 -> conv2 (in-place)
I0318 05:16:46.399788  3501 net.cpp:141] Setting up relu2
I0318 05:16:46.399806  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:46.399811  3501 net.cpp:156] Memory required for data: 773113200
I0318 05:16:46.399816  3501 layer_factory.hpp:77] Creating layer norm2
I0318 05:16:46.399832  3501 net.cpp:91] Creating Layer norm2
I0318 05:16:46.399837  3501 net.cpp:425] norm2 <- conv2
I0318 05:16:46.399847  3501 net.cpp:399] norm2 -> norm2
I0318 05:16:46.400151  3501 net.cpp:141] Setting up norm2
I0318 05:16:46.400173  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:46.400179  3501 net.cpp:156] Memory required for data: 847762800
I0318 05:16:46.400185  3501 layer_factory.hpp:77] Creating layer pool2
I0318 05:16:46.400195  3501 net.cpp:91] Creating Layer pool2
I0318 05:16:46.400200  3501 net.cpp:425] pool2 <- norm2
I0318 05:16:46.400208  3501 net.cpp:399] pool2 -> pool2
I0318 05:16:46.400277  3501 net.cpp:141] Setting up pool2
I0318 05:16:46.400308  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:46.400315  3501 net.cpp:156] Memory required for data: 865068400
I0318 05:16:46.400319  3501 layer_factory.hpp:77] Creating layer conv3
I0318 05:16:46.400338  3501 net.cpp:91] Creating Layer conv3
I0318 05:16:46.400352  3501 net.cpp:425] conv3 <- pool2
I0318 05:16:46.400363  3501 net.cpp:399] conv3 -> conv3
I0318 05:16:46.432004  3501 net.cpp:141] Setting up conv3
I0318 05:16:46.432055  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:46.432061  3501 net.cpp:156] Memory required for data: 891026800
I0318 05:16:46.432083  3501 layer_factory.hpp:77] Creating layer relu3
I0318 05:16:46.432102  3501 net.cpp:91] Creating Layer relu3
I0318 05:16:46.432144  3501 net.cpp:425] relu3 <- conv3
I0318 05:16:46.432157  3501 net.cpp:386] relu3 -> conv3 (in-place)
I0318 05:16:46.432443  3501 net.cpp:141] Setting up relu3
I0318 05:16:46.432464  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:46.432471  3501 net.cpp:156] Memory required for data: 916985200
I0318 05:16:46.432476  3501 layer_factory.hpp:77] Creating layer conv4
I0318 05:16:46.432494  3501 net.cpp:91] Creating Layer conv4
I0318 05:16:46.432502  3501 net.cpp:425] conv4 <- conv3
I0318 05:16:46.432512  3501 net.cpp:399] conv4 -> conv4
I0318 05:16:46.456934  3501 net.cpp:141] Setting up conv4
I0318 05:16:46.456982  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:46.456990  3501 net.cpp:156] Memory required for data: 942943600
I0318 05:16:46.457005  3501 layer_factory.hpp:77] Creating layer relu4
I0318 05:16:46.457020  3501 net.cpp:91] Creating Layer relu4
I0318 05:16:46.457027  3501 net.cpp:425] relu4 <- conv4
I0318 05:16:46.457036  3501 net.cpp:386] relu4 -> conv4 (in-place)
I0318 05:16:46.457211  3501 net.cpp:141] Setting up relu4
I0318 05:16:46.457231  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:46.457237  3501 net.cpp:156] Memory required for data: 968902000
I0318 05:16:46.457242  3501 layer_factory.hpp:77] Creating layer conv5
I0318 05:16:46.457262  3501 net.cpp:91] Creating Layer conv5
I0318 05:16:46.457271  3501 net.cpp:425] conv5 <- conv4
I0318 05:16:46.457283  3501 net.cpp:399] conv5 -> conv5
I0318 05:16:46.474375  3501 net.cpp:141] Setting up conv5
I0318 05:16:46.474431  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:46.474437  3501 net.cpp:156] Memory required for data: 986207600
I0318 05:16:46.474463  3501 layer_factory.hpp:77] Creating layer relu5
I0318 05:16:46.474484  3501 net.cpp:91] Creating Layer relu5
I0318 05:16:46.474493  3501 net.cpp:425] relu5 <- conv5
I0318 05:16:46.474508  3501 net.cpp:386] relu5 -> conv5 (in-place)
I0318 05:16:46.474798  3501 net.cpp:141] Setting up relu5
I0318 05:16:46.474820  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:46.474827  3501 net.cpp:156] Memory required for data: 1003513200
I0318 05:16:46.474833  3501 layer_factory.hpp:77] Creating layer pool5
I0318 05:16:46.474845  3501 net.cpp:91] Creating Layer pool5
I0318 05:16:46.474851  3501 net.cpp:425] pool5 <- conv5
I0318 05:16:46.474862  3501 net.cpp:399] pool5 -> pool5
I0318 05:16:46.474932  3501 net.cpp:141] Setting up pool5
I0318 05:16:46.474952  3501 net.cpp:148] Top shape: 100 256 6 6 (921600)
I0318 05:16:46.474957  3501 net.cpp:156] Memory required for data: 1007199600
I0318 05:16:46.474962  3501 layer_factory.hpp:77] Creating layer fc6
I0318 05:16:46.474977  3501 net.cpp:91] Creating Layer fc6
I0318 05:16:46.474983  3501 net.cpp:425] fc6 <- pool5
I0318 05:16:46.474993  3501 net.cpp:399] fc6 -> fc6
I0318 05:16:47.739014  3501 net.cpp:141] Setting up fc6
I0318 05:16:47.739068  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:47.739074  3501 net.cpp:156] Memory required for data: 1008838000
I0318 05:16:47.739091  3501 layer_factory.hpp:77] Creating layer relu6
I0318 05:16:47.739109  3501 net.cpp:91] Creating Layer relu6
I0318 05:16:47.739115  3501 net.cpp:425] relu6 <- fc6
I0318 05:16:47.739128  3501 net.cpp:386] relu6 -> fc6 (in-place)
I0318 05:16:47.739387  3501 net.cpp:141] Setting up relu6
I0318 05:16:47.739408  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:47.739413  3501 net.cpp:156] Memory required for data: 1010476400
I0318 05:16:47.739418  3501 layer_factory.hpp:77] Creating layer drop6
I0318 05:16:47.739431  3501 net.cpp:91] Creating Layer drop6
I0318 05:16:47.739437  3501 net.cpp:425] drop6 <- fc6
I0318 05:16:47.739446  3501 net.cpp:386] drop6 -> fc6 (in-place)
I0318 05:16:47.739497  3501 net.cpp:141] Setting up drop6
I0318 05:16:47.739517  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:47.739522  3501 net.cpp:156] Memory required for data: 1012114800
I0318 05:16:47.739528  3501 layer_factory.hpp:77] Creating layer fc7
I0318 05:16:47.739547  3501 net.cpp:91] Creating Layer fc7
I0318 05:16:47.739578  3501 net.cpp:425] fc7 <- fc6
I0318 05:16:47.739598  3501 net.cpp:399] fc7 -> fc7
I0318 05:16:48.301506  3501 net.cpp:141] Setting up fc7
I0318 05:16:48.301561  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:48.301568  3501 net.cpp:156] Memory required for data: 1013753200
I0318 05:16:48.301585  3501 layer_factory.hpp:77] Creating layer fc8
I0318 05:16:48.301606  3501 net.cpp:91] Creating Layer fc8
I0318 05:16:48.301614  3501 net.cpp:425] fc8 <- fc7
I0318 05:16:48.301627  3501 net.cpp:399] fc8 -> fc8
I0318 05:16:48.438138  3501 net.cpp:141] Setting up fc8
I0318 05:16:48.438180  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:48.438187  3501 net.cpp:156] Memory required for data: 1014153200
I0318 05:16:48.438201  3501 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0318 05:16:48.438215  3501 net.cpp:91] Creating Layer fc8_fc8_0_split
I0318 05:16:48.438220  3501 net.cpp:425] fc8_fc8_0_split <- fc8
I0318 05:16:48.438230  3501 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0318 05:16:48.438241  3501 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0318 05:16:48.438313  3501 net.cpp:141] Setting up fc8_fc8_0_split
I0318 05:16:48.438333  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:48.438340  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:48.438344  3501 net.cpp:156] Memory required for data: 1014953200
I0318 05:16:48.438350  3501 layer_factory.hpp:77] Creating layer conv1_p
I0318 05:16:48.438370  3501 net.cpp:91] Creating Layer conv1_p
I0318 05:16:48.438379  3501 net.cpp:425] conv1_p <- data_p
I0318 05:16:48.438392  3501 net.cpp:399] conv1_p -> conv1_p
I0318 05:16:48.440726  3501 net.cpp:141] Setting up conv1_p
I0318 05:16:48.440749  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:48.440755  3501 net.cpp:156] Memory required for data: 1131113200
I0318 05:16:48.440768  3501 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0318 05:16:48.440775  3501 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0318 05:16:48.440781  3501 layer_factory.hpp:77] Creating layer relu1_p
I0318 05:16:48.440793  3501 net.cpp:91] Creating Layer relu1_p
I0318 05:16:48.440809  3501 net.cpp:425] relu1_p <- conv1_p
I0318 05:16:48.440819  3501 net.cpp:386] relu1_p -> conv1_p (in-place)
I0318 05:16:48.440991  3501 net.cpp:141] Setting up relu1_p
I0318 05:16:48.441010  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:48.441015  3501 net.cpp:156] Memory required for data: 1247273200
I0318 05:16:48.441021  3501 layer_factory.hpp:77] Creating layer norm1_p
I0318 05:16:48.441035  3501 net.cpp:91] Creating Layer norm1_p
I0318 05:16:48.441040  3501 net.cpp:425] norm1_p <- conv1_p
I0318 05:16:48.441048  3501 net.cpp:399] norm1_p -> norm1_p
I0318 05:16:48.441365  3501 net.cpp:141] Setting up norm1_p
I0318 05:16:48.441386  3501 net.cpp:148] Top shape: 100 96 55 55 (29040000)
I0318 05:16:48.441391  3501 net.cpp:156] Memory required for data: 1363433200
I0318 05:16:48.441397  3501 layer_factory.hpp:77] Creating layer pool1_p
I0318 05:16:48.441407  3501 net.cpp:91] Creating Layer pool1_p
I0318 05:16:48.441413  3501 net.cpp:425] pool1_p <- norm1_p
I0318 05:16:48.441424  3501 net.cpp:399] pool1_p -> pool1_p
I0318 05:16:48.441483  3501 net.cpp:141] Setting up pool1_p
I0318 05:16:48.441498  3501 net.cpp:148] Top shape: 100 96 27 27 (6998400)
I0318 05:16:48.441504  3501 net.cpp:156] Memory required for data: 1391426800
I0318 05:16:48.441509  3501 layer_factory.hpp:77] Creating layer conv2_p
I0318 05:16:48.441526  3501 net.cpp:91] Creating Layer conv2_p
I0318 05:16:48.441535  3501 net.cpp:425] conv2_p <- pool1_p
I0318 05:16:48.441545  3501 net.cpp:399] conv2_p -> conv2_p
I0318 05:16:48.453713  3501 net.cpp:141] Setting up conv2_p
I0318 05:16:48.453737  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:48.453742  3501 net.cpp:156] Memory required for data: 1466076400
I0318 05:16:48.453749  3501 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0318 05:16:48.453785  3501 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0318 05:16:48.453793  3501 layer_factory.hpp:77] Creating layer relu2_p
I0318 05:16:48.453800  3501 net.cpp:91] Creating Layer relu2_p
I0318 05:16:48.453806  3501 net.cpp:425] relu2_p <- conv2_p
I0318 05:16:48.453816  3501 net.cpp:386] relu2_p -> conv2_p (in-place)
I0318 05:16:48.453994  3501 net.cpp:141] Setting up relu2_p
I0318 05:16:48.454013  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:48.454018  3501 net.cpp:156] Memory required for data: 1540726000
I0318 05:16:48.454025  3501 layer_factory.hpp:77] Creating layer norm2_p
I0318 05:16:48.454041  3501 net.cpp:91] Creating Layer norm2_p
I0318 05:16:48.454049  3501 net.cpp:425] norm2_p <- conv2_p
I0318 05:16:48.454058  3501 net.cpp:399] norm2_p -> norm2_p
I0318 05:16:48.454454  3501 net.cpp:141] Setting up norm2_p
I0318 05:16:48.454476  3501 net.cpp:148] Top shape: 100 256 27 27 (18662400)
I0318 05:16:48.454481  3501 net.cpp:156] Memory required for data: 1615375600
I0318 05:16:48.454488  3501 layer_factory.hpp:77] Creating layer pool2_p
I0318 05:16:48.454496  3501 net.cpp:91] Creating Layer pool2_p
I0318 05:16:48.454501  3501 net.cpp:425] pool2_p <- norm2_p
I0318 05:16:48.454511  3501 net.cpp:399] pool2_p -> pool2_p
I0318 05:16:48.454569  3501 net.cpp:141] Setting up pool2_p
I0318 05:16:48.454586  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:48.454591  3501 net.cpp:156] Memory required for data: 1632681200
I0318 05:16:48.454596  3501 layer_factory.hpp:77] Creating layer conv3_p
I0318 05:16:48.454617  3501 net.cpp:91] Creating Layer conv3_p
I0318 05:16:48.454629  3501 net.cpp:425] conv3_p <- pool2_p
I0318 05:16:48.454639  3501 net.cpp:399] conv3_p -> conv3_p
I0318 05:16:48.484994  3501 net.cpp:141] Setting up conv3_p
I0318 05:16:48.485018  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:48.485024  3501 net.cpp:156] Memory required for data: 1658639600
I0318 05:16:48.485031  3501 net.cpp:484] Sharing parameters 'conv3_w' owned by layer 'conv3', param index 0
I0318 05:16:48.485038  3501 net.cpp:484] Sharing parameters 'conv3_b' owned by layer 'conv3', param index 1
I0318 05:16:48.485044  3501 layer_factory.hpp:77] Creating layer relu3_p
I0318 05:16:48.485052  3501 net.cpp:91] Creating Layer relu3_p
I0318 05:16:48.485059  3501 net.cpp:425] relu3_p <- conv3_p
I0318 05:16:48.485067  3501 net.cpp:386] relu3_p -> conv3_p (in-place)
I0318 05:16:48.485337  3501 net.cpp:141] Setting up relu3_p
I0318 05:16:48.485358  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:48.485364  3501 net.cpp:156] Memory required for data: 1684598000
I0318 05:16:48.485369  3501 layer_factory.hpp:77] Creating layer conv4_p
I0318 05:16:48.485386  3501 net.cpp:91] Creating Layer conv4_p
I0318 05:16:48.485393  3501 net.cpp:425] conv4_p <- conv3_p
I0318 05:16:48.485404  3501 net.cpp:399] conv4_p -> conv4_p
I0318 05:16:48.509313  3501 net.cpp:141] Setting up conv4_p
I0318 05:16:48.509336  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:48.509342  3501 net.cpp:156] Memory required for data: 1710556400
I0318 05:16:48.509349  3501 net.cpp:484] Sharing parameters 'conv4_w' owned by layer 'conv4', param index 0
I0318 05:16:48.509356  3501 net.cpp:484] Sharing parameters 'conv4_b' owned by layer 'conv4', param index 1
I0318 05:16:48.509361  3501 layer_factory.hpp:77] Creating layer relu4_p
I0318 05:16:48.509369  3501 net.cpp:91] Creating Layer relu4_p
I0318 05:16:48.509374  3501 net.cpp:425] relu4_p <- conv4_p
I0318 05:16:48.509384  3501 net.cpp:386] relu4_p -> conv4_p (in-place)
I0318 05:16:48.509555  3501 net.cpp:141] Setting up relu4_p
I0318 05:16:48.509574  3501 net.cpp:148] Top shape: 100 384 13 13 (6489600)
I0318 05:16:48.509583  3501 net.cpp:156] Memory required for data: 1736514800
I0318 05:16:48.509588  3501 layer_factory.hpp:77] Creating layer conv5_p
I0318 05:16:48.509604  3501 net.cpp:91] Creating Layer conv5_p
I0318 05:16:48.509610  3501 net.cpp:425] conv5_p <- conv4_p
I0318 05:16:48.509623  3501 net.cpp:399] conv5_p -> conv5_p
I0318 05:16:48.526391  3501 net.cpp:141] Setting up conv5_p
I0318 05:16:48.526418  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:48.526424  3501 net.cpp:156] Memory required for data: 1753820400
I0318 05:16:48.526432  3501 net.cpp:484] Sharing parameters 'conv5_w' owned by layer 'conv5', param index 0
I0318 05:16:48.526437  3501 net.cpp:484] Sharing parameters 'conv5_b' owned by layer 'conv5', param index 1
I0318 05:16:48.526443  3501 layer_factory.hpp:77] Creating layer relu5_p
I0318 05:16:48.526451  3501 net.cpp:91] Creating Layer relu5_p
I0318 05:16:48.526456  3501 net.cpp:425] relu5_p <- conv5_p
I0318 05:16:48.526468  3501 net.cpp:386] relu5_p -> conv5_p (in-place)
I0318 05:16:48.526643  3501 net.cpp:141] Setting up relu5_p
I0318 05:16:48.526662  3501 net.cpp:148] Top shape: 100 256 13 13 (4326400)
I0318 05:16:48.526667  3501 net.cpp:156] Memory required for data: 1771126000
I0318 05:16:48.526674  3501 layer_factory.hpp:77] Creating layer pool5_p
I0318 05:16:48.526682  3501 net.cpp:91] Creating Layer pool5_p
I0318 05:16:48.526687  3501 net.cpp:425] pool5_p <- conv5_p
I0318 05:16:48.526698  3501 net.cpp:399] pool5_p -> pool5_p
I0318 05:16:48.526762  3501 net.cpp:141] Setting up pool5_p
I0318 05:16:48.526778  3501 net.cpp:148] Top shape: 100 256 6 6 (921600)
I0318 05:16:48.526783  3501 net.cpp:156] Memory required for data: 1774812400
I0318 05:16:48.526788  3501 layer_factory.hpp:77] Creating layer fc6_p
I0318 05:16:48.526803  3501 net.cpp:91] Creating Layer fc6_p
I0318 05:16:48.526808  3501 net.cpp:425] fc6_p <- pool5_p
I0318 05:16:48.526818  3501 net.cpp:399] fc6_p -> fc6_p
I0318 05:16:49.788250  3501 net.cpp:141] Setting up fc6_p
I0318 05:16:49.788312  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:49.788319  3501 net.cpp:156] Memory required for data: 1776450800
I0318 05:16:49.788332  3501 net.cpp:484] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0318 05:16:49.788341  3501 net.cpp:484] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0318 05:16:49.788347  3501 layer_factory.hpp:77] Creating layer relu6_p
I0318 05:16:49.788363  3501 net.cpp:91] Creating Layer relu6_p
I0318 05:16:49.788383  3501 net.cpp:425] relu6_p <- fc6_p
I0318 05:16:49.788398  3501 net.cpp:386] relu6_p -> fc6_p (in-place)
I0318 05:16:49.788857  3501 net.cpp:141] Setting up relu6_p
I0318 05:16:49.788879  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:49.788884  3501 net.cpp:156] Memory required for data: 1778089200
I0318 05:16:49.788890  3501 layer_factory.hpp:77] Creating layer drop6_p
I0318 05:16:49.788902  3501 net.cpp:91] Creating Layer drop6_p
I0318 05:16:49.788908  3501 net.cpp:425] drop6_p <- fc6_p
I0318 05:16:49.788918  3501 net.cpp:386] drop6_p -> fc6_p (in-place)
I0318 05:16:49.788967  3501 net.cpp:141] Setting up drop6_p
I0318 05:16:49.788987  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:49.788992  3501 net.cpp:156] Memory required for data: 1779727600
I0318 05:16:49.788998  3501 layer_factory.hpp:77] Creating layer fc7_p
I0318 05:16:49.789014  3501 net.cpp:91] Creating Layer fc7_p
I0318 05:16:49.789031  3501 net.cpp:425] fc7_p <- fc6_p
I0318 05:16:49.789046  3501 net.cpp:399] fc7_p -> fc7_p
I0318 05:16:50.352656  3501 net.cpp:141] Setting up fc7_p
I0318 05:16:50.352710  3501 net.cpp:148] Top shape: 100 4096 (409600)
I0318 05:16:50.352716  3501 net.cpp:156] Memory required for data: 1781366000
I0318 05:16:50.352728  3501 net.cpp:484] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0318 05:16:50.352736  3501 net.cpp:484] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0318 05:16:50.352742  3501 layer_factory.hpp:77] Creating layer fc8_p
I0318 05:16:50.352764  3501 net.cpp:91] Creating Layer fc8_p
I0318 05:16:50.352773  3501 net.cpp:425] fc8_p <- fc7_p
I0318 05:16:50.352785  3501 net.cpp:399] fc8_p -> fc8_p
I0318 05:16:50.489423  3501 net.cpp:141] Setting up fc8_p
I0318 05:16:50.489462  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:50.489470  3501 net.cpp:156] Memory required for data: 1781766000
I0318 05:16:50.489507  3501 net.cpp:484] Sharing parameters 'fc8_w' owned by layer 'fc8', param index 0
I0318 05:16:50.489516  3501 net.cpp:484] Sharing parameters 'fc8_b' owned by layer 'fc8', param index 1
I0318 05:16:50.489521  3501 layer_factory.hpp:77] Creating layer fc8_p_fc8_p_0_split
I0318 05:16:50.489536  3501 net.cpp:91] Creating Layer fc8_p_fc8_p_0_split
I0318 05:16:50.489550  3501 net.cpp:425] fc8_p_fc8_p_0_split <- fc8_p
I0318 05:16:50.489562  3501 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_0
I0318 05:16:50.489573  3501 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_1
I0318 05:16:50.489653  3501 net.cpp:141] Setting up fc8_p_fc8_p_0_split
I0318 05:16:50.489671  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:50.489677  3501 net.cpp:148] Top shape: 100 1000 (100000)
I0318 05:16:50.489682  3501 net.cpp:156] Memory required for data: 1782566000
I0318 05:16:50.489687  3501 layer_factory.hpp:77] Creating layer siamese_accuracy
I0318 05:16:50.489703  3501 net.cpp:91] Creating Layer siamese_accuracy
I0318 05:16:50.489709  3501 net.cpp:425] siamese_accuracy <- fc8_fc8_0_split_0
I0318 05:16:50.489717  3501 net.cpp:425] siamese_accuracy <- fc8_p_fc8_p_0_split_0
I0318 05:16:50.489723  3501 net.cpp:425] siamese_accuracy <- sim_pair_data_1_split_0
I0318 05:16:50.489742  3501 net.cpp:399] siamese_accuracy -> siamese_accuracy
I0318 05:16:50.489909  3501 net.cpp:141] Setting up siamese_accuracy
I0318 05:16:50.489928  3501 net.cpp:148] Top shape: (1)
I0318 05:16:50.489933  3501 net.cpp:151]     with loss weight 1
I0318 05:16:50.489950  3501 net.cpp:156] Memory required for data: 1782566004
I0318 05:16:50.489956  3501 layer_factory.hpp:77] Creating layer loss
I0318 05:16:50.489967  3501 net.cpp:91] Creating Layer loss
I0318 05:16:50.489974  3501 net.cpp:425] loss <- fc8_fc8_0_split_1
I0318 05:16:50.489979  3501 net.cpp:425] loss <- fc8_p_fc8_p_0_split_1
I0318 05:16:50.489986  3501 net.cpp:425] loss <- sim_pair_data_1_split_1
I0318 05:16:50.489995  3501 net.cpp:399] loss -> loss
I0318 05:16:50.490134  3501 net.cpp:141] Setting up loss
I0318 05:16:50.490150  3501 net.cpp:148] Top shape: (1)
I0318 05:16:50.490155  3501 net.cpp:151]     with loss weight 1
I0318 05:16:50.490164  3501 net.cpp:156] Memory required for data: 1782566008
I0318 05:16:50.490170  3501 net.cpp:217] loss needs backward computation.
I0318 05:16:50.490175  3501 net.cpp:217] siamese_accuracy needs backward computation.
I0318 05:16:50.490180  3501 net.cpp:217] fc8_p_fc8_p_0_split needs backward computation.
I0318 05:16:50.490185  3501 net.cpp:217] fc8_p needs backward computation.
I0318 05:16:50.490190  3501 net.cpp:219] fc7_p does not need backward computation.
I0318 05:16:50.490195  3501 net.cpp:219] drop6_p does not need backward computation.
I0318 05:16:50.490200  3501 net.cpp:219] relu6_p does not need backward computation.
I0318 05:16:50.490206  3501 net.cpp:219] fc6_p does not need backward computation.
I0318 05:16:50.490211  3501 net.cpp:219] pool5_p does not need backward computation.
I0318 05:16:50.490216  3501 net.cpp:219] relu5_p does not need backward computation.
I0318 05:16:50.490221  3501 net.cpp:219] conv5_p does not need backward computation.
I0318 05:16:50.490226  3501 net.cpp:219] relu4_p does not need backward computation.
I0318 05:16:50.490231  3501 net.cpp:219] conv4_p does not need backward computation.
I0318 05:16:50.490236  3501 net.cpp:219] relu3_p does not need backward computation.
I0318 05:16:50.490242  3501 net.cpp:219] conv3_p does not need backward computation.
I0318 05:16:50.490247  3501 net.cpp:219] pool2_p does not need backward computation.
I0318 05:16:50.490254  3501 net.cpp:219] norm2_p does not need backward computation.
I0318 05:16:50.490260  3501 net.cpp:219] relu2_p does not need backward computation.
I0318 05:16:50.490265  3501 net.cpp:219] conv2_p does not need backward computation.
I0318 05:16:50.490270  3501 net.cpp:219] pool1_p does not need backward computation.
I0318 05:16:50.490275  3501 net.cpp:219] norm1_p does not need backward computation.
I0318 05:16:50.490293  3501 net.cpp:219] relu1_p does not need backward computation.
I0318 05:16:50.490298  3501 net.cpp:219] conv1_p does not need backward computation.
I0318 05:16:50.490303  3501 net.cpp:217] fc8_fc8_0_split needs backward computation.
I0318 05:16:50.490309  3501 net.cpp:217] fc8 needs backward computation.
I0318 05:16:50.490314  3501 net.cpp:219] fc7 does not need backward computation.
I0318 05:16:50.490319  3501 net.cpp:219] drop6 does not need backward computation.
I0318 05:16:50.490324  3501 net.cpp:219] relu6 does not need backward computation.
I0318 05:16:50.490329  3501 net.cpp:219] fc6 does not need backward computation.
I0318 05:16:50.490334  3501 net.cpp:219] pool5 does not need backward computation.
I0318 05:16:50.490340  3501 net.cpp:219] relu5 does not need backward computation.
I0318 05:16:50.490345  3501 net.cpp:219] conv5 does not need backward computation.
I0318 05:16:50.490350  3501 net.cpp:219] relu4 does not need backward computation.
I0318 05:16:50.490355  3501 net.cpp:219] conv4 does not need backward computation.
I0318 05:16:50.490360  3501 net.cpp:219] relu3 does not need backward computation.
I0318 05:16:50.490365  3501 net.cpp:219] conv3 does not need backward computation.
I0318 05:16:50.490370  3501 net.cpp:219] pool2 does not need backward computation.
I0318 05:16:50.490375  3501 net.cpp:219] norm2 does not need backward computation.
I0318 05:16:50.490381  3501 net.cpp:219] relu2 does not need backward computation.
I0318 05:16:50.490386  3501 net.cpp:219] conv2 does not need backward computation.
I0318 05:16:50.490391  3501 net.cpp:219] pool1 does not need backward computation.
I0318 05:16:50.490396  3501 net.cpp:219] norm1 does not need backward computation.
I0318 05:16:50.490401  3501 net.cpp:219] relu1 does not need backward computation.
I0318 05:16:50.490406  3501 net.cpp:219] conv1 does not need backward computation.
I0318 05:16:50.490411  3501 net.cpp:219] slice_pair does not need backward computation.
I0318 05:16:50.490417  3501 net.cpp:219] sim_pair_data_1_split does not need backward computation.
I0318 05:16:50.490423  3501 net.cpp:219] pair_data does not need backward computation.
I0318 05:16:50.490427  3501 net.cpp:261] This network produces output loss
I0318 05:16:50.490433  3501 net.cpp:261] This network produces output siamese_accuracy
I0318 05:16:50.527426  3501 net.cpp:274] Network initialization done.
I0318 05:16:50.527741  3501 solver.cpp:60] Solver scaffolding done.
I0318 05:16:50.528630  3501 caffe.cpp:129] Finetuning from ./bvlc_alexnet.caffemodel
I0318 05:16:50.976204  3501 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: ./bvlc_alexnet.caffemodel
I0318 05:16:50.976266  3501 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0318 05:16:50.976274  3501 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0318 05:16:50.976483  3501 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./bvlc_alexnet.caffemodel
I0318 05:16:51.170379  3501 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0318 05:16:51.171556  3501 net.cpp:752] Ignoring source layer data
I0318 05:16:51.217638  3501 net.cpp:752] Ignoring source layer relu7
I0318 05:16:51.217687  3501 net.cpp:752] Ignoring source layer drop7
I0318 05:16:51.663080  3501 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: ./bvlc_alexnet.caffemodel
I0318 05:16:51.663136  3501 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0318 05:16:51.663142  3501 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0318 05:16:51.663164  3501 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./bvlc_alexnet.caffemodel
I0318 05:16:51.854558  3501 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0318 05:16:51.855715  3501 net.cpp:752] Ignoring source layer data
I0318 05:16:51.900841  3501 net.cpp:752] Ignoring source layer relu7
I0318 05:16:51.900892  3501 net.cpp:752] Ignoring source layer drop7
I0318 05:16:51.907479  3501 caffe.cpp:219] Starting Optimization
I0318 05:16:51.907534  3501 solver.cpp:279] Solving SiameseAlexNet
I0318 05:16:51.907541  3501 solver.cpp:280] Learning Rate Policy: step
I0318 05:16:51.909412  3501 solver.cpp:337] Iteration 0, Testing net (#0)
I0318 05:18:46.172559  3501 solver.cpp:404]     Test net output #0: loss = 3588.3 (* 1 = 3588.3 loss)
I0318 05:18:46.172679  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:18:46.533381  3501 solver.cpp:228] Iteration 0, loss = 3026.03
I0318 05:18:46.533453  3501 solver.cpp:244]     Train net output #0: loss = 3026.03 (* 1 = 3026.03 loss)
I0318 05:18:46.533483  3501 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0318 05:18:53.788075  3501 solver.cpp:228] Iteration 20, loss = 473.995
I0318 05:18:53.788139  3501 solver.cpp:244]     Train net output #0: loss = 473.995 (* 1 = 473.995 loss)
I0318 05:18:53.788153  3501 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0318 05:19:01.087165  3501 solver.cpp:228] Iteration 40, loss = 167.708
I0318 05:19:01.087231  3501 solver.cpp:244]     Train net output #0: loss = 167.708 (* 1 = 167.708 loss)
I0318 05:19:01.087246  3501 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0318 05:19:08.404378  3501 solver.cpp:228] Iteration 60, loss = 165.122
I0318 05:19:08.404445  3501 solver.cpp:244]     Train net output #0: loss = 165.122 (* 1 = 165.122 loss)
I0318 05:19:08.404458  3501 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0318 05:19:15.732692  3501 solver.cpp:228] Iteration 80, loss = 131.426
I0318 05:19:15.732760  3501 solver.cpp:244]     Train net output #0: loss = 131.426 (* 1 = 131.426 loss)
I0318 05:19:15.732775  3501 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0318 05:19:23.067415  3501 solver.cpp:228] Iteration 100, loss = 165.888
I0318 05:19:23.067567  3501 solver.cpp:244]     Train net output #0: loss = 165.888 (* 1 = 165.888 loss)
I0318 05:19:23.067582  3501 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0318 05:19:30.397675  3501 solver.cpp:228] Iteration 120, loss = 112.005
I0318 05:19:30.397742  3501 solver.cpp:244]     Train net output #0: loss = 112.005 (* 1 = 112.005 loss)
I0318 05:19:30.397754  3501 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0318 05:19:37.732085  3501 solver.cpp:228] Iteration 140, loss = 188.2
I0318 05:19:37.732151  3501 solver.cpp:244]     Train net output #0: loss = 188.2 (* 1 = 188.2 loss)
I0318 05:19:37.732163  3501 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0318 05:19:45.071535  3501 solver.cpp:228] Iteration 160, loss = 247.518
I0318 05:19:45.071600  3501 solver.cpp:244]     Train net output #0: loss = 247.518 (* 1 = 247.518 loss)
I0318 05:19:45.071614  3501 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0318 05:19:52.409605  3501 solver.cpp:228] Iteration 180, loss = 178.906
I0318 05:19:52.409682  3501 solver.cpp:244]     Train net output #0: loss = 178.906 (* 1 = 178.906 loss)
I0318 05:19:52.409696  3501 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0318 05:19:59.744654  3501 solver.cpp:228] Iteration 200, loss = 177.343
I0318 05:19:59.744828  3501 solver.cpp:244]     Train net output #0: loss = 177.343 (* 1 = 177.343 loss)
I0318 05:19:59.744843  3501 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0318 05:20:07.083253  3501 solver.cpp:228] Iteration 220, loss = 60.1938
I0318 05:20:07.083325  3501 solver.cpp:244]     Train net output #0: loss = 60.1937 (* 1 = 60.1937 loss)
I0318 05:20:07.083339  3501 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0318 05:20:14.416177  3501 solver.cpp:228] Iteration 240, loss = 62.9019
I0318 05:20:14.416246  3501 solver.cpp:244]     Train net output #0: loss = 62.9019 (* 1 = 62.9019 loss)
I0318 05:20:14.416260  3501 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0318 05:20:17.719889  3501 solver.cpp:337] Iteration 250, Testing net (#0)
I0318 05:22:11.958719  3501 solver.cpp:404]     Test net output #0: loss = 636.711 (* 1 = 636.711 loss)
I0318 05:22:11.958884  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 05:22:15.908737  3501 solver.cpp:228] Iteration 260, loss = 64.1336
I0318 05:22:15.908802  3501 solver.cpp:244]     Train net output #0: loss = 64.1335 (* 1 = 64.1335 loss)
I0318 05:22:15.908815  3501 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0318 05:22:23.189671  3501 solver.cpp:228] Iteration 280, loss = 95.4324
I0318 05:22:23.189739  3501 solver.cpp:244]     Train net output #0: loss = 95.4323 (* 1 = 95.4323 loss)
I0318 05:22:23.189754  3501 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0318 05:22:30.498756  3501 solver.cpp:228] Iteration 300, loss = 86.6389
I0318 05:22:30.498821  3501 solver.cpp:244]     Train net output #0: loss = 86.6389 (* 1 = 86.6389 loss)
I0318 05:22:30.498836  3501 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0318 05:22:37.820860  3501 solver.cpp:228] Iteration 320, loss = 193.115
I0318 05:22:37.820925  3501 solver.cpp:244]     Train net output #0: loss = 193.115 (* 1 = 193.115 loss)
I0318 05:22:37.820938  3501 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0318 05:22:45.148421  3501 solver.cpp:228] Iteration 340, loss = 250.332
I0318 05:22:45.148571  3501 solver.cpp:244]     Train net output #0: loss = 250.331 (* 1 = 250.331 loss)
I0318 05:22:45.148586  3501 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0318 05:22:52.478669  3501 solver.cpp:228] Iteration 360, loss = 187.633
I0318 05:22:52.478734  3501 solver.cpp:244]     Train net output #0: loss = 187.633 (* 1 = 187.633 loss)
I0318 05:22:52.478749  3501 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0318 05:22:59.811803  3501 solver.cpp:228] Iteration 380, loss = 118.156
I0318 05:22:59.811869  3501 solver.cpp:244]     Train net output #0: loss = 118.156 (* 1 = 118.156 loss)
I0318 05:22:59.811882  3501 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0318 05:23:07.154856  3501 solver.cpp:228] Iteration 400, loss = 93.0455
I0318 05:23:07.154922  3501 solver.cpp:244]     Train net output #0: loss = 93.0454 (* 1 = 93.0454 loss)
I0318 05:23:07.154935  3501 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0318 05:23:14.503214  3501 solver.cpp:228] Iteration 420, loss = 171.089
I0318 05:23:14.503288  3501 solver.cpp:244]     Train net output #0: loss = 171.089 (* 1 = 171.089 loss)
I0318 05:23:14.503300  3501 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0318 05:23:21.841048  3501 solver.cpp:228] Iteration 440, loss = 81.8865
I0318 05:23:21.841195  3501 solver.cpp:244]     Train net output #0: loss = 81.8864 (* 1 = 81.8864 loss)
I0318 05:23:21.841210  3501 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0318 05:23:29.177922  3501 solver.cpp:228] Iteration 460, loss = 121.843
I0318 05:23:29.177987  3501 solver.cpp:244]     Train net output #0: loss = 121.843 (* 1 = 121.843 loss)
I0318 05:23:29.178002  3501 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0318 05:23:36.517536  3501 solver.cpp:228] Iteration 480, loss = 77.5785
I0318 05:23:36.517601  3501 solver.cpp:244]     Train net output #0: loss = 77.5784 (* 1 = 77.5784 loss)
I0318 05:23:36.517616  3501 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0318 05:23:43.478365  3501 solver.cpp:337] Iteration 500, Testing net (#0)
I0318 05:25:37.665382  3501 solver.cpp:404]     Test net output #0: loss = 664.388 (* 1 = 664.388 loss)
I0318 05:25:37.665482  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 05:25:38.002511  3501 solver.cpp:228] Iteration 500, loss = 141.003
I0318 05:25:38.002578  3501 solver.cpp:244]     Train net output #0: loss = 141.003 (* 1 = 141.003 loss)
I0318 05:25:38.002590  3501 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0318 05:25:45.254333  3501 solver.cpp:228] Iteration 520, loss = 219.01
I0318 05:25:45.254400  3501 solver.cpp:244]     Train net output #0: loss = 219.01 (* 1 = 219.01 loss)
I0318 05:25:45.254413  3501 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0318 05:25:52.567711  3501 solver.cpp:228] Iteration 540, loss = 219.79
I0318 05:25:52.567776  3501 solver.cpp:244]     Train net output #0: loss = 219.79 (* 1 = 219.79 loss)
I0318 05:25:52.567790  3501 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0318 05:25:59.898387  3501 solver.cpp:228] Iteration 560, loss = 140.89
I0318 05:25:59.898454  3501 solver.cpp:244]     Train net output #0: loss = 140.89 (* 1 = 140.89 loss)
I0318 05:25:59.898468  3501 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0318 05:26:07.229967  3501 solver.cpp:228] Iteration 580, loss = 77.4465
I0318 05:26:07.230031  3501 solver.cpp:244]     Train net output #0: loss = 77.4464 (* 1 = 77.4464 loss)
I0318 05:26:07.230046  3501 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0318 05:26:14.554507  3501 solver.cpp:228] Iteration 600, loss = 106.912
I0318 05:26:14.554689  3501 solver.cpp:244]     Train net output #0: loss = 106.912 (* 1 = 106.912 loss)
I0318 05:26:14.554703  3501 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0318 05:26:21.888381  3501 solver.cpp:228] Iteration 620, loss = 58.4968
I0318 05:26:21.888447  3501 solver.cpp:244]     Train net output #0: loss = 58.4967 (* 1 = 58.4967 loss)
I0318 05:26:21.888460  3501 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0318 05:26:29.220420  3501 solver.cpp:228] Iteration 640, loss = 93.4738
I0318 05:26:29.220489  3501 solver.cpp:244]     Train net output #0: loss = 93.4736 (* 1 = 93.4736 loss)
I0318 05:26:29.220504  3501 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0318 05:26:36.561014  3501 solver.cpp:228] Iteration 660, loss = 64.1104
I0318 05:26:36.561079  3501 solver.cpp:244]     Train net output #0: loss = 64.1103 (* 1 = 64.1103 loss)
I0318 05:26:36.561094  3501 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0318 05:26:43.904947  3501 solver.cpp:228] Iteration 680, loss = 76.9082
I0318 05:26:43.905014  3501 solver.cpp:244]     Train net output #0: loss = 76.9081 (* 1 = 76.9081 loss)
I0318 05:26:43.905028  3501 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0318 05:26:51.244628  3501 solver.cpp:228] Iteration 700, loss = 140.679
I0318 05:26:51.244770  3501 solver.cpp:244]     Train net output #0: loss = 140.679 (* 1 = 140.679 loss)
I0318 05:26:51.244784  3501 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0318 05:26:58.573864  3501 solver.cpp:228] Iteration 720, loss = 104.874
I0318 05:26:58.573930  3501 solver.cpp:244]     Train net output #0: loss = 104.874 (* 1 = 104.874 loss)
I0318 05:26:58.573942  3501 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0318 05:27:05.913487  3501 solver.cpp:228] Iteration 740, loss = 111.2
I0318 05:27:05.913553  3501 solver.cpp:244]     Train net output #0: loss = 111.2 (* 1 = 111.2 loss)
I0318 05:27:05.913568  3501 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0318 05:27:09.220808  3501 solver.cpp:337] Iteration 750, Testing net (#0)
I0318 05:29:03.438145  3501 solver.cpp:404]     Test net output #0: loss = 647.941 (* 1 = 647.941 loss)
I0318 05:29:03.438261  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:29:07.389387  3501 solver.cpp:228] Iteration 760, loss = 66.9537
I0318 05:29:07.389451  3501 solver.cpp:244]     Train net output #0: loss = 66.9536 (* 1 = 66.9536 loss)
I0318 05:29:07.389466  3501 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0318 05:29:14.659437  3501 solver.cpp:228] Iteration 780, loss = 76.5002
I0318 05:29:14.659503  3501 solver.cpp:244]     Train net output #0: loss = 76.5001 (* 1 = 76.5001 loss)
I0318 05:29:14.659518  3501 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0318 05:29:21.956245  3501 solver.cpp:228] Iteration 800, loss = 65.5435
I0318 05:29:21.956323  3501 solver.cpp:244]     Train net output #0: loss = 65.5435 (* 1 = 65.5435 loss)
I0318 05:29:21.956337  3501 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0318 05:29:29.276907  3501 solver.cpp:228] Iteration 820, loss = 126.707
I0318 05:29:29.276973  3501 solver.cpp:244]     Train net output #0: loss = 126.707 (* 1 = 126.707 loss)
I0318 05:29:29.276986  3501 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0318 05:29:36.608681  3501 solver.cpp:228] Iteration 840, loss = 65.9316
I0318 05:29:36.608860  3501 solver.cpp:244]     Train net output #0: loss = 65.9315 (* 1 = 65.9315 loss)
I0318 05:29:36.608875  3501 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0318 05:29:43.952500  3501 solver.cpp:228] Iteration 860, loss = 126.186
I0318 05:29:43.952565  3501 solver.cpp:244]     Train net output #0: loss = 126.186 (* 1 = 126.186 loss)
I0318 05:29:43.952579  3501 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0318 05:29:51.293903  3501 solver.cpp:228] Iteration 880, loss = 137.573
I0318 05:29:51.293982  3501 solver.cpp:244]     Train net output #0: loss = 137.573 (* 1 = 137.573 loss)
I0318 05:29:51.293998  3501 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0318 05:29:58.626240  3501 solver.cpp:228] Iteration 900, loss = 97.928
I0318 05:29:58.626307  3501 solver.cpp:244]     Train net output #0: loss = 97.9279 (* 1 = 97.9279 loss)
I0318 05:29:58.626319  3501 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0318 05:30:05.959136  3501 solver.cpp:228] Iteration 920, loss = 93.7721
I0318 05:30:05.959199  3501 solver.cpp:244]     Train net output #0: loss = 93.7721 (* 1 = 93.7721 loss)
I0318 05:30:05.959213  3501 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0318 05:30:13.301105  3501 solver.cpp:228] Iteration 940, loss = 38.8797
I0318 05:30:13.301208  3501 solver.cpp:244]     Train net output #0: loss = 38.8797 (* 1 = 38.8797 loss)
I0318 05:30:13.301221  3501 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0318 05:30:20.646417  3501 solver.cpp:228] Iteration 960, loss = 85.3563
I0318 05:30:20.646492  3501 solver.cpp:244]     Train net output #0: loss = 85.3563 (* 1 = 85.3563 loss)
I0318 05:30:20.646507  3501 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0318 05:30:27.995121  3501 solver.cpp:228] Iteration 980, loss = 77.61
I0318 05:30:27.995199  3501 solver.cpp:244]     Train net output #0: loss = 77.61 (* 1 = 77.61 loss)
I0318 05:30:27.995214  3501 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0318 05:30:34.969086  3501 solver.cpp:337] Iteration 1000, Testing net (#0)
I0318 05:32:29.172583  3501 solver.cpp:404]     Test net output #0: loss = 684.178 (* 1 = 684.178 loss)
I0318 05:32:29.172704  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 05:32:29.509661  3501 solver.cpp:228] Iteration 1000, loss = 116.541
I0318 05:32:29.509724  3501 solver.cpp:244]     Train net output #0: loss = 116.541 (* 1 = 116.541 loss)
I0318 05:32:29.509738  3501 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0318 05:32:36.741098  3501 solver.cpp:228] Iteration 1020, loss = 69.5057
I0318 05:32:36.741163  3501 solver.cpp:244]     Train net output #0: loss = 69.5057 (* 1 = 69.5057 loss)
I0318 05:32:36.741178  3501 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0318 05:32:44.027935  3501 solver.cpp:228] Iteration 1040, loss = 104.164
I0318 05:32:44.028002  3501 solver.cpp:244]     Train net output #0: loss = 104.164 (* 1 = 104.164 loss)
I0318 05:32:44.028017  3501 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0318 05:32:51.347901  3501 solver.cpp:228] Iteration 1060, loss = 100.073
I0318 05:32:51.347967  3501 solver.cpp:244]     Train net output #0: loss = 100.073 (* 1 = 100.073 loss)
I0318 05:32:51.347980  3501 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0318 05:32:58.681931  3501 solver.cpp:228] Iteration 1080, loss = 103.016
I0318 05:32:58.682001  3501 solver.cpp:244]     Train net output #0: loss = 103.016 (* 1 = 103.016 loss)
I0318 05:32:58.682016  3501 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0318 05:33:06.013104  3501 solver.cpp:228] Iteration 1100, loss = 93.8834
I0318 05:33:06.013244  3501 solver.cpp:244]     Train net output #0: loss = 93.8834 (* 1 = 93.8834 loss)
I0318 05:33:06.013259  3501 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0318 05:33:13.343302  3501 solver.cpp:228] Iteration 1120, loss = 19.8449
I0318 05:33:13.343366  3501 solver.cpp:244]     Train net output #0: loss = 19.8449 (* 1 = 19.8449 loss)
I0318 05:33:13.343380  3501 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0318 05:33:20.680491  3501 solver.cpp:228] Iteration 1140, loss = 116.247
I0318 05:33:20.680563  3501 solver.cpp:244]     Train net output #0: loss = 116.247 (* 1 = 116.247 loss)
I0318 05:33:20.680577  3501 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0318 05:33:28.015317  3501 solver.cpp:228] Iteration 1160, loss = 69.1674
I0318 05:33:28.015383  3501 solver.cpp:244]     Train net output #0: loss = 69.1674 (* 1 = 69.1674 loss)
I0318 05:33:28.015398  3501 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0318 05:33:35.351398  3501 solver.cpp:228] Iteration 1180, loss = 53.8928
I0318 05:33:35.351467  3501 solver.cpp:244]     Train net output #0: loss = 53.8928 (* 1 = 53.8928 loss)
I0318 05:33:35.351481  3501 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0318 05:33:42.684108  3501 solver.cpp:228] Iteration 1200, loss = 58.9055
I0318 05:33:42.684280  3501 solver.cpp:244]     Train net output #0: loss = 58.9054 (* 1 = 58.9054 loss)
I0318 05:33:42.684295  3501 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0318 05:33:50.017757  3501 solver.cpp:228] Iteration 1220, loss = 98.8051
I0318 05:33:50.017820  3501 solver.cpp:244]     Train net output #0: loss = 98.8051 (* 1 = 98.8051 loss)
I0318 05:33:50.017835  3501 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0318 05:33:57.348126  3501 solver.cpp:228] Iteration 1240, loss = 118.365
I0318 05:33:57.348196  3501 solver.cpp:244]     Train net output #0: loss = 118.364 (* 1 = 118.364 loss)
I0318 05:33:57.348209  3501 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0318 05:34:00.646663  3501 solver.cpp:337] Iteration 1250, Testing net (#0)
I0318 05:35:54.878598  3501 solver.cpp:404]     Test net output #0: loss = 719.6 (* 1 = 719.6 loss)
I0318 05:35:54.878721  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 05:35:58.836921  3501 solver.cpp:228] Iteration 1260, loss = 100.421
I0318 05:35:58.836987  3501 solver.cpp:244]     Train net output #0: loss = 100.421 (* 1 = 100.421 loss)
I0318 05:35:58.837000  3501 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0318 05:36:06.118926  3501 solver.cpp:228] Iteration 1280, loss = 99.5574
I0318 05:36:06.119000  3501 solver.cpp:244]     Train net output #0: loss = 99.5573 (* 1 = 99.5573 loss)
I0318 05:36:06.119017  3501 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0318 05:36:13.425166  3501 solver.cpp:228] Iteration 1300, loss = 71.6943
I0318 05:36:13.425232  3501 solver.cpp:244]     Train net output #0: loss = 71.6943 (* 1 = 71.6943 loss)
I0318 05:36:13.425246  3501 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0318 05:36:20.758191  3501 solver.cpp:228] Iteration 1320, loss = 66.3699
I0318 05:36:20.758257  3501 solver.cpp:244]     Train net output #0: loss = 66.3698 (* 1 = 66.3698 loss)
I0318 05:36:20.758275  3501 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0318 05:36:28.089663  3501 solver.cpp:228] Iteration 1340, loss = 68.0805
I0318 05:36:28.089789  3501 solver.cpp:244]     Train net output #0: loss = 68.0804 (* 1 = 68.0804 loss)
I0318 05:36:28.089804  3501 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0318 05:36:35.431299  3501 solver.cpp:228] Iteration 1360, loss = 91.3325
I0318 05:36:35.431363  3501 solver.cpp:244]     Train net output #0: loss = 91.3324 (* 1 = 91.3324 loss)
I0318 05:36:35.431377  3501 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0318 05:36:42.765764  3501 solver.cpp:228] Iteration 1380, loss = 90.6828
I0318 05:36:42.765830  3501 solver.cpp:244]     Train net output #0: loss = 90.6827 (* 1 = 90.6827 loss)
I0318 05:36:42.765843  3501 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0318 05:36:50.097991  3501 solver.cpp:228] Iteration 1400, loss = 113.552
I0318 05:36:50.098065  3501 solver.cpp:244]     Train net output #0: loss = 113.552 (* 1 = 113.552 loss)
I0318 05:36:50.098081  3501 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0318 05:36:57.440371  3501 solver.cpp:228] Iteration 1420, loss = 96.0262
I0318 05:36:57.440443  3501 solver.cpp:244]     Train net output #0: loss = 96.0261 (* 1 = 96.0261 loss)
I0318 05:36:57.440457  3501 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0318 05:37:04.773993  3501 solver.cpp:228] Iteration 1440, loss = 113.961
I0318 05:37:04.774157  3501 solver.cpp:244]     Train net output #0: loss = 113.961 (* 1 = 113.961 loss)
I0318 05:37:04.774171  3501 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0318 05:37:12.104918  3501 solver.cpp:228] Iteration 1460, loss = 87.2463
I0318 05:37:12.104984  3501 solver.cpp:244]     Train net output #0: loss = 87.2463 (* 1 = 87.2463 loss)
I0318 05:37:12.104997  3501 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0318 05:37:19.441723  3501 solver.cpp:228] Iteration 1480, loss = 50.7634
I0318 05:37:19.441788  3501 solver.cpp:244]     Train net output #0: loss = 50.7633 (* 1 = 50.7633 loss)
I0318 05:37:19.441803  3501 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0318 05:37:26.415889  3501 solver.cpp:337] Iteration 1500, Testing net (#0)
I0318 05:39:20.610440  3501 solver.cpp:404]     Test net output #0: loss = 697.229 (* 1 = 697.229 loss)
I0318 05:39:20.610561  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 05:39:20.946797  3501 solver.cpp:228] Iteration 1500, loss = 84.4026
I0318 05:39:20.946862  3501 solver.cpp:244]     Train net output #0: loss = 84.4025 (* 1 = 84.4025 loss)
I0318 05:39:20.946877  3501 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0318 05:39:28.178100  3501 solver.cpp:228] Iteration 1520, loss = 46.3369
I0318 05:39:28.178164  3501 solver.cpp:244]     Train net output #0: loss = 46.3368 (* 1 = 46.3368 loss)
I0318 05:39:28.178179  3501 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0318 05:39:35.452673  3501 solver.cpp:228] Iteration 1540, loss = 51.5776
I0318 05:39:35.452739  3501 solver.cpp:244]     Train net output #0: loss = 51.5775 (* 1 = 51.5775 loss)
I0318 05:39:35.452754  3501 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0318 05:39:42.764264  3501 solver.cpp:228] Iteration 1560, loss = 68.4206
I0318 05:39:42.764345  3501 solver.cpp:244]     Train net output #0: loss = 68.4206 (* 1 = 68.4206 loss)
I0318 05:39:42.764360  3501 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0318 05:39:50.101640  3501 solver.cpp:228] Iteration 1580, loss = 105.715
I0318 05:39:50.101706  3501 solver.cpp:244]     Train net output #0: loss = 105.715 (* 1 = 105.715 loss)
I0318 05:39:50.101718  3501 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0318 05:39:57.430037  3501 solver.cpp:228] Iteration 1600, loss = 87.8551
I0318 05:39:57.430167  3501 solver.cpp:244]     Train net output #0: loss = 87.8551 (* 1 = 87.8551 loss)
I0318 05:39:57.430182  3501 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0318 05:40:04.763924  3501 solver.cpp:228] Iteration 1620, loss = 102.044
I0318 05:40:04.763995  3501 solver.cpp:244]     Train net output #0: loss = 102.044 (* 1 = 102.044 loss)
I0318 05:40:04.764009  3501 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0318 05:40:12.099196  3501 solver.cpp:228] Iteration 1640, loss = 60.9494
I0318 05:40:12.099263  3501 solver.cpp:244]     Train net output #0: loss = 60.9493 (* 1 = 60.9493 loss)
I0318 05:40:12.099277  3501 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0318 05:40:19.429167  3501 solver.cpp:228] Iteration 1660, loss = 45.1869
I0318 05:40:19.429232  3501 solver.cpp:244]     Train net output #0: loss = 45.1868 (* 1 = 45.1868 loss)
I0318 05:40:19.429246  3501 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0318 05:40:26.769417  3501 solver.cpp:228] Iteration 1680, loss = 72.1446
I0318 05:40:26.769479  3501 solver.cpp:244]     Train net output #0: loss = 72.1445 (* 1 = 72.1445 loss)
I0318 05:40:26.769493  3501 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0318 05:40:34.104033  3501 solver.cpp:228] Iteration 1700, loss = 40.7527
I0318 05:40:34.104135  3501 solver.cpp:244]     Train net output #0: loss = 40.7526 (* 1 = 40.7526 loss)
I0318 05:40:34.104148  3501 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0318 05:40:41.432132  3501 solver.cpp:228] Iteration 1720, loss = 99.3915
I0318 05:40:41.432198  3501 solver.cpp:244]     Train net output #0: loss = 99.3915 (* 1 = 99.3915 loss)
I0318 05:40:41.432211  3501 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0318 05:40:48.759855  3501 solver.cpp:228] Iteration 1740, loss = 78.6291
I0318 05:40:48.759920  3501 solver.cpp:244]     Train net output #0: loss = 78.6291 (* 1 = 78.6291 loss)
I0318 05:40:48.759934  3501 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0318 05:40:52.061439  3501 solver.cpp:337] Iteration 1750, Testing net (#0)
I0318 05:42:46.278854  3501 solver.cpp:404]     Test net output #0: loss = 668.796 (* 1 = 668.796 loss)
I0318 05:42:46.279008  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:42:50.234808  3501 solver.cpp:228] Iteration 1760, loss = 105.789
I0318 05:42:50.234874  3501 solver.cpp:244]     Train net output #0: loss = 105.789 (* 1 = 105.789 loss)
I0318 05:42:50.234889  3501 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0318 05:42:57.518002  3501 solver.cpp:228] Iteration 1780, loss = 135.046
I0318 05:42:57.518066  3501 solver.cpp:244]     Train net output #0: loss = 135.046 (* 1 = 135.046 loss)
I0318 05:42:57.518080  3501 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0318 05:43:04.834169  3501 solver.cpp:228] Iteration 1800, loss = 88.1173
I0318 05:43:04.834234  3501 solver.cpp:244]     Train net output #0: loss = 88.1173 (* 1 = 88.1173 loss)
I0318 05:43:04.834249  3501 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0318 05:43:12.165977  3501 solver.cpp:228] Iteration 1820, loss = 83.3372
I0318 05:43:12.166043  3501 solver.cpp:244]     Train net output #0: loss = 83.3371 (* 1 = 83.3371 loss)
I0318 05:43:12.166056  3501 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0318 05:43:19.505735  3501 solver.cpp:228] Iteration 1840, loss = 27.1651
I0318 05:43:19.505867  3501 solver.cpp:244]     Train net output #0: loss = 27.165 (* 1 = 27.165 loss)
I0318 05:43:19.505882  3501 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0318 05:43:26.854984  3501 solver.cpp:228] Iteration 1860, loss = 79.1217
I0318 05:43:26.855051  3501 solver.cpp:244]     Train net output #0: loss = 79.1216 (* 1 = 79.1216 loss)
I0318 05:43:26.855064  3501 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0318 05:43:34.198395  3501 solver.cpp:228] Iteration 1880, loss = 53.6713
I0318 05:43:34.198460  3501 solver.cpp:244]     Train net output #0: loss = 53.6712 (* 1 = 53.6712 loss)
I0318 05:43:34.198474  3501 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0318 05:43:41.533443  3501 solver.cpp:228] Iteration 1900, loss = 80.3935
I0318 05:43:41.533511  3501 solver.cpp:244]     Train net output #0: loss = 80.3935 (* 1 = 80.3935 loss)
I0318 05:43:41.533524  3501 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0318 05:43:48.870311  3501 solver.cpp:228] Iteration 1920, loss = 72.1189
I0318 05:43:48.870378  3501 solver.cpp:244]     Train net output #0: loss = 72.1189 (* 1 = 72.1189 loss)
I0318 05:43:48.870391  3501 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0318 05:43:56.203197  3501 solver.cpp:228] Iteration 1940, loss = 162.1
I0318 05:43:56.203336  3501 solver.cpp:244]     Train net output #0: loss = 162.1 (* 1 = 162.1 loss)
I0318 05:43:56.203349  3501 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0318 05:44:03.542860  3501 solver.cpp:228] Iteration 1960, loss = 65.6718
I0318 05:44:03.542922  3501 solver.cpp:244]     Train net output #0: loss = 65.6717 (* 1 = 65.6717 loss)
I0318 05:44:03.542937  3501 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0318 05:44:10.879381  3501 solver.cpp:228] Iteration 1980, loss = 145.306
I0318 05:44:10.879446  3501 solver.cpp:244]     Train net output #0: loss = 145.306 (* 1 = 145.306 loss)
I0318 05:44:10.879461  3501 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0318 05:44:17.850261  3501 solver.cpp:337] Iteration 2000, Testing net (#0)
I0318 05:46:12.035043  3501 solver.cpp:404]     Test net output #0: loss = 679.644 (* 1 = 679.644 loss)
I0318 05:46:12.035128  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:46:12.373738  3501 solver.cpp:228] Iteration 2000, loss = 87.0948
I0318 05:46:12.373813  3501 solver.cpp:244]     Train net output #0: loss = 87.0947 (* 1 = 87.0947 loss)
I0318 05:46:12.373829  3501 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0318 05:46:19.637750  3501 solver.cpp:228] Iteration 2020, loss = 53.805
I0318 05:46:19.637845  3501 solver.cpp:244]     Train net output #0: loss = 53.8049 (* 1 = 53.8049 loss)
I0318 05:46:19.637869  3501 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0318 05:46:26.937232  3501 solver.cpp:228] Iteration 2040, loss = 51.3532
I0318 05:46:26.937300  3501 solver.cpp:244]     Train net output #0: loss = 51.3531 (* 1 = 51.3531 loss)
I0318 05:46:26.937314  3501 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0318 05:46:34.245959  3501 solver.cpp:228] Iteration 2060, loss = 84.5597
I0318 05:46:34.246027  3501 solver.cpp:244]     Train net output #0: loss = 84.5597 (* 1 = 84.5597 loss)
I0318 05:46:34.246042  3501 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0318 05:46:41.573279  3501 solver.cpp:228] Iteration 2080, loss = 98.5255
I0318 05:46:41.573345  3501 solver.cpp:244]     Train net output #0: loss = 98.5254 (* 1 = 98.5254 loss)
I0318 05:46:41.573359  3501 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0318 05:46:48.910116  3501 solver.cpp:228] Iteration 2100, loss = 52.1529
I0318 05:46:48.910291  3501 solver.cpp:244]     Train net output #0: loss = 52.1529 (* 1 = 52.1529 loss)
I0318 05:46:48.910306  3501 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0318 05:46:56.236405  3501 solver.cpp:228] Iteration 2120, loss = 62.9944
I0318 05:46:56.236469  3501 solver.cpp:244]     Train net output #0: loss = 62.9943 (* 1 = 62.9943 loss)
I0318 05:46:56.236484  3501 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0318 05:47:03.562748  3501 solver.cpp:228] Iteration 2140, loss = 106.052
I0318 05:47:03.562813  3501 solver.cpp:244]     Train net output #0: loss = 106.052 (* 1 = 106.052 loss)
I0318 05:47:03.562827  3501 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0318 05:47:10.898676  3501 solver.cpp:228] Iteration 2160, loss = 72.8564
I0318 05:47:10.898741  3501 solver.cpp:244]     Train net output #0: loss = 72.8564 (* 1 = 72.8564 loss)
I0318 05:47:10.898754  3501 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0318 05:47:18.240773  3501 solver.cpp:228] Iteration 2180, loss = 94.24
I0318 05:47:18.240839  3501 solver.cpp:244]     Train net output #0: loss = 94.2399 (* 1 = 94.2399 loss)
I0318 05:47:18.240852  3501 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0318 05:47:25.581907  3501 solver.cpp:228] Iteration 2200, loss = 36.8198
I0318 05:47:25.582064  3501 solver.cpp:244]     Train net output #0: loss = 36.8198 (* 1 = 36.8198 loss)
I0318 05:47:25.582079  3501 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0318 05:47:32.919600  3501 solver.cpp:228] Iteration 2220, loss = 46.4156
I0318 05:47:32.919663  3501 solver.cpp:244]     Train net output #0: loss = 46.4155 (* 1 = 46.4155 loss)
I0318 05:47:32.919677  3501 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0318 05:47:40.250726  3501 solver.cpp:228] Iteration 2240, loss = 40.1581
I0318 05:47:40.250790  3501 solver.cpp:244]     Train net output #0: loss = 40.1581 (* 1 = 40.1581 loss)
I0318 05:47:40.250804  3501 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0318 05:47:43.554378  3501 solver.cpp:337] Iteration 2250, Testing net (#0)
I0318 05:49:37.754557  3501 solver.cpp:404]     Test net output #0: loss = 711.656 (* 1 = 711.656 loss)
I0318 05:49:37.754632  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 05:49:41.707110  3501 solver.cpp:228] Iteration 2260, loss = 98.7795
I0318 05:49:41.707175  3501 solver.cpp:244]     Train net output #0: loss = 98.7795 (* 1 = 98.7795 loss)
I0318 05:49:41.707188  3501 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0318 05:49:48.976828  3501 solver.cpp:228] Iteration 2280, loss = 52.7036
I0318 05:49:48.976892  3501 solver.cpp:244]     Train net output #0: loss = 52.7036 (* 1 = 52.7036 loss)
I0318 05:49:48.976907  3501 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0318 05:49:56.273417  3501 solver.cpp:228] Iteration 2300, loss = 87.4548
I0318 05:49:56.273486  3501 solver.cpp:244]     Train net output #0: loss = 87.4547 (* 1 = 87.4547 loss)
I0318 05:49:56.273499  3501 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0318 05:50:03.592455  3501 solver.cpp:228] Iteration 2320, loss = 103.895
I0318 05:50:03.592519  3501 solver.cpp:244]     Train net output #0: loss = 103.895 (* 1 = 103.895 loss)
I0318 05:50:03.592533  3501 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0318 05:50:10.922780  3501 solver.cpp:228] Iteration 2340, loss = 101.049
I0318 05:50:10.922958  3501 solver.cpp:244]     Train net output #0: loss = 101.049 (* 1 = 101.049 loss)
I0318 05:50:10.922974  3501 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0318 05:50:18.254333  3501 solver.cpp:228] Iteration 2360, loss = 73.8567
I0318 05:50:18.254400  3501 solver.cpp:244]     Train net output #0: loss = 73.8566 (* 1 = 73.8566 loss)
I0318 05:50:18.254413  3501 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0318 05:50:25.591217  3501 solver.cpp:228] Iteration 2380, loss = 68.8477
I0318 05:50:25.591282  3501 solver.cpp:244]     Train net output #0: loss = 68.8476 (* 1 = 68.8476 loss)
I0318 05:50:25.591296  3501 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0318 05:50:32.931629  3501 solver.cpp:228] Iteration 2400, loss = 42.7762
I0318 05:50:32.931694  3501 solver.cpp:244]     Train net output #0: loss = 42.7762 (* 1 = 42.7762 loss)
I0318 05:50:32.931707  3501 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0318 05:50:40.264560  3501 solver.cpp:228] Iteration 2420, loss = 35.873
I0318 05:50:40.264626  3501 solver.cpp:244]     Train net output #0: loss = 35.8729 (* 1 = 35.8729 loss)
I0318 05:50:40.264638  3501 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0318 05:50:47.602653  3501 solver.cpp:228] Iteration 2440, loss = 100.797
I0318 05:50:47.602798  3501 solver.cpp:244]     Train net output #0: loss = 100.797 (* 1 = 100.797 loss)
I0318 05:50:47.602814  3501 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0318 05:50:54.942935  3501 solver.cpp:228] Iteration 2460, loss = 51.3451
I0318 05:50:54.943009  3501 solver.cpp:244]     Train net output #0: loss = 51.3451 (* 1 = 51.3451 loss)
I0318 05:50:54.943024  3501 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0318 05:51:02.284317  3501 solver.cpp:228] Iteration 2480, loss = 79.6268
I0318 05:51:02.284387  3501 solver.cpp:244]     Train net output #0: loss = 79.6268 (* 1 = 79.6268 loss)
I0318 05:51:02.284404  3501 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0318 05:51:09.250608  3501 solver.cpp:337] Iteration 2500, Testing net (#0)
I0318 05:53:03.476841  3501 solver.cpp:404]     Test net output #0: loss = 708.666 (* 1 = 708.666 loss)
I0318 05:53:03.476918  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 05:53:03.813681  3501 solver.cpp:228] Iteration 2500, loss = 117.711
I0318 05:53:03.813747  3501 solver.cpp:244]     Train net output #0: loss = 117.711 (* 1 = 117.711 loss)
I0318 05:53:03.813761  3501 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0318 05:53:11.060564  3501 solver.cpp:228] Iteration 2520, loss = 141.237
I0318 05:53:11.060638  3501 solver.cpp:244]     Train net output #0: loss = 141.237 (* 1 = 141.237 loss)
I0318 05:53:11.060652  3501 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0318 05:53:18.361563  3501 solver.cpp:228] Iteration 2540, loss = 84.3044
I0318 05:53:18.361627  3501 solver.cpp:244]     Train net output #0: loss = 84.3044 (* 1 = 84.3044 loss)
I0318 05:53:18.361640  3501 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0318 05:53:25.681318  3501 solver.cpp:228] Iteration 2560, loss = 22.892
I0318 05:53:25.681385  3501 solver.cpp:244]     Train net output #0: loss = 22.892 (* 1 = 22.892 loss)
I0318 05:53:25.681397  3501 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0318 05:53:33.015080  3501 solver.cpp:228] Iteration 2580, loss = 33.9665
I0318 05:53:33.015141  3501 solver.cpp:244]     Train net output #0: loss = 33.9664 (* 1 = 33.9664 loss)
I0318 05:53:33.015156  3501 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0318 05:53:40.360015  3501 solver.cpp:228] Iteration 2600, loss = 46.0504
I0318 05:53:40.360198  3501 solver.cpp:244]     Train net output #0: loss = 46.0503 (* 1 = 46.0503 loss)
I0318 05:53:40.360213  3501 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0318 05:53:47.702921  3501 solver.cpp:228] Iteration 2620, loss = 100.111
I0318 05:53:47.702986  3501 solver.cpp:244]     Train net output #0: loss = 100.111 (* 1 = 100.111 loss)
I0318 05:53:47.702999  3501 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0318 05:53:55.036000  3501 solver.cpp:228] Iteration 2640, loss = 61.4832
I0318 05:53:55.036064  3501 solver.cpp:244]     Train net output #0: loss = 61.4831 (* 1 = 61.4831 loss)
I0318 05:53:55.036078  3501 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0318 05:54:02.373286  3501 solver.cpp:228] Iteration 2660, loss = 58.1438
I0318 05:54:02.373350  3501 solver.cpp:244]     Train net output #0: loss = 58.1437 (* 1 = 58.1437 loss)
I0318 05:54:02.373363  3501 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0318 05:54:09.693928  3501 solver.cpp:228] Iteration 2680, loss = 77.1797
I0318 05:54:09.693994  3501 solver.cpp:244]     Train net output #0: loss = 77.1797 (* 1 = 77.1797 loss)
I0318 05:54:09.694006  3501 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0318 05:54:17.028789  3501 solver.cpp:228] Iteration 2700, loss = 82.2102
I0318 05:54:17.028940  3501 solver.cpp:244]     Train net output #0: loss = 82.2101 (* 1 = 82.2101 loss)
I0318 05:54:17.028957  3501 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0318 05:54:24.363862  3501 solver.cpp:228] Iteration 2720, loss = 39.1452
I0318 05:54:24.363925  3501 solver.cpp:244]     Train net output #0: loss = 39.1452 (* 1 = 39.1452 loss)
I0318 05:54:24.363940  3501 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0318 05:54:31.695755  3501 solver.cpp:228] Iteration 2740, loss = 44.3028
I0318 05:54:31.695818  3501 solver.cpp:244]     Train net output #0: loss = 44.3027 (* 1 = 44.3027 loss)
I0318 05:54:31.695832  3501 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0318 05:54:34.996670  3501 solver.cpp:337] Iteration 2750, Testing net (#0)
I0318 05:56:29.200413  3501 solver.cpp:404]     Test net output #0: loss = 695.896 (* 1 = 695.896 loss)
I0318 05:56:29.200551  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:56:33.150662  3501 solver.cpp:228] Iteration 2760, loss = 36.3218
I0318 05:56:33.150722  3501 solver.cpp:244]     Train net output #0: loss = 36.3217 (* 1 = 36.3217 loss)
I0318 05:56:33.150735  3501 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0318 05:56:40.426292  3501 solver.cpp:228] Iteration 2780, loss = 40.3642
I0318 05:56:40.426367  3501 solver.cpp:244]     Train net output #0: loss = 40.3641 (* 1 = 40.3641 loss)
I0318 05:56:40.426381  3501 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0318 05:56:47.733547  3501 solver.cpp:228] Iteration 2800, loss = 100.657
I0318 05:56:47.733619  3501 solver.cpp:244]     Train net output #0: loss = 100.657 (* 1 = 100.657 loss)
I0318 05:56:47.733634  3501 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0318 05:56:55.048859  3501 solver.cpp:228] Iteration 2820, loss = 68.6969
I0318 05:56:55.048926  3501 solver.cpp:244]     Train net output #0: loss = 68.6969 (* 1 = 68.6969 loss)
I0318 05:56:55.048940  3501 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0318 05:57:02.385259  3501 solver.cpp:228] Iteration 2840, loss = 73.7181
I0318 05:57:02.385409  3501 solver.cpp:244]     Train net output #0: loss = 73.718 (* 1 = 73.718 loss)
I0318 05:57:02.385426  3501 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I0318 05:57:09.717634  3501 solver.cpp:228] Iteration 2860, loss = 114.502
I0318 05:57:09.717701  3501 solver.cpp:244]     Train net output #0: loss = 114.502 (* 1 = 114.502 loss)
I0318 05:57:09.717715  3501 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I0318 05:57:17.053864  3501 solver.cpp:228] Iteration 2880, loss = 63.5639
I0318 05:57:17.053930  3501 solver.cpp:244]     Train net output #0: loss = 63.5638 (* 1 = 63.5638 loss)
I0318 05:57:17.053943  3501 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I0318 05:57:24.394891  3501 solver.cpp:228] Iteration 2900, loss = 66.6438
I0318 05:57:24.394954  3501 solver.cpp:244]     Train net output #0: loss = 66.6437 (* 1 = 66.6437 loss)
I0318 05:57:24.394968  3501 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0318 05:57:31.740864  3501 solver.cpp:228] Iteration 2920, loss = 35.0945
I0318 05:57:31.740928  3501 solver.cpp:244]     Train net output #0: loss = 35.0944 (* 1 = 35.0944 loss)
I0318 05:57:31.740942  3501 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I0318 05:57:39.076843  3501 solver.cpp:228] Iteration 2940, loss = 71.6693
I0318 05:57:39.077034  3501 solver.cpp:244]     Train net output #0: loss = 71.6692 (* 1 = 71.6692 loss)
I0318 05:57:39.077049  3501 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I0318 05:57:46.406425  3501 solver.cpp:228] Iteration 2960, loss = 47.1345
I0318 05:57:46.406497  3501 solver.cpp:244]     Train net output #0: loss = 47.1344 (* 1 = 47.1344 loss)
I0318 05:57:46.406509  3501 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I0318 05:57:53.741307  3501 solver.cpp:228] Iteration 2980, loss = 99.3356
I0318 05:57:53.741372  3501 solver.cpp:244]     Train net output #0: loss = 99.3355 (* 1 = 99.3355 loss)
I0318 05:57:53.741385  3501 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I0318 05:58:00.712893  3501 solver.cpp:337] Iteration 3000, Testing net (#0)
I0318 05:59:54.923635  3501 solver.cpp:404]     Test net output #0: loss = 677.581 (* 1 = 677.581 loss)
I0318 05:59:54.923776  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 05:59:55.261181  3501 solver.cpp:228] Iteration 3000, loss = 57.5553
I0318 05:59:55.261245  3501 solver.cpp:244]     Train net output #0: loss = 57.5552 (* 1 = 57.5552 loss)
I0318 05:59:55.261258  3501 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0318 06:00:02.519431  3501 solver.cpp:228] Iteration 3020, loss = 82.131
I0318 06:00:02.519501  3501 solver.cpp:244]     Train net output #0: loss = 82.1309 (* 1 = 82.1309 loss)
I0318 06:00:02.519515  3501 sgd_solver.cpp:106] Iteration 3020, lr = 0.01
I0318 06:00:09.809542  3501 solver.cpp:228] Iteration 3040, loss = 62.8105
I0318 06:00:09.809607  3501 solver.cpp:244]     Train net output #0: loss = 62.8104 (* 1 = 62.8104 loss)
I0318 06:00:09.809622  3501 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I0318 06:00:17.118675  3501 solver.cpp:228] Iteration 3060, loss = 85.1671
I0318 06:00:17.118755  3501 solver.cpp:244]     Train net output #0: loss = 85.167 (* 1 = 85.167 loss)
I0318 06:00:17.118772  3501 sgd_solver.cpp:106] Iteration 3060, lr = 0.01
I0318 06:00:24.447935  3501 solver.cpp:228] Iteration 3080, loss = 61.0068
I0318 06:00:24.447999  3501 solver.cpp:244]     Train net output #0: loss = 61.0067 (* 1 = 61.0067 loss)
I0318 06:00:24.448012  3501 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I0318 06:00:31.779917  3501 solver.cpp:228] Iteration 3100, loss = 20.3926
I0318 06:00:31.780063  3501 solver.cpp:244]     Train net output #0: loss = 20.3925 (* 1 = 20.3925 loss)
I0318 06:00:31.780077  3501 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0318 06:00:39.113175  3501 solver.cpp:228] Iteration 3120, loss = 37.8385
I0318 06:00:39.113240  3501 solver.cpp:244]     Train net output #0: loss = 37.8384 (* 1 = 37.8384 loss)
I0318 06:00:39.113253  3501 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I0318 06:00:46.453157  3501 solver.cpp:228] Iteration 3140, loss = 89.745
I0318 06:00:46.453220  3501 solver.cpp:244]     Train net output #0: loss = 89.7449 (* 1 = 89.7449 loss)
I0318 06:00:46.453234  3501 sgd_solver.cpp:106] Iteration 3140, lr = 0.01
I0318 06:00:53.785809  3501 solver.cpp:228] Iteration 3160, loss = 79.6801
I0318 06:00:53.785873  3501 solver.cpp:244]     Train net output #0: loss = 79.6799 (* 1 = 79.6799 loss)
I0318 06:00:53.785887  3501 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I0318 06:01:01.126327  3501 solver.cpp:228] Iteration 3180, loss = 79.0779
I0318 06:01:01.126399  3501 solver.cpp:244]     Train net output #0: loss = 79.0778 (* 1 = 79.0778 loss)
I0318 06:01:01.126413  3501 sgd_solver.cpp:106] Iteration 3180, lr = 0.01
I0318 06:01:08.467473  3501 solver.cpp:228] Iteration 3200, loss = 83.0081
I0318 06:01:08.467658  3501 solver.cpp:244]     Train net output #0: loss = 83.0079 (* 1 = 83.0079 loss)
I0318 06:01:08.467672  3501 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0318 06:01:15.798899  3501 solver.cpp:228] Iteration 3220, loss = 70.1091
I0318 06:01:15.798964  3501 solver.cpp:244]     Train net output #0: loss = 70.109 (* 1 = 70.109 loss)
I0318 06:01:15.798979  3501 sgd_solver.cpp:106] Iteration 3220, lr = 0.01
I0318 06:01:23.124768  3501 solver.cpp:228] Iteration 3240, loss = 64.0944
I0318 06:01:23.124830  3501 solver.cpp:244]     Train net output #0: loss = 64.0943 (* 1 = 64.0943 loss)
I0318 06:01:23.124842  3501 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I0318 06:01:26.424576  3501 solver.cpp:337] Iteration 3250, Testing net (#0)
I0318 06:03:20.673398  3501 solver.cpp:404]     Test net output #0: loss = 665.166 (* 1 = 665.166 loss)
I0318 06:03:20.673527  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:03:24.618170  3501 solver.cpp:228] Iteration 3260, loss = 42.9765
I0318 06:03:24.618233  3501 solver.cpp:244]     Train net output #0: loss = 42.9764 (* 1 = 42.9764 loss)
I0318 06:03:24.618247  3501 sgd_solver.cpp:106] Iteration 3260, lr = 0.01
I0318 06:03:31.890700  3501 solver.cpp:228] Iteration 3280, loss = 74.417
I0318 06:03:31.890764  3501 solver.cpp:244]     Train net output #0: loss = 74.4169 (* 1 = 74.4169 loss)
I0318 06:03:31.890779  3501 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I0318 06:03:39.187580  3501 solver.cpp:228] Iteration 3300, loss = 55.1384
I0318 06:03:39.187645  3501 solver.cpp:244]     Train net output #0: loss = 55.1383 (* 1 = 55.1383 loss)
I0318 06:03:39.187659  3501 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0318 06:03:46.503088  3501 solver.cpp:228] Iteration 3320, loss = 74.9629
I0318 06:03:46.503152  3501 solver.cpp:244]     Train net output #0: loss = 74.9628 (* 1 = 74.9628 loss)
I0318 06:03:46.503167  3501 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I0318 06:03:53.835045  3501 solver.cpp:228] Iteration 3340, loss = 72.0603
I0318 06:03:53.835192  3501 solver.cpp:244]     Train net output #0: loss = 72.0602 (* 1 = 72.0602 loss)
I0318 06:03:53.835209  3501 sgd_solver.cpp:106] Iteration 3340, lr = 0.01
I0318 06:04:01.173467  3501 solver.cpp:228] Iteration 3360, loss = 46.3835
I0318 06:04:01.173532  3501 solver.cpp:244]     Train net output #0: loss = 46.3834 (* 1 = 46.3834 loss)
I0318 06:04:01.173545  3501 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I0318 06:04:08.506484  3501 solver.cpp:228] Iteration 3380, loss = 61.803
I0318 06:04:08.506556  3501 solver.cpp:244]     Train net output #0: loss = 61.8029 (* 1 = 61.8029 loss)
I0318 06:04:08.506569  3501 sgd_solver.cpp:106] Iteration 3380, lr = 0.01
I0318 06:04:15.839578  3501 solver.cpp:228] Iteration 3400, loss = 129.814
I0318 06:04:15.839643  3501 solver.cpp:244]     Train net output #0: loss = 129.813 (* 1 = 129.813 loss)
I0318 06:04:15.839658  3501 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0318 06:04:23.176342  3501 solver.cpp:228] Iteration 3420, loss = 115.723
I0318 06:04:23.176401  3501 solver.cpp:244]     Train net output #0: loss = 115.723 (* 1 = 115.723 loss)
I0318 06:04:23.176415  3501 sgd_solver.cpp:106] Iteration 3420, lr = 0.01
I0318 06:04:30.502929  3501 solver.cpp:228] Iteration 3440, loss = 86.1554
I0318 06:04:30.503082  3501 solver.cpp:244]     Train net output #0: loss = 86.1553 (* 1 = 86.1553 loss)
I0318 06:04:30.503101  3501 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
I0318 06:04:37.840000  3501 solver.cpp:228] Iteration 3460, loss = 17.2832
I0318 06:04:37.840067  3501 solver.cpp:244]     Train net output #0: loss = 17.2831 (* 1 = 17.2831 loss)
I0318 06:04:37.840081  3501 sgd_solver.cpp:106] Iteration 3460, lr = 0.01
I0318 06:04:45.171037  3501 solver.cpp:228] Iteration 3480, loss = 21.1737
I0318 06:04:45.171102  3501 solver.cpp:244]     Train net output #0: loss = 21.1736 (* 1 = 21.1736 loss)
I0318 06:04:45.171115  3501 sgd_solver.cpp:106] Iteration 3480, lr = 0.01
I0318 06:04:52.136382  3501 solver.cpp:337] Iteration 3500, Testing net (#0)
I0318 06:06:46.337255  3501 solver.cpp:404]     Test net output #0: loss = 693.695 (* 1 = 693.695 loss)
I0318 06:06:46.337410  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 06:06:46.674505  3501 solver.cpp:228] Iteration 3500, loss = 27.2895
I0318 06:06:46.674569  3501 solver.cpp:244]     Train net output #0: loss = 27.2894 (* 1 = 27.2894 loss)
I0318 06:06:46.674582  3501 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I0318 06:06:53.927100  3501 solver.cpp:228] Iteration 3520, loss = 60.3404
I0318 06:06:53.927163  3501 solver.cpp:244]     Train net output #0: loss = 60.3404 (* 1 = 60.3404 loss)
I0318 06:06:53.927177  3501 sgd_solver.cpp:106] Iteration 3520, lr = 0.01
I0318 06:07:01.230556  3501 solver.cpp:228] Iteration 3540, loss = 45.6521
I0318 06:07:01.230619  3501 solver.cpp:244]     Train net output #0: loss = 45.652 (* 1 = 45.652 loss)
I0318 06:07:01.230633  3501 sgd_solver.cpp:106] Iteration 3540, lr = 0.01
I0318 06:07:08.553021  3501 solver.cpp:228] Iteration 3560, loss = 86.1191
I0318 06:07:08.553097  3501 solver.cpp:244]     Train net output #0: loss = 86.1191 (* 1 = 86.1191 loss)
I0318 06:07:08.553112  3501 sgd_solver.cpp:106] Iteration 3560, lr = 0.01
I0318 06:07:15.886466  3501 solver.cpp:228] Iteration 3580, loss = 103.723
I0318 06:07:15.886543  3501 solver.cpp:244]     Train net output #0: loss = 103.723 (* 1 = 103.723 loss)
I0318 06:07:15.886559  3501 sgd_solver.cpp:106] Iteration 3580, lr = 0.01
I0318 06:07:23.225023  3501 solver.cpp:228] Iteration 3600, loss = 104.799
I0318 06:07:23.225126  3501 solver.cpp:244]     Train net output #0: loss = 104.799 (* 1 = 104.799 loss)
I0318 06:07:23.225141  3501 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I0318 06:07:30.562202  3501 solver.cpp:228] Iteration 3620, loss = 68.3461
I0318 06:07:30.562265  3501 solver.cpp:244]     Train net output #0: loss = 68.3461 (* 1 = 68.3461 loss)
I0318 06:07:30.562279  3501 sgd_solver.cpp:106] Iteration 3620, lr = 0.01
I0318 06:07:37.896270  3501 solver.cpp:228] Iteration 3640, loss = 53.1224
I0318 06:07:37.896347  3501 solver.cpp:244]     Train net output #0: loss = 53.1223 (* 1 = 53.1223 loss)
I0318 06:07:37.896360  3501 sgd_solver.cpp:106] Iteration 3640, lr = 0.01
I0318 06:07:45.231356  3501 solver.cpp:228] Iteration 3660, loss = 52.3362
I0318 06:07:45.231420  3501 solver.cpp:244]     Train net output #0: loss = 52.3361 (* 1 = 52.3361 loss)
I0318 06:07:45.231433  3501 sgd_solver.cpp:106] Iteration 3660, lr = 0.01
I0318 06:07:52.557049  3501 solver.cpp:228] Iteration 3680, loss = 27.2081
I0318 06:07:52.557114  3501 solver.cpp:244]     Train net output #0: loss = 27.208 (* 1 = 27.208 loss)
I0318 06:07:52.557128  3501 sgd_solver.cpp:106] Iteration 3680, lr = 0.01
I0318 06:07:59.887014  3501 solver.cpp:228] Iteration 3700, loss = 43.1986
I0318 06:07:59.887159  3501 solver.cpp:244]     Train net output #0: loss = 43.1985 (* 1 = 43.1985 loss)
I0318 06:07:59.887173  3501 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I0318 06:08:07.212527  3501 solver.cpp:228] Iteration 3720, loss = 57.6456
I0318 06:08:07.212594  3501 solver.cpp:244]     Train net output #0: loss = 57.6456 (* 1 = 57.6456 loss)
I0318 06:08:07.212606  3501 sgd_solver.cpp:106] Iteration 3720, lr = 0.01
I0318 06:08:14.545842  3501 solver.cpp:228] Iteration 3740, loss = 82.2621
I0318 06:08:14.545908  3501 solver.cpp:244]     Train net output #0: loss = 82.262 (* 1 = 82.262 loss)
I0318 06:08:14.545922  3501 sgd_solver.cpp:106] Iteration 3740, lr = 0.01
I0318 06:08:17.843137  3501 solver.cpp:337] Iteration 3750, Testing net (#0)
I0318 06:10:12.057404  3501 solver.cpp:404]     Test net output #0: loss = 672.728 (* 1 = 672.728 loss)
I0318 06:10:12.057529  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 06:10:16.014102  3501 solver.cpp:228] Iteration 3760, loss = 97.5222
I0318 06:10:16.014171  3501 solver.cpp:244]     Train net output #0: loss = 97.5221 (* 1 = 97.5221 loss)
I0318 06:10:16.014184  3501 sgd_solver.cpp:106] Iteration 3760, lr = 0.01
I0318 06:10:23.279243  3501 solver.cpp:228] Iteration 3780, loss = 68.4473
I0318 06:10:23.279302  3501 solver.cpp:244]     Train net output #0: loss = 68.4472 (* 1 = 68.4472 loss)
I0318 06:10:23.279317  3501 sgd_solver.cpp:106] Iteration 3780, lr = 0.01
I0318 06:10:30.586109  3501 solver.cpp:228] Iteration 3800, loss = 26.5092
I0318 06:10:30.586174  3501 solver.cpp:244]     Train net output #0: loss = 26.5091 (* 1 = 26.5091 loss)
I0318 06:10:30.586189  3501 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I0318 06:10:37.916790  3501 solver.cpp:228] Iteration 3820, loss = 55.2809
I0318 06:10:37.916853  3501 solver.cpp:244]     Train net output #0: loss = 55.2808 (* 1 = 55.2808 loss)
I0318 06:10:37.916867  3501 sgd_solver.cpp:106] Iteration 3820, lr = 0.01
I0318 06:10:45.253011  3501 solver.cpp:228] Iteration 3840, loss = 98.5103
I0318 06:10:45.253147  3501 solver.cpp:244]     Train net output #0: loss = 98.5102 (* 1 = 98.5102 loss)
I0318 06:10:45.253162  3501 sgd_solver.cpp:106] Iteration 3840, lr = 0.01
I0318 06:10:52.591962  3501 solver.cpp:228] Iteration 3860, loss = 39.936
I0318 06:10:52.592028  3501 solver.cpp:244]     Train net output #0: loss = 39.9359 (* 1 = 39.9359 loss)
I0318 06:10:52.592042  3501 sgd_solver.cpp:106] Iteration 3860, lr = 0.01
I0318 06:10:59.923251  3501 solver.cpp:228] Iteration 3880, loss = 72.4428
I0318 06:10:59.923319  3501 solver.cpp:244]     Train net output #0: loss = 72.4427 (* 1 = 72.4427 loss)
I0318 06:10:59.923333  3501 sgd_solver.cpp:106] Iteration 3880, lr = 0.01
I0318 06:11:07.253706  3501 solver.cpp:228] Iteration 3900, loss = 44.6876
I0318 06:11:07.253769  3501 solver.cpp:244]     Train net output #0: loss = 44.6875 (* 1 = 44.6875 loss)
I0318 06:11:07.253783  3501 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I0318 06:11:14.583492  3501 solver.cpp:228] Iteration 3920, loss = 84.5599
I0318 06:11:14.583556  3501 solver.cpp:244]     Train net output #0: loss = 84.5598 (* 1 = 84.5598 loss)
I0318 06:11:14.583570  3501 sgd_solver.cpp:106] Iteration 3920, lr = 0.01
I0318 06:11:21.904533  3501 solver.cpp:228] Iteration 3940, loss = 68.7984
I0318 06:11:21.904680  3501 solver.cpp:244]     Train net output #0: loss = 68.7983 (* 1 = 68.7983 loss)
I0318 06:11:21.904695  3501 sgd_solver.cpp:106] Iteration 3940, lr = 0.01
I0318 06:11:29.229691  3501 solver.cpp:228] Iteration 3960, loss = 144.257
I0318 06:11:29.229754  3501 solver.cpp:244]     Train net output #0: loss = 144.257 (* 1 = 144.257 loss)
I0318 06:11:29.229768  3501 sgd_solver.cpp:106] Iteration 3960, lr = 0.01
I0318 06:11:36.566045  3501 solver.cpp:228] Iteration 3980, loss = 45.6112
I0318 06:11:36.566108  3501 solver.cpp:244]     Train net output #0: loss = 45.6111 (* 1 = 45.6111 loss)
I0318 06:11:36.566121  3501 sgd_solver.cpp:106] Iteration 3980, lr = 0.01
I0318 06:11:43.536505  3501 solver.cpp:337] Iteration 4000, Testing net (#0)
I0318 06:13:37.735664  3501 solver.cpp:404]     Test net output #0: loss = 693.524 (* 1 = 693.524 loss)
I0318 06:13:37.735813  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:13:38.071501  3501 solver.cpp:228] Iteration 4000, loss = 53.8447
I0318 06:13:38.071568  3501 solver.cpp:244]     Train net output #0: loss = 53.8446 (* 1 = 53.8446 loss)
I0318 06:13:38.071583  3501 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0318 06:13:45.315228  3501 solver.cpp:228] Iteration 4020, loss = 41.9283
I0318 06:13:45.315292  3501 solver.cpp:244]     Train net output #0: loss = 41.9282 (* 1 = 41.9282 loss)
I0318 06:13:45.315306  3501 sgd_solver.cpp:106] Iteration 4020, lr = 0.01
I0318 06:13:52.594727  3501 solver.cpp:228] Iteration 4040, loss = 58.2287
I0318 06:13:52.594791  3501 solver.cpp:244]     Train net output #0: loss = 58.2286 (* 1 = 58.2286 loss)
I0318 06:13:52.594805  3501 sgd_solver.cpp:106] Iteration 4040, lr = 0.01
I0318 06:13:59.896728  3501 solver.cpp:228] Iteration 4060, loss = 92.7063
I0318 06:13:59.896791  3501 solver.cpp:244]     Train net output #0: loss = 92.7062 (* 1 = 92.7062 loss)
I0318 06:13:59.896805  3501 sgd_solver.cpp:106] Iteration 4060, lr = 0.01
I0318 06:14:07.222569  3501 solver.cpp:228] Iteration 4080, loss = 65.5714
I0318 06:14:07.222636  3501 solver.cpp:244]     Train net output #0: loss = 65.5713 (* 1 = 65.5713 loss)
I0318 06:14:07.222651  3501 sgd_solver.cpp:106] Iteration 4080, lr = 0.01
I0318 06:14:14.557368  3501 solver.cpp:228] Iteration 4100, loss = 105.558
I0318 06:14:14.557555  3501 solver.cpp:244]     Train net output #0: loss = 105.558 (* 1 = 105.558 loss)
I0318 06:14:14.557572  3501 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I0318 06:14:21.899231  3501 solver.cpp:228] Iteration 4120, loss = 83.4483
I0318 06:14:21.899296  3501 solver.cpp:244]     Train net output #0: loss = 83.4482 (* 1 = 83.4482 loss)
I0318 06:14:21.899308  3501 sgd_solver.cpp:106] Iteration 4120, lr = 0.01
I0318 06:14:29.240566  3501 solver.cpp:228] Iteration 4140, loss = 61.6735
I0318 06:14:29.240628  3501 solver.cpp:244]     Train net output #0: loss = 61.6734 (* 1 = 61.6734 loss)
I0318 06:14:29.240643  3501 sgd_solver.cpp:106] Iteration 4140, lr = 0.01
I0318 06:14:36.579372  3501 solver.cpp:228] Iteration 4160, loss = 52.3203
I0318 06:14:36.579435  3501 solver.cpp:244]     Train net output #0: loss = 52.3202 (* 1 = 52.3202 loss)
I0318 06:14:36.579448  3501 sgd_solver.cpp:106] Iteration 4160, lr = 0.01
I0318 06:14:43.914253  3501 solver.cpp:228] Iteration 4180, loss = 46.7127
I0318 06:14:43.914317  3501 solver.cpp:244]     Train net output #0: loss = 46.7126 (* 1 = 46.7126 loss)
I0318 06:14:43.914330  3501 sgd_solver.cpp:106] Iteration 4180, lr = 0.01
I0318 06:14:51.254323  3501 solver.cpp:228] Iteration 4200, loss = 68.9531
I0318 06:14:51.254467  3501 solver.cpp:244]     Train net output #0: loss = 68.953 (* 1 = 68.953 loss)
I0318 06:14:51.254480  3501 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I0318 06:14:58.590785  3501 solver.cpp:228] Iteration 4220, loss = 52.1953
I0318 06:14:58.590848  3501 solver.cpp:244]     Train net output #0: loss = 52.1952 (* 1 = 52.1952 loss)
I0318 06:14:58.590862  3501 sgd_solver.cpp:106] Iteration 4220, lr = 0.01
I0318 06:15:05.920248  3501 solver.cpp:228] Iteration 4240, loss = 52.1604
I0318 06:15:05.920325  3501 solver.cpp:244]     Train net output #0: loss = 52.1603 (* 1 = 52.1603 loss)
I0318 06:15:05.920342  3501 sgd_solver.cpp:106] Iteration 4240, lr = 0.01
I0318 06:15:09.218917  3501 solver.cpp:337] Iteration 4250, Testing net (#0)
I0318 06:17:03.442715  3501 solver.cpp:404]     Test net output #0: loss = 663.805 (* 1 = 663.805 loss)
I0318 06:17:03.442791  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 06:17:07.411751  3501 solver.cpp:228] Iteration 4260, loss = 56.2161
I0318 06:17:07.411815  3501 solver.cpp:244]     Train net output #0: loss = 56.216 (* 1 = 56.216 loss)
I0318 06:17:07.411829  3501 sgd_solver.cpp:106] Iteration 4260, lr = 0.01
I0318 06:17:14.712025  3501 solver.cpp:228] Iteration 4280, loss = 48.0093
I0318 06:17:14.712091  3501 solver.cpp:244]     Train net output #0: loss = 48.0092 (* 1 = 48.0092 loss)
I0318 06:17:14.712105  3501 sgd_solver.cpp:106] Iteration 4280, lr = 0.01
I0318 06:17:22.033047  3501 solver.cpp:228] Iteration 4300, loss = 113.315
I0318 06:17:22.033114  3501 solver.cpp:244]     Train net output #0: loss = 113.315 (* 1 = 113.315 loss)
I0318 06:17:22.033128  3501 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I0318 06:17:29.362413  3501 solver.cpp:228] Iteration 4320, loss = 51.3433
I0318 06:17:29.362489  3501 solver.cpp:244]     Train net output #0: loss = 51.3432 (* 1 = 51.3432 loss)
I0318 06:17:29.362505  3501 sgd_solver.cpp:106] Iteration 4320, lr = 0.01
I0318 06:17:36.698665  3501 solver.cpp:228] Iteration 4340, loss = 52.965
I0318 06:17:36.698822  3501 solver.cpp:244]     Train net output #0: loss = 52.9649 (* 1 = 52.9649 loss)
I0318 06:17:36.698838  3501 sgd_solver.cpp:106] Iteration 4340, lr = 0.01
I0318 06:17:44.036166  3501 solver.cpp:228] Iteration 4360, loss = 47.4487
I0318 06:17:44.036238  3501 solver.cpp:244]     Train net output #0: loss = 47.4486 (* 1 = 47.4486 loss)
I0318 06:17:44.036253  3501 sgd_solver.cpp:106] Iteration 4360, lr = 0.01
I0318 06:17:51.378388  3501 solver.cpp:228] Iteration 4380, loss = 39.8253
I0318 06:17:51.378466  3501 solver.cpp:244]     Train net output #0: loss = 39.8252 (* 1 = 39.8252 loss)
I0318 06:17:51.378484  3501 sgd_solver.cpp:106] Iteration 4380, lr = 0.01
I0318 06:17:58.715823  3501 solver.cpp:228] Iteration 4400, loss = 42.44
I0318 06:17:58.715888  3501 solver.cpp:244]     Train net output #0: loss = 42.4399 (* 1 = 42.4399 loss)
I0318 06:17:58.715900  3501 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I0318 06:18:06.058091  3501 solver.cpp:228] Iteration 4420, loss = 56.3075
I0318 06:18:06.058154  3501 solver.cpp:244]     Train net output #0: loss = 56.3074 (* 1 = 56.3074 loss)
I0318 06:18:06.058172  3501 sgd_solver.cpp:106] Iteration 4420, lr = 0.01
I0318 06:18:13.386695  3501 solver.cpp:228] Iteration 4440, loss = 50.1569
I0318 06:18:13.386868  3501 solver.cpp:244]     Train net output #0: loss = 50.1568 (* 1 = 50.1568 loss)
I0318 06:18:13.386884  3501 sgd_solver.cpp:106] Iteration 4440, lr = 0.01
I0318 06:18:20.720588  3501 solver.cpp:228] Iteration 4460, loss = 60.3479
I0318 06:18:20.720651  3501 solver.cpp:244]     Train net output #0: loss = 60.3478 (* 1 = 60.3478 loss)
I0318 06:18:20.720665  3501 sgd_solver.cpp:106] Iteration 4460, lr = 0.01
I0318 06:18:28.047649  3501 solver.cpp:228] Iteration 4480, loss = 65.939
I0318 06:18:28.047713  3501 solver.cpp:244]     Train net output #0: loss = 65.9389 (* 1 = 65.9389 loss)
I0318 06:18:28.047726  3501 sgd_solver.cpp:106] Iteration 4480, lr = 0.01
I0318 06:18:35.002542  3501 solver.cpp:337] Iteration 4500, Testing net (#0)
I0318 06:20:29.186707  3501 solver.cpp:404]     Test net output #0: loss = 655.43 (* 1 = 655.43 loss)
I0318 06:20:29.186790  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 06:20:29.523082  3501 solver.cpp:228] Iteration 4500, loss = 44.8197
I0318 06:20:29.523145  3501 solver.cpp:244]     Train net output #0: loss = 44.8196 (* 1 = 44.8196 loss)
I0318 06:20:29.523159  3501 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I0318 06:20:36.767369  3501 solver.cpp:228] Iteration 4520, loss = 46.0832
I0318 06:20:36.767433  3501 solver.cpp:244]     Train net output #0: loss = 46.0831 (* 1 = 46.0831 loss)
I0318 06:20:36.767447  3501 sgd_solver.cpp:106] Iteration 4520, lr = 0.01
I0318 06:20:44.059825  3501 solver.cpp:228] Iteration 4540, loss = 16.1342
I0318 06:20:44.059887  3501 solver.cpp:244]     Train net output #0: loss = 16.1341 (* 1 = 16.1341 loss)
I0318 06:20:44.059901  3501 sgd_solver.cpp:106] Iteration 4540, lr = 0.01
I0318 06:20:51.378187  3501 solver.cpp:228] Iteration 4560, loss = 35.3939
I0318 06:20:51.378250  3501 solver.cpp:244]     Train net output #0: loss = 35.3938 (* 1 = 35.3938 loss)
I0318 06:20:51.378264  3501 sgd_solver.cpp:106] Iteration 4560, lr = 0.01
I0318 06:20:58.701962  3501 solver.cpp:228] Iteration 4580, loss = 49.0912
I0318 06:20:58.702026  3501 solver.cpp:244]     Train net output #0: loss = 49.0911 (* 1 = 49.0911 loss)
I0318 06:20:58.702039  3501 sgd_solver.cpp:106] Iteration 4580, lr = 0.01
I0318 06:21:06.023555  3501 solver.cpp:228] Iteration 4600, loss = 40.3025
I0318 06:21:06.023690  3501 solver.cpp:244]     Train net output #0: loss = 40.3024 (* 1 = 40.3024 loss)
I0318 06:21:06.023705  3501 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I0318 06:21:13.352895  3501 solver.cpp:228] Iteration 4620, loss = 54.9099
I0318 06:21:13.352962  3501 solver.cpp:244]     Train net output #0: loss = 54.9098 (* 1 = 54.9098 loss)
I0318 06:21:13.352975  3501 sgd_solver.cpp:106] Iteration 4620, lr = 0.01
I0318 06:21:20.692723  3501 solver.cpp:228] Iteration 4640, loss = 91.0289
I0318 06:21:20.692786  3501 solver.cpp:244]     Train net output #0: loss = 91.0288 (* 1 = 91.0288 loss)
I0318 06:21:20.692800  3501 sgd_solver.cpp:106] Iteration 4640, lr = 0.01
I0318 06:21:28.027539  3501 solver.cpp:228] Iteration 4660, loss = 58.1452
I0318 06:21:28.027603  3501 solver.cpp:244]     Train net output #0: loss = 58.1451 (* 1 = 58.1451 loss)
I0318 06:21:28.027616  3501 sgd_solver.cpp:106] Iteration 4660, lr = 0.01
I0318 06:21:35.361740  3501 solver.cpp:228] Iteration 4680, loss = 60.7825
I0318 06:21:35.361805  3501 solver.cpp:244]     Train net output #0: loss = 60.7824 (* 1 = 60.7824 loss)
I0318 06:21:35.361819  3501 sgd_solver.cpp:106] Iteration 4680, lr = 0.01
I0318 06:21:42.689478  3501 solver.cpp:228] Iteration 4700, loss = 59.732
I0318 06:21:42.689656  3501 solver.cpp:244]     Train net output #0: loss = 59.7319 (* 1 = 59.7319 loss)
I0318 06:21:42.689672  3501 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I0318 06:21:50.016203  3501 solver.cpp:228] Iteration 4720, loss = 35.4066
I0318 06:21:50.016268  3501 solver.cpp:244]     Train net output #0: loss = 35.4065 (* 1 = 35.4065 loss)
I0318 06:21:50.016281  3501 sgd_solver.cpp:106] Iteration 4720, lr = 0.01
I0318 06:21:57.339987  3501 solver.cpp:228] Iteration 4740, loss = 42.5031
I0318 06:21:57.340051  3501 solver.cpp:244]     Train net output #0: loss = 42.503 (* 1 = 42.503 loss)
I0318 06:21:57.340065  3501 sgd_solver.cpp:106] Iteration 4740, lr = 0.01
I0318 06:22:00.635608  3501 solver.cpp:337] Iteration 4750, Testing net (#0)
I0318 06:23:54.843505  3501 solver.cpp:404]     Test net output #0: loss = 650.905 (* 1 = 650.905 loss)
I0318 06:23:54.843629  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 06:23:58.788501  3501 solver.cpp:228] Iteration 4760, loss = 55.2
I0318 06:23:58.788564  3501 solver.cpp:244]     Train net output #0: loss = 55.1999 (* 1 = 55.1999 loss)
I0318 06:23:58.788578  3501 sgd_solver.cpp:106] Iteration 4760, lr = 0.01
I0318 06:24:06.057750  3501 solver.cpp:228] Iteration 4780, loss = 36.5305
I0318 06:24:06.057812  3501 solver.cpp:244]     Train net output #0: loss = 36.5304 (* 1 = 36.5304 loss)
I0318 06:24:06.057826  3501 sgd_solver.cpp:106] Iteration 4780, lr = 0.01
I0318 06:24:13.365453  3501 solver.cpp:228] Iteration 4800, loss = 42.7281
I0318 06:24:13.365520  3501 solver.cpp:244]     Train net output #0: loss = 42.7281 (* 1 = 42.7281 loss)
I0318 06:24:13.365533  3501 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I0318 06:24:20.693528  3501 solver.cpp:228] Iteration 4820, loss = 65.8236
I0318 06:24:20.693593  3501 solver.cpp:244]     Train net output #0: loss = 65.8235 (* 1 = 65.8235 loss)
I0318 06:24:20.693606  3501 sgd_solver.cpp:106] Iteration 4820, lr = 0.01
I0318 06:24:28.024186  3501 solver.cpp:228] Iteration 4840, loss = 65.3358
I0318 06:24:28.024348  3501 solver.cpp:244]     Train net output #0: loss = 65.3357 (* 1 = 65.3357 loss)
I0318 06:24:28.024363  3501 sgd_solver.cpp:106] Iteration 4840, lr = 0.01
I0318 06:24:35.368146  3501 solver.cpp:228] Iteration 4860, loss = 92.8324
I0318 06:24:35.368211  3501 solver.cpp:244]     Train net output #0: loss = 92.8323 (* 1 = 92.8323 loss)
I0318 06:24:35.368226  3501 sgd_solver.cpp:106] Iteration 4860, lr = 0.01
I0318 06:24:42.716159  3501 solver.cpp:228] Iteration 4880, loss = 59.4122
I0318 06:24:42.716224  3501 solver.cpp:244]     Train net output #0: loss = 59.4121 (* 1 = 59.4121 loss)
I0318 06:24:42.716238  3501 sgd_solver.cpp:106] Iteration 4880, lr = 0.01
I0318 06:24:50.057013  3501 solver.cpp:228] Iteration 4900, loss = 22.0879
I0318 06:24:50.057082  3501 solver.cpp:244]     Train net output #0: loss = 22.0879 (* 1 = 22.0879 loss)
I0318 06:24:50.057102  3501 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I0318 06:24:57.400221  3501 solver.cpp:228] Iteration 4920, loss = 37.8457
I0318 06:24:57.400287  3501 solver.cpp:244]     Train net output #0: loss = 37.8456 (* 1 = 37.8456 loss)
I0318 06:24:57.400313  3501 sgd_solver.cpp:106] Iteration 4920, lr = 0.01
I0318 06:25:04.740720  3501 solver.cpp:228] Iteration 4940, loss = 61.9102
I0318 06:25:04.740873  3501 solver.cpp:244]     Train net output #0: loss = 61.9101 (* 1 = 61.9101 loss)
I0318 06:25:04.740888  3501 sgd_solver.cpp:106] Iteration 4940, lr = 0.01
I0318 06:25:12.076335  3501 solver.cpp:228] Iteration 4960, loss = 78.8207
I0318 06:25:12.076398  3501 solver.cpp:244]     Train net output #0: loss = 78.8206 (* 1 = 78.8206 loss)
I0318 06:25:12.076411  3501 sgd_solver.cpp:106] Iteration 4960, lr = 0.01
I0318 06:25:19.415341  3501 solver.cpp:228] Iteration 4980, loss = 77.7416
I0318 06:25:19.415405  3501 solver.cpp:244]     Train net output #0: loss = 77.7415 (* 1 = 77.7415 loss)
I0318 06:25:19.415419  3501 sgd_solver.cpp:106] Iteration 4980, lr = 0.01
I0318 06:25:26.389219  3501 solver.cpp:454] Snapshotting to binary proto file ./caffe_alexnet_train_iter_5000.caffemodel
I0318 06:25:28.122985  3501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./caffe_alexnet_train_iter_5000.solverstate
I0318 06:25:28.528230  3501 solver.cpp:337] Iteration 5000, Testing net (#0)
I0318 06:27:22.677394  3501 solver.cpp:404]     Test net output #0: loss = 661.636 (* 1 = 661.636 loss)
I0318 06:27:22.677554  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:27:23.014446  3501 solver.cpp:228] Iteration 5000, loss = 85.9292
I0318 06:27:23.014505  3501 solver.cpp:244]     Train net output #0: loss = 85.9291 (* 1 = 85.9291 loss)
I0318 06:27:23.014519  3501 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I0318 06:27:30.259579  3501 solver.cpp:228] Iteration 5020, loss = 60.4522
I0318 06:27:30.259644  3501 solver.cpp:244]     Train net output #0: loss = 60.4521 (* 1 = 60.4521 loss)
I0318 06:27:30.259657  3501 sgd_solver.cpp:106] Iteration 5020, lr = 0.01
I0318 06:27:37.567024  3501 solver.cpp:228] Iteration 5040, loss = 104.093
I0318 06:27:37.567090  3501 solver.cpp:244]     Train net output #0: loss = 104.093 (* 1 = 104.093 loss)
I0318 06:27:37.567103  3501 sgd_solver.cpp:106] Iteration 5040, lr = 0.01
I0318 06:27:44.892526  3501 solver.cpp:228] Iteration 5060, loss = 41.083
I0318 06:27:44.892592  3501 solver.cpp:244]     Train net output #0: loss = 41.0829 (* 1 = 41.0829 loss)
I0318 06:27:44.892606  3501 sgd_solver.cpp:106] Iteration 5060, lr = 0.01
I0318 06:27:52.224474  3501 solver.cpp:228] Iteration 5080, loss = 31.5607
I0318 06:27:52.224539  3501 solver.cpp:244]     Train net output #0: loss = 31.5606 (* 1 = 31.5606 loss)
I0318 06:27:52.224551  3501 sgd_solver.cpp:106] Iteration 5080, lr = 0.01
I0318 06:27:59.557772  3501 solver.cpp:228] Iteration 5100, loss = 38.1486
I0318 06:27:59.557936  3501 solver.cpp:244]     Train net output #0: loss = 38.1485 (* 1 = 38.1485 loss)
I0318 06:27:59.557953  3501 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I0318 06:28:06.887593  3501 solver.cpp:228] Iteration 5120, loss = 23.7518
I0318 06:28:06.887652  3501 solver.cpp:244]     Train net output #0: loss = 23.7517 (* 1 = 23.7517 loss)
I0318 06:28:06.887665  3501 sgd_solver.cpp:106] Iteration 5120, lr = 0.01
I0318 06:28:14.222501  3501 solver.cpp:228] Iteration 5140, loss = 53.196
I0318 06:28:14.222571  3501 solver.cpp:244]     Train net output #0: loss = 53.1959 (* 1 = 53.1959 loss)
I0318 06:28:14.222585  3501 sgd_solver.cpp:106] Iteration 5140, lr = 0.01
I0318 06:28:21.564064  3501 solver.cpp:228] Iteration 5160, loss = 32.5523
I0318 06:28:21.564137  3501 solver.cpp:244]     Train net output #0: loss = 32.5522 (* 1 = 32.5522 loss)
I0318 06:28:21.564153  3501 sgd_solver.cpp:106] Iteration 5160, lr = 0.01
I0318 06:28:28.902508  3501 solver.cpp:228] Iteration 5180, loss = 56.59
I0318 06:28:28.902573  3501 solver.cpp:244]     Train net output #0: loss = 56.5899 (* 1 = 56.5899 loss)
I0318 06:28:28.902587  3501 sgd_solver.cpp:106] Iteration 5180, lr = 0.01
I0318 06:28:36.238548  3501 solver.cpp:228] Iteration 5200, loss = 117.833
I0318 06:28:36.238694  3501 solver.cpp:244]     Train net output #0: loss = 117.833 (* 1 = 117.833 loss)
I0318 06:28:36.238709  3501 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I0318 06:28:43.573191  3501 solver.cpp:228] Iteration 5220, loss = 61.878
I0318 06:28:43.573256  3501 solver.cpp:244]     Train net output #0: loss = 61.8778 (* 1 = 61.8778 loss)
I0318 06:28:43.573271  3501 sgd_solver.cpp:106] Iteration 5220, lr = 0.01
I0318 06:28:50.897698  3501 solver.cpp:228] Iteration 5240, loss = 57.0188
I0318 06:28:50.897763  3501 solver.cpp:244]     Train net output #0: loss = 57.0186 (* 1 = 57.0186 loss)
I0318 06:28:50.897776  3501 sgd_solver.cpp:106] Iteration 5240, lr = 0.01
I0318 06:28:54.189878  3501 solver.cpp:337] Iteration 5250, Testing net (#0)
I0318 06:30:48.369549  3501 solver.cpp:404]     Test net output #0: loss = 636.977 (* 1 = 636.977 loss)
I0318 06:30:48.369709  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:30:52.318526  3501 solver.cpp:228] Iteration 5260, loss = 38.3612
I0318 06:30:52.318590  3501 solver.cpp:244]     Train net output #0: loss = 38.3611 (* 1 = 38.3611 loss)
I0318 06:30:52.318603  3501 sgd_solver.cpp:106] Iteration 5260, lr = 0.01
I0318 06:30:59.589182  3501 solver.cpp:228] Iteration 5280, loss = 41.4197
I0318 06:30:59.589246  3501 solver.cpp:244]     Train net output #0: loss = 41.4195 (* 1 = 41.4195 loss)
I0318 06:30:59.589260  3501 sgd_solver.cpp:106] Iteration 5280, lr = 0.01
I0318 06:31:06.900113  3501 solver.cpp:228] Iteration 5300, loss = 22.627
I0318 06:31:06.900178  3501 solver.cpp:244]     Train net output #0: loss = 22.6269 (* 1 = 22.6269 loss)
I0318 06:31:06.900192  3501 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I0318 06:31:14.223937  3501 solver.cpp:228] Iteration 5320, loss = 62.3795
I0318 06:31:14.224001  3501 solver.cpp:244]     Train net output #0: loss = 62.3794 (* 1 = 62.3794 loss)
I0318 06:31:14.224015  3501 sgd_solver.cpp:106] Iteration 5320, lr = 0.01
I0318 06:31:21.554424  3501 solver.cpp:228] Iteration 5340, loss = 42.572
I0318 06:31:21.554565  3501 solver.cpp:244]     Train net output #0: loss = 42.5719 (* 1 = 42.5719 loss)
I0318 06:31:21.554581  3501 sgd_solver.cpp:106] Iteration 5340, lr = 0.01
I0318 06:31:28.880866  3501 solver.cpp:228] Iteration 5360, loss = 62.4215
I0318 06:31:28.880929  3501 solver.cpp:244]     Train net output #0: loss = 62.4214 (* 1 = 62.4214 loss)
I0318 06:31:28.880944  3501 sgd_solver.cpp:106] Iteration 5360, lr = 0.01
I0318 06:31:36.212869  3501 solver.cpp:228] Iteration 5380, loss = 84.9296
I0318 06:31:36.212934  3501 solver.cpp:244]     Train net output #0: loss = 84.9295 (* 1 = 84.9295 loss)
I0318 06:31:36.212949  3501 sgd_solver.cpp:106] Iteration 5380, lr = 0.01
I0318 06:31:43.555101  3501 solver.cpp:228] Iteration 5400, loss = 117.139
I0318 06:31:43.555166  3501 solver.cpp:244]     Train net output #0: loss = 117.139 (* 1 = 117.139 loss)
I0318 06:31:43.555179  3501 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I0318 06:31:50.889905  3501 solver.cpp:228] Iteration 5420, loss = 33.931
I0318 06:31:50.889971  3501 solver.cpp:244]     Train net output #0: loss = 33.9309 (* 1 = 33.9309 loss)
I0318 06:31:50.889986  3501 sgd_solver.cpp:106] Iteration 5420, lr = 0.01
I0318 06:31:58.234915  3501 solver.cpp:228] Iteration 5440, loss = 14.6157
I0318 06:31:58.235054  3501 solver.cpp:244]     Train net output #0: loss = 14.6156 (* 1 = 14.6156 loss)
I0318 06:31:58.235069  3501 sgd_solver.cpp:106] Iteration 5440, lr = 0.01
I0318 06:32:05.584127  3501 solver.cpp:228] Iteration 5460, loss = 53.9449
I0318 06:32:05.584202  3501 solver.cpp:244]     Train net output #0: loss = 53.9448 (* 1 = 53.9448 loss)
I0318 06:32:05.584216  3501 sgd_solver.cpp:106] Iteration 5460, lr = 0.01
I0318 06:32:12.918215  3501 solver.cpp:228] Iteration 5480, loss = 27.0014
I0318 06:32:12.918279  3501 solver.cpp:244]     Train net output #0: loss = 27.0013 (* 1 = 27.0013 loss)
I0318 06:32:12.918293  3501 sgd_solver.cpp:106] Iteration 5480, lr = 0.01
I0318 06:32:19.887738  3501 solver.cpp:337] Iteration 5500, Testing net (#0)
I0318 06:34:14.125854  3501 solver.cpp:404]     Test net output #0: loss = 688.454 (* 1 = 688.454 loss)
I0318 06:34:14.125977  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 06:34:14.462957  3501 solver.cpp:228] Iteration 5500, loss = 41.2253
I0318 06:34:14.463021  3501 solver.cpp:244]     Train net output #0: loss = 41.2251 (* 1 = 41.2251 loss)
I0318 06:34:14.463034  3501 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I0318 06:34:21.709827  3501 solver.cpp:228] Iteration 5520, loss = 50.7923
I0318 06:34:21.709892  3501 solver.cpp:244]     Train net output #0: loss = 50.7922 (* 1 = 50.7922 loss)
I0318 06:34:21.709904  3501 sgd_solver.cpp:106] Iteration 5520, lr = 0.01
I0318 06:34:29.006165  3501 solver.cpp:228] Iteration 5540, loss = 76.1764
I0318 06:34:29.006230  3501 solver.cpp:244]     Train net output #0: loss = 76.1763 (* 1 = 76.1763 loss)
I0318 06:34:29.006244  3501 sgd_solver.cpp:106] Iteration 5540, lr = 0.01
I0318 06:34:36.318253  3501 solver.cpp:228] Iteration 5560, loss = 53.7258
I0318 06:34:36.318321  3501 solver.cpp:244]     Train net output #0: loss = 53.7257 (* 1 = 53.7257 loss)
I0318 06:34:36.318336  3501 sgd_solver.cpp:106] Iteration 5560, lr = 0.01
I0318 06:34:43.647779  3501 solver.cpp:228] Iteration 5580, loss = 66.0907
I0318 06:34:43.647845  3501 solver.cpp:244]     Train net output #0: loss = 66.0906 (* 1 = 66.0906 loss)
I0318 06:34:43.647857  3501 sgd_solver.cpp:106] Iteration 5580, lr = 0.01
I0318 06:34:50.982259  3501 solver.cpp:228] Iteration 5600, loss = 37.1508
I0318 06:34:50.982444  3501 solver.cpp:244]     Train net output #0: loss = 37.1507 (* 1 = 37.1507 loss)
I0318 06:34:50.982458  3501 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I0318 06:34:58.321794  3501 solver.cpp:228] Iteration 5620, loss = 31.0293
I0318 06:34:58.321857  3501 solver.cpp:244]     Train net output #0: loss = 31.0292 (* 1 = 31.0292 loss)
I0318 06:34:58.321871  3501 sgd_solver.cpp:106] Iteration 5620, lr = 0.01
I0318 06:35:05.663182  3501 solver.cpp:228] Iteration 5640, loss = 32.8731
I0318 06:35:05.663247  3501 solver.cpp:244]     Train net output #0: loss = 32.873 (* 1 = 32.873 loss)
I0318 06:35:05.663261  3501 sgd_solver.cpp:106] Iteration 5640, lr = 0.01
I0318 06:35:12.996034  3501 solver.cpp:228] Iteration 5660, loss = 35.3695
I0318 06:35:12.996100  3501 solver.cpp:244]     Train net output #0: loss = 35.3694 (* 1 = 35.3694 loss)
I0318 06:35:12.996114  3501 sgd_solver.cpp:106] Iteration 5660, lr = 0.01
I0318 06:35:20.332669  3501 solver.cpp:228] Iteration 5680, loss = 52.5859
I0318 06:35:20.332734  3501 solver.cpp:244]     Train net output #0: loss = 52.5858 (* 1 = 52.5858 loss)
I0318 06:35:20.332748  3501 sgd_solver.cpp:106] Iteration 5680, lr = 0.01
I0318 06:35:27.665566  3501 solver.cpp:228] Iteration 5700, loss = 25.8116
I0318 06:35:27.665709  3501 solver.cpp:244]     Train net output #0: loss = 25.8115 (* 1 = 25.8115 loss)
I0318 06:35:27.665724  3501 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I0318 06:35:34.996918  3501 solver.cpp:228] Iteration 5720, loss = 79.0537
I0318 06:35:34.996984  3501 solver.cpp:244]     Train net output #0: loss = 79.0536 (* 1 = 79.0536 loss)
I0318 06:35:34.996997  3501 sgd_solver.cpp:106] Iteration 5720, lr = 0.01
I0318 06:35:42.330909  3501 solver.cpp:228] Iteration 5740, loss = 100.321
I0318 06:35:42.330974  3501 solver.cpp:244]     Train net output #0: loss = 100.321 (* 1 = 100.321 loss)
I0318 06:35:42.330988  3501 sgd_solver.cpp:106] Iteration 5740, lr = 0.01
I0318 06:35:45.631476  3501 solver.cpp:337] Iteration 5750, Testing net (#0)
I0318 06:37:39.851645  3501 solver.cpp:404]     Test net output #0: loss = 670.836 (* 1 = 670.836 loss)
I0318 06:37:39.851721  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 06:37:43.801122  3501 solver.cpp:228] Iteration 5760, loss = 68.5175
I0318 06:37:43.801187  3501 solver.cpp:244]     Train net output #0: loss = 68.5174 (* 1 = 68.5174 loss)
I0318 06:37:43.801200  3501 sgd_solver.cpp:106] Iteration 5760, lr = 0.01
I0318 06:37:51.061913  3501 solver.cpp:228] Iteration 5780, loss = 32.9454
I0318 06:37:51.061981  3501 solver.cpp:244]     Train net output #0: loss = 32.9452 (* 1 = 32.9452 loss)
I0318 06:37:51.061995  3501 sgd_solver.cpp:106] Iteration 5780, lr = 0.01
I0318 06:37:58.370911  3501 solver.cpp:228] Iteration 5800, loss = 18.8334
I0318 06:37:58.370980  3501 solver.cpp:244]     Train net output #0: loss = 18.8333 (* 1 = 18.8333 loss)
I0318 06:37:58.370995  3501 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I0318 06:38:05.702304  3501 solver.cpp:228] Iteration 5820, loss = 39.2044
I0318 06:38:05.702370  3501 solver.cpp:244]     Train net output #0: loss = 39.2043 (* 1 = 39.2043 loss)
I0318 06:38:05.702384  3501 sgd_solver.cpp:106] Iteration 5820, lr = 0.01
I0318 06:38:13.041029  3501 solver.cpp:228] Iteration 5840, loss = 53.2001
I0318 06:38:13.041198  3501 solver.cpp:244]     Train net output #0: loss = 53.2 (* 1 = 53.2 loss)
I0318 06:38:13.041216  3501 sgd_solver.cpp:106] Iteration 5840, lr = 0.01
I0318 06:38:20.386329  3501 solver.cpp:228] Iteration 5860, loss = 52.399
I0318 06:38:20.386392  3501 solver.cpp:244]     Train net output #0: loss = 52.3989 (* 1 = 52.3989 loss)
I0318 06:38:20.386405  3501 sgd_solver.cpp:106] Iteration 5860, lr = 0.01
I0318 06:38:27.726348  3501 solver.cpp:228] Iteration 5880, loss = 55.2565
I0318 06:38:27.726414  3501 solver.cpp:244]     Train net output #0: loss = 55.2564 (* 1 = 55.2564 loss)
I0318 06:38:27.726428  3501 sgd_solver.cpp:106] Iteration 5880, lr = 0.01
I0318 06:38:35.064044  3501 solver.cpp:228] Iteration 5900, loss = 36.015
I0318 06:38:35.064110  3501 solver.cpp:244]     Train net output #0: loss = 36.0149 (* 1 = 36.0149 loss)
I0318 06:38:35.064124  3501 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I0318 06:38:42.408148  3501 solver.cpp:228] Iteration 5920, loss = 55.4389
I0318 06:38:42.408226  3501 solver.cpp:244]     Train net output #0: loss = 55.4388 (* 1 = 55.4388 loss)
I0318 06:38:42.408242  3501 sgd_solver.cpp:106] Iteration 5920, lr = 0.01
I0318 06:38:49.750346  3501 solver.cpp:228] Iteration 5940, loss = 40.998
I0318 06:38:49.750530  3501 solver.cpp:244]     Train net output #0: loss = 40.9979 (* 1 = 40.9979 loss)
I0318 06:38:49.750546  3501 sgd_solver.cpp:106] Iteration 5940, lr = 0.01
I0318 06:38:57.088919  3501 solver.cpp:228] Iteration 5960, loss = 45.5249
I0318 06:38:57.088984  3501 solver.cpp:244]     Train net output #0: loss = 45.5248 (* 1 = 45.5248 loss)
I0318 06:38:57.088996  3501 sgd_solver.cpp:106] Iteration 5960, lr = 0.01
I0318 06:39:04.423014  3501 solver.cpp:228] Iteration 5980, loss = 15.3951
I0318 06:39:04.423080  3501 solver.cpp:244]     Train net output #0: loss = 15.395 (* 1 = 15.395 loss)
I0318 06:39:04.423094  3501 sgd_solver.cpp:106] Iteration 5980, lr = 0.01
I0318 06:39:11.392653  3501 solver.cpp:337] Iteration 6000, Testing net (#0)
I0318 06:41:05.605432  3501 solver.cpp:404]     Test net output #0: loss = 675.652 (* 1 = 675.652 loss)
I0318 06:41:05.605509  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:41:05.942741  3501 solver.cpp:228] Iteration 6000, loss = 47.3106
I0318 06:41:05.942806  3501 solver.cpp:244]     Train net output #0: loss = 47.3105 (* 1 = 47.3105 loss)
I0318 06:41:05.942821  3501 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I0318 06:41:13.186213  3501 solver.cpp:228] Iteration 6020, loss = 54.493
I0318 06:41:13.186272  3501 solver.cpp:244]     Train net output #0: loss = 54.4929 (* 1 = 54.4929 loss)
I0318 06:41:13.186287  3501 sgd_solver.cpp:106] Iteration 6020, lr = 0.01
I0318 06:41:20.481402  3501 solver.cpp:228] Iteration 6040, loss = 72.1767
I0318 06:41:20.481467  3501 solver.cpp:244]     Train net output #0: loss = 72.1766 (* 1 = 72.1766 loss)
I0318 06:41:20.481482  3501 sgd_solver.cpp:106] Iteration 6040, lr = 0.01
I0318 06:41:27.798794  3501 solver.cpp:228] Iteration 6060, loss = 80.8051
I0318 06:41:27.798861  3501 solver.cpp:244]     Train net output #0: loss = 80.805 (* 1 = 80.805 loss)
I0318 06:41:27.798874  3501 sgd_solver.cpp:106] Iteration 6060, lr = 0.01
I0318 06:41:35.119714  3501 solver.cpp:228] Iteration 6080, loss = 55.3834
I0318 06:41:35.119779  3501 solver.cpp:244]     Train net output #0: loss = 55.3833 (* 1 = 55.3833 loss)
I0318 06:41:35.119793  3501 sgd_solver.cpp:106] Iteration 6080, lr = 0.01
I0318 06:41:42.446115  3501 solver.cpp:228] Iteration 6100, loss = 109.333
I0318 06:41:42.446257  3501 solver.cpp:244]     Train net output #0: loss = 109.333 (* 1 = 109.333 loss)
I0318 06:41:42.446271  3501 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I0318 06:41:49.770871  3501 solver.cpp:228] Iteration 6120, loss = 48.6296
I0318 06:41:49.770936  3501 solver.cpp:244]     Train net output #0: loss = 48.6295 (* 1 = 48.6295 loss)
I0318 06:41:49.770949  3501 sgd_solver.cpp:106] Iteration 6120, lr = 0.01
I0318 06:41:57.103307  3501 solver.cpp:228] Iteration 6140, loss = 25.7236
I0318 06:41:57.103373  3501 solver.cpp:244]     Train net output #0: loss = 25.7235 (* 1 = 25.7235 loss)
I0318 06:41:57.103385  3501 sgd_solver.cpp:106] Iteration 6140, lr = 0.01
I0318 06:42:04.447382  3501 solver.cpp:228] Iteration 6160, loss = 19.5669
I0318 06:42:04.447453  3501 solver.cpp:244]     Train net output #0: loss = 19.5668 (* 1 = 19.5668 loss)
I0318 06:42:04.447468  3501 sgd_solver.cpp:106] Iteration 6160, lr = 0.01
I0318 06:42:11.793200  3501 solver.cpp:228] Iteration 6180, loss = 14.8528
I0318 06:42:11.793267  3501 solver.cpp:244]     Train net output #0: loss = 14.8527 (* 1 = 14.8527 loss)
I0318 06:42:11.793280  3501 sgd_solver.cpp:106] Iteration 6180, lr = 0.01
I0318 06:42:19.140338  3501 solver.cpp:228] Iteration 6200, loss = 74.9564
I0318 06:42:19.140534  3501 solver.cpp:244]     Train net output #0: loss = 74.9563 (* 1 = 74.9563 loss)
I0318 06:42:19.140550  3501 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I0318 06:42:26.487704  3501 solver.cpp:228] Iteration 6220, loss = 73.5272
I0318 06:42:26.487771  3501 solver.cpp:244]     Train net output #0: loss = 73.5271 (* 1 = 73.5271 loss)
I0318 06:42:26.487783  3501 sgd_solver.cpp:106] Iteration 6220, lr = 0.01
I0318 06:42:33.834033  3501 solver.cpp:228] Iteration 6240, loss = 36.3666
I0318 06:42:33.834105  3501 solver.cpp:244]     Train net output #0: loss = 36.3665 (* 1 = 36.3665 loss)
I0318 06:42:33.834118  3501 sgd_solver.cpp:106] Iteration 6240, lr = 0.01
I0318 06:42:37.136554  3501 solver.cpp:337] Iteration 6250, Testing net (#0)
I0318 06:44:31.331161  3501 solver.cpp:404]     Test net output #0: loss = 628.56 (* 1 = 628.56 loss)
I0318 06:44:31.331300  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:44:35.295878  3501 solver.cpp:228] Iteration 6260, loss = 86.0606
I0318 06:44:35.295948  3501 solver.cpp:244]     Train net output #0: loss = 86.0605 (* 1 = 86.0605 loss)
I0318 06:44:35.295961  3501 sgd_solver.cpp:106] Iteration 6260, lr = 0.01
I0318 06:44:42.594147  3501 solver.cpp:228] Iteration 6280, loss = 66.4996
I0318 06:44:42.594213  3501 solver.cpp:244]     Train net output #0: loss = 66.4995 (* 1 = 66.4995 loss)
I0318 06:44:42.594228  3501 sgd_solver.cpp:106] Iteration 6280, lr = 0.01
I0318 06:44:49.912628  3501 solver.cpp:228] Iteration 6300, loss = 57.6169
I0318 06:44:49.912693  3501 solver.cpp:244]     Train net output #0: loss = 57.6168 (* 1 = 57.6168 loss)
I0318 06:44:49.912706  3501 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I0318 06:44:57.235571  3501 solver.cpp:228] Iteration 6320, loss = 60.2223
I0318 06:44:57.235637  3501 solver.cpp:244]     Train net output #0: loss = 60.2222 (* 1 = 60.2222 loss)
I0318 06:44:57.235651  3501 sgd_solver.cpp:106] Iteration 6320, lr = 0.01
I0318 06:45:04.560999  3501 solver.cpp:228] Iteration 6340, loss = 28.714
I0318 06:45:04.561136  3501 solver.cpp:244]     Train net output #0: loss = 28.714 (* 1 = 28.714 loss)
I0318 06:45:04.561151  3501 sgd_solver.cpp:106] Iteration 6340, lr = 0.01
I0318 06:45:11.883198  3501 solver.cpp:228] Iteration 6360, loss = 55.936
I0318 06:45:11.883266  3501 solver.cpp:244]     Train net output #0: loss = 55.936 (* 1 = 55.936 loss)
I0318 06:45:11.883281  3501 sgd_solver.cpp:106] Iteration 6360, lr = 0.01
I0318 06:45:19.217226  3501 solver.cpp:228] Iteration 6380, loss = 46.9471
I0318 06:45:19.217298  3501 solver.cpp:244]     Train net output #0: loss = 46.947 (* 1 = 46.947 loss)
I0318 06:45:19.217314  3501 sgd_solver.cpp:106] Iteration 6380, lr = 0.01
I0318 06:45:26.554126  3501 solver.cpp:228] Iteration 6400, loss = 33.5658
I0318 06:45:26.554190  3501 solver.cpp:244]     Train net output #0: loss = 33.5657 (* 1 = 33.5657 loss)
I0318 06:45:26.554203  3501 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I0318 06:45:33.902717  3501 solver.cpp:228] Iteration 6420, loss = 38.0987
I0318 06:45:33.902781  3501 solver.cpp:244]     Train net output #0: loss = 38.0987 (* 1 = 38.0987 loss)
I0318 06:45:33.902796  3501 sgd_solver.cpp:106] Iteration 6420, lr = 0.01
I0318 06:45:41.248396  3501 solver.cpp:228] Iteration 6440, loss = 51.2637
I0318 06:45:41.248581  3501 solver.cpp:244]     Train net output #0: loss = 51.2636 (* 1 = 51.2636 loss)
I0318 06:45:41.248598  3501 sgd_solver.cpp:106] Iteration 6440, lr = 0.01
I0318 06:45:48.602640  3501 solver.cpp:228] Iteration 6460, loss = 99.4082
I0318 06:45:48.602705  3501 solver.cpp:244]     Train net output #0: loss = 99.4081 (* 1 = 99.4081 loss)
I0318 06:45:48.602720  3501 sgd_solver.cpp:106] Iteration 6460, lr = 0.01
I0318 06:45:55.944324  3501 solver.cpp:228] Iteration 6480, loss = 61.6675
I0318 06:45:55.944386  3501 solver.cpp:244]     Train net output #0: loss = 61.6674 (* 1 = 61.6674 loss)
I0318 06:45:55.944401  3501 sgd_solver.cpp:106] Iteration 6480, lr = 0.01
I0318 06:46:02.911831  3501 solver.cpp:337] Iteration 6500, Testing net (#0)
I0318 06:47:57.146445  3501 solver.cpp:404]     Test net output #0: loss = 653.826 (* 1 = 653.826 loss)
I0318 06:47:57.146526  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:47:57.484582  3501 solver.cpp:228] Iteration 6500, loss = 66.8851
I0318 06:47:57.484649  3501 solver.cpp:244]     Train net output #0: loss = 66.885 (* 1 = 66.885 loss)
I0318 06:47:57.484668  3501 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I0318 06:48:04.744171  3501 solver.cpp:228] Iteration 6520, loss = 20.1589
I0318 06:48:04.744238  3501 solver.cpp:244]     Train net output #0: loss = 20.1588 (* 1 = 20.1588 loss)
I0318 06:48:04.744252  3501 sgd_solver.cpp:106] Iteration 6520, lr = 0.01
I0318 06:48:12.042664  3501 solver.cpp:228] Iteration 6540, loss = 78.6881
I0318 06:48:12.042729  3501 solver.cpp:244]     Train net output #0: loss = 78.688 (* 1 = 78.688 loss)
I0318 06:48:12.042743  3501 sgd_solver.cpp:106] Iteration 6540, lr = 0.01
I0318 06:48:19.348574  3501 solver.cpp:228] Iteration 6560, loss = 44.8587
I0318 06:48:19.348639  3501 solver.cpp:244]     Train net output #0: loss = 44.8586 (* 1 = 44.8586 loss)
I0318 06:48:19.348652  3501 sgd_solver.cpp:106] Iteration 6560, lr = 0.01
I0318 06:48:26.678527  3501 solver.cpp:228] Iteration 6580, loss = 43.769
I0318 06:48:26.678591  3501 solver.cpp:244]     Train net output #0: loss = 43.7689 (* 1 = 43.7689 loss)
I0318 06:48:26.678606  3501 sgd_solver.cpp:106] Iteration 6580, lr = 0.01
I0318 06:48:34.013109  3501 solver.cpp:228] Iteration 6600, loss = 63.5757
I0318 06:48:34.013247  3501 solver.cpp:244]     Train net output #0: loss = 63.5756 (* 1 = 63.5756 loss)
I0318 06:48:34.013262  3501 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I0318 06:48:41.353124  3501 solver.cpp:228] Iteration 6620, loss = 51.6217
I0318 06:48:41.353188  3501 solver.cpp:244]     Train net output #0: loss = 51.6216 (* 1 = 51.6216 loss)
I0318 06:48:41.353200  3501 sgd_solver.cpp:106] Iteration 6620, lr = 0.01
I0318 06:48:48.701143  3501 solver.cpp:228] Iteration 6640, loss = 58.7374
I0318 06:48:48.701205  3501 solver.cpp:244]     Train net output #0: loss = 58.7373 (* 1 = 58.7373 loss)
I0318 06:48:48.701218  3501 sgd_solver.cpp:106] Iteration 6640, lr = 0.01
I0318 06:48:56.045277  3501 solver.cpp:228] Iteration 6660, loss = 84.0666
I0318 06:48:56.045338  3501 solver.cpp:244]     Train net output #0: loss = 84.0665 (* 1 = 84.0665 loss)
I0318 06:48:56.045352  3501 sgd_solver.cpp:106] Iteration 6660, lr = 0.01
I0318 06:49:03.394088  3501 solver.cpp:228] Iteration 6680, loss = 38.7014
I0318 06:49:03.394150  3501 solver.cpp:244]     Train net output #0: loss = 38.7013 (* 1 = 38.7013 loss)
I0318 06:49:03.394163  3501 sgd_solver.cpp:106] Iteration 6680, lr = 0.01
I0318 06:49:10.738040  3501 solver.cpp:228] Iteration 6700, loss = 26.8233
I0318 06:49:10.738219  3501 solver.cpp:244]     Train net output #0: loss = 26.8232 (* 1 = 26.8232 loss)
I0318 06:49:10.738235  3501 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I0318 06:49:18.083683  3501 solver.cpp:228] Iteration 6720, loss = 45.4115
I0318 06:49:18.083766  3501 solver.cpp:244]     Train net output #0: loss = 45.4114 (* 1 = 45.4114 loss)
I0318 06:49:18.083782  3501 sgd_solver.cpp:106] Iteration 6720, lr = 0.01
I0318 06:49:25.418332  3501 solver.cpp:228] Iteration 6740, loss = 25.4303
I0318 06:49:25.418398  3501 solver.cpp:244]     Train net output #0: loss = 25.4302 (* 1 = 25.4302 loss)
I0318 06:49:25.418411  3501 sgd_solver.cpp:106] Iteration 6740, lr = 0.01
I0318 06:49:28.724320  3501 solver.cpp:337] Iteration 6750, Testing net (#0)
I0318 06:51:22.907704  3501 solver.cpp:404]     Test net output #0: loss = 661.362 (* 1 = 661.362 loss)
I0318 06:51:22.907866  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 06:51:26.854789  3501 solver.cpp:228] Iteration 6760, loss = 70.373
I0318 06:51:26.854856  3501 solver.cpp:244]     Train net output #0: loss = 70.3729 (* 1 = 70.3729 loss)
I0318 06:51:26.854869  3501 sgd_solver.cpp:106] Iteration 6760, lr = 0.01
I0318 06:51:34.128587  3501 solver.cpp:228] Iteration 6780, loss = 45.4201
I0318 06:51:34.128650  3501 solver.cpp:244]     Train net output #0: loss = 45.42 (* 1 = 45.42 loss)
I0318 06:51:34.128664  3501 sgd_solver.cpp:106] Iteration 6780, lr = 0.01
I0318 06:51:41.428383  3501 solver.cpp:228] Iteration 6800, loss = 43.1978
I0318 06:51:41.428448  3501 solver.cpp:244]     Train net output #0: loss = 43.1976 (* 1 = 43.1976 loss)
I0318 06:51:41.428462  3501 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I0318 06:51:48.743806  3501 solver.cpp:228] Iteration 6820, loss = 58.3925
I0318 06:51:48.743871  3501 solver.cpp:244]     Train net output #0: loss = 58.3924 (* 1 = 58.3924 loss)
I0318 06:51:48.743885  3501 sgd_solver.cpp:106] Iteration 6820, lr = 0.01
I0318 06:51:56.066594  3501 solver.cpp:228] Iteration 6840, loss = 51.5581
I0318 06:51:56.066743  3501 solver.cpp:244]     Train net output #0: loss = 51.558 (* 1 = 51.558 loss)
I0318 06:51:56.066758  3501 sgd_solver.cpp:106] Iteration 6840, lr = 0.01
I0318 06:52:03.398109  3501 solver.cpp:228] Iteration 6860, loss = 38.1707
I0318 06:52:03.398174  3501 solver.cpp:244]     Train net output #0: loss = 38.1705 (* 1 = 38.1705 loss)
I0318 06:52:03.398186  3501 sgd_solver.cpp:106] Iteration 6860, lr = 0.01
I0318 06:52:10.735970  3501 solver.cpp:228] Iteration 6880, loss = 32.2478
I0318 06:52:10.736035  3501 solver.cpp:244]     Train net output #0: loss = 32.2477 (* 1 = 32.2477 loss)
I0318 06:52:10.736049  3501 sgd_solver.cpp:106] Iteration 6880, lr = 0.01
I0318 06:52:18.083395  3501 solver.cpp:228] Iteration 6900, loss = 44.6193
I0318 06:52:18.083465  3501 solver.cpp:244]     Train net output #0: loss = 44.6192 (* 1 = 44.6192 loss)
I0318 06:52:18.083479  3501 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I0318 06:52:25.421967  3501 solver.cpp:228] Iteration 6920, loss = 30.3302
I0318 06:52:25.422031  3501 solver.cpp:244]     Train net output #0: loss = 30.3301 (* 1 = 30.3301 loss)
I0318 06:52:25.422045  3501 sgd_solver.cpp:106] Iteration 6920, lr = 0.01
I0318 06:52:32.750900  3501 solver.cpp:228] Iteration 6940, loss = 70.371
I0318 06:52:32.751039  3501 solver.cpp:244]     Train net output #0: loss = 70.3709 (* 1 = 70.3709 loss)
I0318 06:52:32.751055  3501 sgd_solver.cpp:106] Iteration 6940, lr = 0.01
I0318 06:52:40.077919  3501 solver.cpp:228] Iteration 6960, loss = 52.1442
I0318 06:52:40.077983  3501 solver.cpp:244]     Train net output #0: loss = 52.1441 (* 1 = 52.1441 loss)
I0318 06:52:40.077997  3501 sgd_solver.cpp:106] Iteration 6960, lr = 0.01
I0318 06:52:47.406724  3501 solver.cpp:228] Iteration 6980, loss = 62.5192
I0318 06:52:47.406792  3501 solver.cpp:244]     Train net output #0: loss = 62.519 (* 1 = 62.519 loss)
I0318 06:52:47.406805  3501 sgd_solver.cpp:106] Iteration 6980, lr = 0.01
I0318 06:52:54.371311  3501 solver.cpp:337] Iteration 7000, Testing net (#0)
I0318 06:54:48.626566  3501 solver.cpp:404]     Test net output #0: loss = 637.925 (* 1 = 637.925 loss)
I0318 06:54:48.626705  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 06:54:48.964287  3501 solver.cpp:228] Iteration 7000, loss = 62.5142
I0318 06:54:48.964360  3501 solver.cpp:244]     Train net output #0: loss = 62.514 (* 1 = 62.514 loss)
I0318 06:54:48.964375  3501 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I0318 06:54:56.217488  3501 solver.cpp:228] Iteration 7020, loss = 57.6283
I0318 06:54:56.217551  3501 solver.cpp:244]     Train net output #0: loss = 57.6282 (* 1 = 57.6282 loss)
I0318 06:54:56.217566  3501 sgd_solver.cpp:106] Iteration 7020, lr = 0.01
I0318 06:55:03.508193  3501 solver.cpp:228] Iteration 7040, loss = 77.2352
I0318 06:55:03.508258  3501 solver.cpp:244]     Train net output #0: loss = 77.2351 (* 1 = 77.2351 loss)
I0318 06:55:03.508272  3501 sgd_solver.cpp:106] Iteration 7040, lr = 0.01
I0318 06:55:10.826864  3501 solver.cpp:228] Iteration 7060, loss = 40.4286
I0318 06:55:10.826928  3501 solver.cpp:244]     Train net output #0: loss = 40.4284 (* 1 = 40.4284 loss)
I0318 06:55:10.826942  3501 sgd_solver.cpp:106] Iteration 7060, lr = 0.01
I0318 06:55:18.160619  3501 solver.cpp:228] Iteration 7080, loss = 24.7514
I0318 06:55:18.160683  3501 solver.cpp:244]     Train net output #0: loss = 24.7513 (* 1 = 24.7513 loss)
I0318 06:55:18.160697  3501 sgd_solver.cpp:106] Iteration 7080, lr = 0.01
I0318 06:55:25.490193  3501 solver.cpp:228] Iteration 7100, loss = 44.6643
I0318 06:55:25.490381  3501 solver.cpp:244]     Train net output #0: loss = 44.6642 (* 1 = 44.6642 loss)
I0318 06:55:25.490396  3501 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I0318 06:55:32.823591  3501 solver.cpp:228] Iteration 7120, loss = 34.7003
I0318 06:55:32.823662  3501 solver.cpp:244]     Train net output #0: loss = 34.7002 (* 1 = 34.7002 loss)
I0318 06:55:32.823674  3501 sgd_solver.cpp:106] Iteration 7120, lr = 0.01
I0318 06:55:40.157788  3501 solver.cpp:228] Iteration 7140, loss = 50.0691
I0318 06:55:40.157853  3501 solver.cpp:244]     Train net output #0: loss = 50.069 (* 1 = 50.069 loss)
I0318 06:55:40.157867  3501 sgd_solver.cpp:106] Iteration 7140, lr = 0.01
I0318 06:55:47.481413  3501 solver.cpp:228] Iteration 7160, loss = 75.7605
I0318 06:55:47.481482  3501 solver.cpp:244]     Train net output #0: loss = 75.7604 (* 1 = 75.7604 loss)
I0318 06:55:47.481495  3501 sgd_solver.cpp:106] Iteration 7160, lr = 0.01
I0318 06:55:54.805435  3501 solver.cpp:228] Iteration 7180, loss = 82.3143
I0318 06:55:54.805501  3501 solver.cpp:244]     Train net output #0: loss = 82.3141 (* 1 = 82.3141 loss)
I0318 06:55:54.805515  3501 sgd_solver.cpp:106] Iteration 7180, lr = 0.01
I0318 06:56:02.129729  3501 solver.cpp:228] Iteration 7200, loss = 71.1945
I0318 06:56:02.129881  3501 solver.cpp:244]     Train net output #0: loss = 71.1944 (* 1 = 71.1944 loss)
I0318 06:56:02.129897  3501 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I0318 06:56:09.460574  3501 solver.cpp:228] Iteration 7220, loss = 67.8237
I0318 06:56:09.460640  3501 solver.cpp:244]     Train net output #0: loss = 67.8236 (* 1 = 67.8236 loss)
I0318 06:56:09.460654  3501 sgd_solver.cpp:106] Iteration 7220, lr = 0.01
I0318 06:56:16.789644  3501 solver.cpp:228] Iteration 7240, loss = 16.7713
I0318 06:56:16.789710  3501 solver.cpp:244]     Train net output #0: loss = 16.7712 (* 1 = 16.7712 loss)
I0318 06:56:16.789722  3501 sgd_solver.cpp:106] Iteration 7240, lr = 0.01
I0318 06:56:20.087888  3501 solver.cpp:337] Iteration 7250, Testing net (#0)
I0318 06:58:14.308197  3501 solver.cpp:404]     Test net output #0: loss = 647.018 (* 1 = 647.018 loss)
I0318 06:58:14.308324  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 06:58:18.259692  3501 solver.cpp:228] Iteration 7260, loss = 24.2264
I0318 06:58:18.259757  3501 solver.cpp:244]     Train net output #0: loss = 24.2263 (* 1 = 24.2263 loss)
I0318 06:58:18.259771  3501 sgd_solver.cpp:106] Iteration 7260, lr = 0.01
I0318 06:58:25.533102  3501 solver.cpp:228] Iteration 7280, loss = 45.2588
I0318 06:58:25.533169  3501 solver.cpp:244]     Train net output #0: loss = 45.2587 (* 1 = 45.2587 loss)
I0318 06:58:25.533181  3501 sgd_solver.cpp:106] Iteration 7280, lr = 0.01
I0318 06:58:32.833200  3501 solver.cpp:228] Iteration 7300, loss = 37.313
I0318 06:58:32.833266  3501 solver.cpp:244]     Train net output #0: loss = 37.3128 (* 1 = 37.3128 loss)
I0318 06:58:32.833281  3501 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I0318 06:58:40.159452  3501 solver.cpp:228] Iteration 7320, loss = 73.4842
I0318 06:58:40.159518  3501 solver.cpp:244]     Train net output #0: loss = 73.484 (* 1 = 73.484 loss)
I0318 06:58:40.159530  3501 sgd_solver.cpp:106] Iteration 7320, lr = 0.01
I0318 06:58:47.494603  3501 solver.cpp:228] Iteration 7340, loss = 118.427
I0318 06:58:47.494797  3501 solver.cpp:244]     Train net output #0: loss = 118.426 (* 1 = 118.426 loss)
I0318 06:58:47.494813  3501 sgd_solver.cpp:106] Iteration 7340, lr = 0.01
I0318 06:58:54.829663  3501 solver.cpp:228] Iteration 7360, loss = 95.2477
I0318 06:58:54.829735  3501 solver.cpp:244]     Train net output #0: loss = 95.2475 (* 1 = 95.2475 loss)
I0318 06:58:54.829751  3501 sgd_solver.cpp:106] Iteration 7360, lr = 0.01
I0318 06:59:02.164638  3501 solver.cpp:228] Iteration 7380, loss = 85.9618
I0318 06:59:02.164715  3501 solver.cpp:244]     Train net output #0: loss = 85.9617 (* 1 = 85.9617 loss)
I0318 06:59:02.164729  3501 sgd_solver.cpp:106] Iteration 7380, lr = 0.01
I0318 06:59:09.501910  3501 solver.cpp:228] Iteration 7400, loss = 28.229
I0318 06:59:09.501976  3501 solver.cpp:244]     Train net output #0: loss = 28.2289 (* 1 = 28.2289 loss)
I0318 06:59:09.501989  3501 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I0318 06:59:16.838731  3501 solver.cpp:228] Iteration 7420, loss = 28.876
I0318 06:59:16.838801  3501 solver.cpp:244]     Train net output #0: loss = 28.8759 (* 1 = 28.8759 loss)
I0318 06:59:16.838815  3501 sgd_solver.cpp:106] Iteration 7420, lr = 0.01
I0318 06:59:24.183776  3501 solver.cpp:228] Iteration 7440, loss = 67.7679
I0318 06:59:24.183928  3501 solver.cpp:244]     Train net output #0: loss = 67.7678 (* 1 = 67.7678 loss)
I0318 06:59:24.183943  3501 sgd_solver.cpp:106] Iteration 7440, lr = 0.01
I0318 06:59:31.526556  3501 solver.cpp:228] Iteration 7460, loss = 64.9503
I0318 06:59:31.526623  3501 solver.cpp:244]     Train net output #0: loss = 64.9502 (* 1 = 64.9502 loss)
I0318 06:59:31.526638  3501 sgd_solver.cpp:106] Iteration 7460, lr = 0.01
I0318 06:59:38.865169  3501 solver.cpp:228] Iteration 7480, loss = 64.2564
I0318 06:59:38.865241  3501 solver.cpp:244]     Train net output #0: loss = 64.2563 (* 1 = 64.2563 loss)
I0318 06:59:38.865254  3501 sgd_solver.cpp:106] Iteration 7480, lr = 0.01
I0318 06:59:45.837213  3501 solver.cpp:337] Iteration 7500, Testing net (#0)
I0318 07:01:40.048429  3501 solver.cpp:404]     Test net output #0: loss = 667.421 (* 1 = 667.421 loss)
I0318 07:01:40.048503  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 07:01:40.384953  3501 solver.cpp:228] Iteration 7500, loss = 44.0828
I0318 07:01:40.385016  3501 solver.cpp:244]     Train net output #0: loss = 44.0827 (* 1 = 44.0827 loss)
I0318 07:01:40.385030  3501 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I0318 07:01:47.621500  3501 solver.cpp:228] Iteration 7520, loss = 59.1322
I0318 07:01:47.621564  3501 solver.cpp:244]     Train net output #0: loss = 59.1321 (* 1 = 59.1321 loss)
I0318 07:01:47.621578  3501 sgd_solver.cpp:106] Iteration 7520, lr = 0.01
I0318 07:01:54.911927  3501 solver.cpp:228] Iteration 7540, loss = 125.027
I0318 07:01:54.911993  3501 solver.cpp:244]     Train net output #0: loss = 125.027 (* 1 = 125.027 loss)
I0318 07:01:54.912006  3501 sgd_solver.cpp:106] Iteration 7540, lr = 0.01
I0318 07:02:02.221345  3501 solver.cpp:228] Iteration 7560, loss = 55.8867
I0318 07:02:02.221416  3501 solver.cpp:244]     Train net output #0: loss = 55.8866 (* 1 = 55.8866 loss)
I0318 07:02:02.221432  3501 sgd_solver.cpp:106] Iteration 7560, lr = 0.01
I0318 07:02:09.536121  3501 solver.cpp:228] Iteration 7580, loss = 69.6363
I0318 07:02:09.536185  3501 solver.cpp:244]     Train net output #0: loss = 69.6362 (* 1 = 69.6362 loss)
I0318 07:02:09.536200  3501 sgd_solver.cpp:106] Iteration 7580, lr = 0.01
I0318 07:02:16.848716  3501 solver.cpp:228] Iteration 7600, loss = 23.5503
I0318 07:02:16.848896  3501 solver.cpp:244]     Train net output #0: loss = 23.5502 (* 1 = 23.5502 loss)
I0318 07:02:16.848911  3501 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I0318 07:02:24.168129  3501 solver.cpp:228] Iteration 7620, loss = 37.0035
I0318 07:02:24.168195  3501 solver.cpp:244]     Train net output #0: loss = 37.0034 (* 1 = 37.0034 loss)
I0318 07:02:24.168207  3501 sgd_solver.cpp:106] Iteration 7620, lr = 0.01
I0318 07:02:31.496999  3501 solver.cpp:228] Iteration 7640, loss = 47.7962
I0318 07:02:31.497064  3501 solver.cpp:244]     Train net output #0: loss = 47.7961 (* 1 = 47.7961 loss)
I0318 07:02:31.497078  3501 sgd_solver.cpp:106] Iteration 7640, lr = 0.01
I0318 07:02:38.829746  3501 solver.cpp:228] Iteration 7660, loss = 96.5129
I0318 07:02:38.829813  3501 solver.cpp:244]     Train net output #0: loss = 96.5128 (* 1 = 96.5128 loss)
I0318 07:02:38.829826  3501 sgd_solver.cpp:106] Iteration 7660, lr = 0.01
I0318 07:02:46.169142  3501 solver.cpp:228] Iteration 7680, loss = 33.5543
I0318 07:02:46.169205  3501 solver.cpp:244]     Train net output #0: loss = 33.5542 (* 1 = 33.5542 loss)
I0318 07:02:46.169219  3501 sgd_solver.cpp:106] Iteration 7680, lr = 0.01
I0318 07:02:53.509476  3501 solver.cpp:228] Iteration 7700, loss = 51.6373
I0318 07:02:53.509640  3501 solver.cpp:244]     Train net output #0: loss = 51.6372 (* 1 = 51.6372 loss)
I0318 07:02:53.509656  3501 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I0318 07:03:00.841490  3501 solver.cpp:228] Iteration 7720, loss = 68.9669
I0318 07:03:00.841553  3501 solver.cpp:244]     Train net output #0: loss = 68.9668 (* 1 = 68.9668 loss)
I0318 07:03:00.841567  3501 sgd_solver.cpp:106] Iteration 7720, lr = 0.01
I0318 07:03:08.170964  3501 solver.cpp:228] Iteration 7740, loss = 40.2164
I0318 07:03:08.171027  3501 solver.cpp:244]     Train net output #0: loss = 40.2163 (* 1 = 40.2163 loss)
I0318 07:03:08.171041  3501 sgd_solver.cpp:106] Iteration 7740, lr = 0.01
I0318 07:03:11.469774  3501 solver.cpp:337] Iteration 7750, Testing net (#0)
I0318 07:05:05.681154  3501 solver.cpp:404]     Test net output #0: loss = 653.619 (* 1 = 653.619 loss)
I0318 07:05:05.681305  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 07:05:09.642015  3501 solver.cpp:228] Iteration 7760, loss = 29.8527
I0318 07:05:09.642074  3501 solver.cpp:244]     Train net output #0: loss = 29.8526 (* 1 = 29.8526 loss)
I0318 07:05:09.642088  3501 sgd_solver.cpp:106] Iteration 7760, lr = 0.01
I0318 07:05:16.916409  3501 solver.cpp:228] Iteration 7780, loss = 21.7566
I0318 07:05:16.916472  3501 solver.cpp:244]     Train net output #0: loss = 21.7565 (* 1 = 21.7565 loss)
I0318 07:05:16.916486  3501 sgd_solver.cpp:106] Iteration 7780, lr = 0.01
I0318 07:05:24.234330  3501 solver.cpp:228] Iteration 7800, loss = 43.0175
I0318 07:05:24.234395  3501 solver.cpp:244]     Train net output #0: loss = 43.0174 (* 1 = 43.0174 loss)
I0318 07:05:24.234408  3501 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I0318 07:05:31.557847  3501 solver.cpp:228] Iteration 7820, loss = 34.379
I0318 07:05:31.557911  3501 solver.cpp:244]     Train net output #0: loss = 34.3789 (* 1 = 34.3789 loss)
I0318 07:05:31.557925  3501 sgd_solver.cpp:106] Iteration 7820, lr = 0.01
I0318 07:05:38.887557  3501 solver.cpp:228] Iteration 7840, loss = 77.9257
I0318 07:05:38.887678  3501 solver.cpp:244]     Train net output #0: loss = 77.9256 (* 1 = 77.9256 loss)
I0318 07:05:38.887693  3501 sgd_solver.cpp:106] Iteration 7840, lr = 0.01
I0318 07:05:46.216414  3501 solver.cpp:228] Iteration 7860, loss = 28.1153
I0318 07:05:46.216478  3501 solver.cpp:244]     Train net output #0: loss = 28.1152 (* 1 = 28.1152 loss)
I0318 07:05:46.216492  3501 sgd_solver.cpp:106] Iteration 7860, lr = 0.01
I0318 07:05:53.543489  3501 solver.cpp:228] Iteration 7880, loss = 73.6144
I0318 07:05:53.543555  3501 solver.cpp:244]     Train net output #0: loss = 73.6143 (* 1 = 73.6143 loss)
I0318 07:05:53.543568  3501 sgd_solver.cpp:106] Iteration 7880, lr = 0.01
I0318 07:06:00.868976  3501 solver.cpp:228] Iteration 7900, loss = 57.2414
I0318 07:06:00.869041  3501 solver.cpp:244]     Train net output #0: loss = 57.2413 (* 1 = 57.2413 loss)
I0318 07:06:00.869055  3501 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I0318 07:06:08.196395  3501 solver.cpp:228] Iteration 7920, loss = 43.4183
I0318 07:06:08.196458  3501 solver.cpp:244]     Train net output #0: loss = 43.4182 (* 1 = 43.4182 loss)
I0318 07:06:08.196472  3501 sgd_solver.cpp:106] Iteration 7920, lr = 0.01
I0318 07:06:15.531155  3501 solver.cpp:228] Iteration 7940, loss = 51.4753
I0318 07:06:15.531334  3501 solver.cpp:244]     Train net output #0: loss = 51.4752 (* 1 = 51.4752 loss)
I0318 07:06:15.531350  3501 sgd_solver.cpp:106] Iteration 7940, lr = 0.01
I0318 07:06:22.855947  3501 solver.cpp:228] Iteration 7960, loss = 47.2272
I0318 07:06:22.856016  3501 solver.cpp:244]     Train net output #0: loss = 47.2271 (* 1 = 47.2271 loss)
I0318 07:06:22.856030  3501 sgd_solver.cpp:106] Iteration 7960, lr = 0.01
I0318 07:06:30.182957  3501 solver.cpp:228] Iteration 7980, loss = 41.6198
I0318 07:06:30.183024  3501 solver.cpp:244]     Train net output #0: loss = 41.6197 (* 1 = 41.6197 loss)
I0318 07:06:30.183039  3501 sgd_solver.cpp:106] Iteration 7980, lr = 0.01
I0318 07:06:37.140087  3501 solver.cpp:337] Iteration 8000, Testing net (#0)
I0318 07:08:31.375481  3501 solver.cpp:404]     Test net output #0: loss = 642.301 (* 1 = 642.301 loss)
I0318 07:08:31.375613  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 07:08:31.711428  3501 solver.cpp:228] Iteration 8000, loss = 33.0123
I0318 07:08:31.711493  3501 solver.cpp:244]     Train net output #0: loss = 33.0122 (* 1 = 33.0122 loss)
I0318 07:08:31.711505  3501 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I0318 07:08:38.968601  3501 solver.cpp:228] Iteration 8020, loss = 55.2216
I0318 07:08:38.968667  3501 solver.cpp:244]     Train net output #0: loss = 55.2215 (* 1 = 55.2215 loss)
I0318 07:08:38.968680  3501 sgd_solver.cpp:106] Iteration 8020, lr = 0.01
I0318 07:08:46.262666  3501 solver.cpp:228] Iteration 8040, loss = 33.7206
I0318 07:08:46.262732  3501 solver.cpp:244]     Train net output #0: loss = 33.7205 (* 1 = 33.7205 loss)
I0318 07:08:46.262745  3501 sgd_solver.cpp:106] Iteration 8040, lr = 0.01
I0318 07:08:53.572888  3501 solver.cpp:228] Iteration 8060, loss = 55.5943
I0318 07:08:53.572953  3501 solver.cpp:244]     Train net output #0: loss = 55.5942 (* 1 = 55.5942 loss)
I0318 07:08:53.572966  3501 sgd_solver.cpp:106] Iteration 8060, lr = 0.01
I0318 07:09:00.897346  3501 solver.cpp:228] Iteration 8080, loss = 52.8143
I0318 07:09:00.897410  3501 solver.cpp:244]     Train net output #0: loss = 52.8142 (* 1 = 52.8142 loss)
I0318 07:09:00.897424  3501 sgd_solver.cpp:106] Iteration 8080, lr = 0.01
I0318 07:09:08.234208  3501 solver.cpp:228] Iteration 8100, loss = 57.6022
I0318 07:09:08.234357  3501 solver.cpp:244]     Train net output #0: loss = 57.6021 (* 1 = 57.6021 loss)
I0318 07:09:08.234372  3501 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I0318 07:09:15.572918  3501 solver.cpp:228] Iteration 8120, loss = 25.5167
I0318 07:09:15.572984  3501 solver.cpp:244]     Train net output #0: loss = 25.5166 (* 1 = 25.5166 loss)
I0318 07:09:15.572998  3501 sgd_solver.cpp:106] Iteration 8120, lr = 0.01
I0318 07:09:22.916366  3501 solver.cpp:228] Iteration 8140, loss = 27.0876
I0318 07:09:22.916431  3501 solver.cpp:244]     Train net output #0: loss = 27.0876 (* 1 = 27.0876 loss)
I0318 07:09:22.916446  3501 sgd_solver.cpp:106] Iteration 8140, lr = 0.01
I0318 07:09:30.263260  3501 solver.cpp:228] Iteration 8160, loss = 42.3363
I0318 07:09:30.263326  3501 solver.cpp:244]     Train net output #0: loss = 42.3362 (* 1 = 42.3362 loss)
I0318 07:09:30.263340  3501 sgd_solver.cpp:106] Iteration 8160, lr = 0.01
I0318 07:09:37.603735  3501 solver.cpp:228] Iteration 8180, loss = 41.7408
I0318 07:09:37.603801  3501 solver.cpp:244]     Train net output #0: loss = 41.7408 (* 1 = 41.7408 loss)
I0318 07:09:37.603814  3501 sgd_solver.cpp:106] Iteration 8180, lr = 0.01
I0318 07:09:44.933748  3501 solver.cpp:228] Iteration 8200, loss = 76.2375
I0318 07:09:44.933926  3501 solver.cpp:244]     Train net output #0: loss = 76.2374 (* 1 = 76.2374 loss)
I0318 07:09:44.933941  3501 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I0318 07:09:52.276120  3501 solver.cpp:228] Iteration 8220, loss = 32.6115
I0318 07:09:52.276185  3501 solver.cpp:244]     Train net output #0: loss = 32.6114 (* 1 = 32.6114 loss)
I0318 07:09:52.276198  3501 sgd_solver.cpp:106] Iteration 8220, lr = 0.01
I0318 07:09:59.615267  3501 solver.cpp:228] Iteration 8240, loss = 75.6313
I0318 07:09:59.615334  3501 solver.cpp:244]     Train net output #0: loss = 75.6312 (* 1 = 75.6312 loss)
I0318 07:09:59.615346  3501 sgd_solver.cpp:106] Iteration 8240, lr = 0.01
I0318 07:10:02.917618  3501 solver.cpp:337] Iteration 8250, Testing net (#0)
I0318 07:11:57.122998  3501 solver.cpp:404]     Test net output #0: loss = 642.383 (* 1 = 642.383 loss)
I0318 07:11:57.123080  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:12:01.075484  3501 solver.cpp:228] Iteration 8260, loss = 67.0536
I0318 07:12:01.075549  3501 solver.cpp:244]     Train net output #0: loss = 67.0536 (* 1 = 67.0536 loss)
I0318 07:12:01.075563  3501 sgd_solver.cpp:106] Iteration 8260, lr = 0.01
I0318 07:12:08.346875  3501 solver.cpp:228] Iteration 8280, loss = 69.7546
I0318 07:12:08.346945  3501 solver.cpp:244]     Train net output #0: loss = 69.7545 (* 1 = 69.7545 loss)
I0318 07:12:08.346958  3501 sgd_solver.cpp:106] Iteration 8280, lr = 0.01
I0318 07:12:15.656929  3501 solver.cpp:228] Iteration 8300, loss = 41.1595
I0318 07:12:15.656993  3501 solver.cpp:244]     Train net output #0: loss = 41.1595 (* 1 = 41.1595 loss)
I0318 07:12:15.657006  3501 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I0318 07:12:22.975946  3501 solver.cpp:228] Iteration 8320, loss = 22.4542
I0318 07:12:22.976011  3501 solver.cpp:244]     Train net output #0: loss = 22.4541 (* 1 = 22.4541 loss)
I0318 07:12:22.976024  3501 sgd_solver.cpp:106] Iteration 8320, lr = 0.01
I0318 07:12:30.305467  3501 solver.cpp:228] Iteration 8340, loss = 34.5089
I0318 07:12:30.305611  3501 solver.cpp:244]     Train net output #0: loss = 34.5088 (* 1 = 34.5088 loss)
I0318 07:12:30.305626  3501 sgd_solver.cpp:106] Iteration 8340, lr = 0.01
I0318 07:12:37.636292  3501 solver.cpp:228] Iteration 8360, loss = 52.0035
I0318 07:12:37.636368  3501 solver.cpp:244]     Train net output #0: loss = 52.0034 (* 1 = 52.0034 loss)
I0318 07:12:37.636381  3501 sgd_solver.cpp:106] Iteration 8360, lr = 0.01
I0318 07:12:44.963207  3501 solver.cpp:228] Iteration 8380, loss = 48.988
I0318 07:12:44.963271  3501 solver.cpp:244]     Train net output #0: loss = 48.9879 (* 1 = 48.9879 loss)
I0318 07:12:44.963285  3501 sgd_solver.cpp:106] Iteration 8380, lr = 0.01
I0318 07:12:52.305160  3501 solver.cpp:228] Iteration 8400, loss = 40.5948
I0318 07:12:52.305227  3501 solver.cpp:244]     Train net output #0: loss = 40.5947 (* 1 = 40.5947 loss)
I0318 07:12:52.305239  3501 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
I0318 07:12:59.647549  3501 solver.cpp:228] Iteration 8420, loss = 65.8282
I0318 07:12:59.647614  3501 solver.cpp:244]     Train net output #0: loss = 65.8281 (* 1 = 65.8281 loss)
I0318 07:12:59.647629  3501 sgd_solver.cpp:106] Iteration 8420, lr = 0.01
I0318 07:13:06.985990  3501 solver.cpp:228] Iteration 8440, loss = 80.2187
I0318 07:13:06.986136  3501 solver.cpp:244]     Train net output #0: loss = 80.2186 (* 1 = 80.2186 loss)
I0318 07:13:06.986150  3501 sgd_solver.cpp:106] Iteration 8440, lr = 0.01
I0318 07:13:14.323771  3501 solver.cpp:228] Iteration 8460, loss = 83.5279
I0318 07:13:14.323837  3501 solver.cpp:244]     Train net output #0: loss = 83.5278 (* 1 = 83.5278 loss)
I0318 07:13:14.323850  3501 sgd_solver.cpp:106] Iteration 8460, lr = 0.01
I0318 07:13:21.655239  3501 solver.cpp:228] Iteration 8480, loss = 42.5836
I0318 07:13:21.655313  3501 solver.cpp:244]     Train net output #0: loss = 42.5836 (* 1 = 42.5836 loss)
I0318 07:13:21.655328  3501 sgd_solver.cpp:106] Iteration 8480, lr = 0.01
I0318 07:13:28.618369  3501 solver.cpp:337] Iteration 8500, Testing net (#0)
I0318 07:15:22.828605  3501 solver.cpp:404]     Test net output #0: loss = 635.198 (* 1 = 635.198 loss)
I0318 07:15:22.828763  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:15:23.166148  3501 solver.cpp:228] Iteration 8500, loss = 36.4169
I0318 07:15:23.166210  3501 solver.cpp:244]     Train net output #0: loss = 36.4168 (* 1 = 36.4168 loss)
I0318 07:15:23.166224  3501 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I0318 07:15:30.437183  3501 solver.cpp:228] Iteration 8520, loss = 38.1074
I0318 07:15:30.437260  3501 solver.cpp:244]     Train net output #0: loss = 38.1074 (* 1 = 38.1074 loss)
I0318 07:15:30.437275  3501 sgd_solver.cpp:106] Iteration 8520, lr = 0.01
I0318 07:15:37.757309  3501 solver.cpp:228] Iteration 8540, loss = 32.7873
I0318 07:15:37.757387  3501 solver.cpp:244]     Train net output #0: loss = 32.7872 (* 1 = 32.7872 loss)
I0318 07:15:37.757405  3501 sgd_solver.cpp:106] Iteration 8540, lr = 0.01
I0318 07:15:45.089177  3501 solver.cpp:228] Iteration 8560, loss = 54.526
I0318 07:15:45.089246  3501 solver.cpp:244]     Train net output #0: loss = 54.526 (* 1 = 54.526 loss)
I0318 07:15:45.089258  3501 sgd_solver.cpp:106] Iteration 8560, lr = 0.01
I0318 07:15:52.425092  3501 solver.cpp:228] Iteration 8580, loss = 46.0956
I0318 07:15:52.425156  3501 solver.cpp:244]     Train net output #0: loss = 46.0955 (* 1 = 46.0955 loss)
I0318 07:15:52.425170  3501 sgd_solver.cpp:106] Iteration 8580, lr = 0.01
I0318 07:15:59.762667  3501 solver.cpp:228] Iteration 8600, loss = 44.9171
I0318 07:15:59.762809  3501 solver.cpp:244]     Train net output #0: loss = 44.917 (* 1 = 44.917 loss)
I0318 07:15:59.762825  3501 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I0318 07:16:07.100869  3501 solver.cpp:228] Iteration 8620, loss = 48.4738
I0318 07:16:07.100934  3501 solver.cpp:244]     Train net output #0: loss = 48.4737 (* 1 = 48.4737 loss)
I0318 07:16:07.100947  3501 sgd_solver.cpp:106] Iteration 8620, lr = 0.01
I0318 07:16:14.432190  3501 solver.cpp:228] Iteration 8640, loss = 59.8179
I0318 07:16:14.432258  3501 solver.cpp:244]     Train net output #0: loss = 59.8179 (* 1 = 59.8179 loss)
I0318 07:16:14.432272  3501 sgd_solver.cpp:106] Iteration 8640, lr = 0.01
I0318 07:16:21.762581  3501 solver.cpp:228] Iteration 8660, loss = 50.6358
I0318 07:16:21.762653  3501 solver.cpp:244]     Train net output #0: loss = 50.6358 (* 1 = 50.6358 loss)
I0318 07:16:21.762668  3501 sgd_solver.cpp:106] Iteration 8660, lr = 0.01
I0318 07:16:29.094000  3501 solver.cpp:228] Iteration 8680, loss = 37.0872
I0318 07:16:29.094065  3501 solver.cpp:244]     Train net output #0: loss = 37.0872 (* 1 = 37.0872 loss)
I0318 07:16:29.094079  3501 sgd_solver.cpp:106] Iteration 8680, lr = 0.01
I0318 07:16:36.432627  3501 solver.cpp:228] Iteration 8700, loss = 46.392
I0318 07:16:36.432765  3501 solver.cpp:244]     Train net output #0: loss = 46.392 (* 1 = 46.392 loss)
I0318 07:16:36.432780  3501 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I0318 07:16:43.768118  3501 solver.cpp:228] Iteration 8720, loss = 46.6166
I0318 07:16:43.768183  3501 solver.cpp:244]     Train net output #0: loss = 46.6165 (* 1 = 46.6165 loss)
I0318 07:16:43.768198  3501 sgd_solver.cpp:106] Iteration 8720, lr = 0.01
I0318 07:16:51.106981  3501 solver.cpp:228] Iteration 8740, loss = 29.969
I0318 07:16:51.107046  3501 solver.cpp:244]     Train net output #0: loss = 29.969 (* 1 = 29.969 loss)
I0318 07:16:51.107060  3501 sgd_solver.cpp:106] Iteration 8740, lr = 0.01
I0318 07:16:54.409274  3501 solver.cpp:337] Iteration 8750, Testing net (#0)
I0318 07:18:48.608701  3501 solver.cpp:404]     Test net output #0: loss = 591.858 (* 1 = 591.858 loss)
I0318 07:18:48.608815  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 07:18:52.555994  3501 solver.cpp:228] Iteration 8760, loss = 39.0609
I0318 07:18:52.556061  3501 solver.cpp:244]     Train net output #0: loss = 39.0609 (* 1 = 39.0609 loss)
I0318 07:18:52.556073  3501 sgd_solver.cpp:106] Iteration 8760, lr = 0.01
I0318 07:18:59.840018  3501 solver.cpp:228] Iteration 8780, loss = 32.4997
I0318 07:18:59.840086  3501 solver.cpp:244]     Train net output #0: loss = 32.4996 (* 1 = 32.4996 loss)
I0318 07:18:59.840101  3501 sgd_solver.cpp:106] Iteration 8780, lr = 0.01
I0318 07:19:07.156774  3501 solver.cpp:228] Iteration 8800, loss = 110.25
I0318 07:19:07.156849  3501 solver.cpp:244]     Train net output #0: loss = 110.25 (* 1 = 110.25 loss)
I0318 07:19:07.156864  3501 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I0318 07:19:14.489037  3501 solver.cpp:228] Iteration 8820, loss = 33.8376
I0318 07:19:14.489102  3501 solver.cpp:244]     Train net output #0: loss = 33.8376 (* 1 = 33.8376 loss)
I0318 07:19:14.489116  3501 sgd_solver.cpp:106] Iteration 8820, lr = 0.01
I0318 07:19:21.821604  3501 solver.cpp:228] Iteration 8840, loss = 29.2288
I0318 07:19:21.821784  3501 solver.cpp:244]     Train net output #0: loss = 29.2287 (* 1 = 29.2287 loss)
I0318 07:19:21.821799  3501 sgd_solver.cpp:106] Iteration 8840, lr = 0.01
I0318 07:19:29.161396  3501 solver.cpp:228] Iteration 8860, loss = 26.7216
I0318 07:19:29.161460  3501 solver.cpp:244]     Train net output #0: loss = 26.7215 (* 1 = 26.7215 loss)
I0318 07:19:29.161473  3501 sgd_solver.cpp:106] Iteration 8860, lr = 0.01
I0318 07:19:36.497066  3501 solver.cpp:228] Iteration 8880, loss = 38.5723
I0318 07:19:36.497138  3501 solver.cpp:244]     Train net output #0: loss = 38.5722 (* 1 = 38.5722 loss)
I0318 07:19:36.497153  3501 sgd_solver.cpp:106] Iteration 8880, lr = 0.01
I0318 07:19:43.836676  3501 solver.cpp:228] Iteration 8900, loss = 46.918
I0318 07:19:43.836742  3501 solver.cpp:244]     Train net output #0: loss = 46.918 (* 1 = 46.918 loss)
I0318 07:19:43.836756  3501 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I0318 07:19:51.175941  3501 solver.cpp:228] Iteration 8920, loss = 36.43
I0318 07:19:51.176007  3501 solver.cpp:244]     Train net output #0: loss = 36.4299 (* 1 = 36.4299 loss)
I0318 07:19:51.176021  3501 sgd_solver.cpp:106] Iteration 8920, lr = 0.01
I0318 07:19:58.513659  3501 solver.cpp:228] Iteration 8940, loss = 41.2203
I0318 07:19:58.513813  3501 solver.cpp:244]     Train net output #0: loss = 41.2202 (* 1 = 41.2202 loss)
I0318 07:19:58.513826  3501 sgd_solver.cpp:106] Iteration 8940, lr = 0.01
I0318 07:20:05.839730  3501 solver.cpp:228] Iteration 8960, loss = 68.3541
I0318 07:20:05.839797  3501 solver.cpp:244]     Train net output #0: loss = 68.3541 (* 1 = 68.3541 loss)
I0318 07:20:05.839812  3501 sgd_solver.cpp:106] Iteration 8960, lr = 0.01
I0318 07:20:13.168853  3501 solver.cpp:228] Iteration 8980, loss = 73.3954
I0318 07:20:13.168915  3501 solver.cpp:244]     Train net output #0: loss = 73.3954 (* 1 = 73.3954 loss)
I0318 07:20:13.168928  3501 sgd_solver.cpp:106] Iteration 8980, lr = 0.01
I0318 07:20:20.134431  3501 solver.cpp:337] Iteration 9000, Testing net (#0)
I0318 07:22:14.376376  3501 solver.cpp:404]     Test net output #0: loss = 627.272 (* 1 = 627.272 loss)
I0318 07:22:14.376487  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 07:22:14.713887  3501 solver.cpp:228] Iteration 9000, loss = 86.5276
I0318 07:22:14.713949  3501 solver.cpp:244]     Train net output #0: loss = 86.5275 (* 1 = 86.5275 loss)
I0318 07:22:14.713963  3501 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I0318 07:22:21.963402  3501 solver.cpp:228] Iteration 9020, loss = 37.6607
I0318 07:22:21.963469  3501 solver.cpp:244]     Train net output #0: loss = 37.6606 (* 1 = 37.6606 loss)
I0318 07:22:21.963482  3501 sgd_solver.cpp:106] Iteration 9020, lr = 0.01
I0318 07:22:29.255614  3501 solver.cpp:228] Iteration 9040, loss = 15.6623
I0318 07:22:29.255678  3501 solver.cpp:244]     Train net output #0: loss = 15.6622 (* 1 = 15.6622 loss)
I0318 07:22:29.255692  3501 sgd_solver.cpp:106] Iteration 9040, lr = 0.01
I0318 07:22:36.564525  3501 solver.cpp:228] Iteration 9060, loss = 14.3281
I0318 07:22:36.564591  3501 solver.cpp:244]     Train net output #0: loss = 14.3281 (* 1 = 14.3281 loss)
I0318 07:22:36.564605  3501 sgd_solver.cpp:106] Iteration 9060, lr = 0.01
I0318 07:22:43.888800  3501 solver.cpp:228] Iteration 9080, loss = 32.4594
I0318 07:22:43.888869  3501 solver.cpp:244]     Train net output #0: loss = 32.4593 (* 1 = 32.4593 loss)
I0318 07:22:43.888882  3501 sgd_solver.cpp:106] Iteration 9080, lr = 0.01
I0318 07:22:51.221035  3501 solver.cpp:228] Iteration 9100, loss = 61.9731
I0318 07:22:51.221240  3501 solver.cpp:244]     Train net output #0: loss = 61.9731 (* 1 = 61.9731 loss)
I0318 07:22:51.221256  3501 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I0318 07:22:58.555827  3501 solver.cpp:228] Iteration 9120, loss = 36.4193
I0318 07:22:58.555898  3501 solver.cpp:244]     Train net output #0: loss = 36.4192 (* 1 = 36.4192 loss)
I0318 07:22:58.555912  3501 sgd_solver.cpp:106] Iteration 9120, lr = 0.01
I0318 07:23:05.890765  3501 solver.cpp:228] Iteration 9140, loss = 63.2533
I0318 07:23:05.890833  3501 solver.cpp:244]     Train net output #0: loss = 63.2532 (* 1 = 63.2532 loss)
I0318 07:23:05.890848  3501 sgd_solver.cpp:106] Iteration 9140, lr = 0.01
I0318 07:23:13.232456  3501 solver.cpp:228] Iteration 9160, loss = 76.7045
I0318 07:23:13.232517  3501 solver.cpp:244]     Train net output #0: loss = 76.7044 (* 1 = 76.7044 loss)
I0318 07:23:13.232532  3501 sgd_solver.cpp:106] Iteration 9160, lr = 0.01
I0318 07:23:20.573127  3501 solver.cpp:228] Iteration 9180, loss = 42.2735
I0318 07:23:20.573195  3501 solver.cpp:244]     Train net output #0: loss = 42.2734 (* 1 = 42.2734 loss)
I0318 07:23:20.573210  3501 sgd_solver.cpp:106] Iteration 9180, lr = 0.01
I0318 07:23:27.913434  3501 solver.cpp:228] Iteration 9200, loss = 41.4947
I0318 07:23:27.913595  3501 solver.cpp:244]     Train net output #0: loss = 41.4947 (* 1 = 41.4947 loss)
I0318 07:23:27.913611  3501 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I0318 07:23:35.256886  3501 solver.cpp:228] Iteration 9220, loss = 18.1276
I0318 07:23:35.256950  3501 solver.cpp:244]     Train net output #0: loss = 18.1276 (* 1 = 18.1276 loss)
I0318 07:23:35.256964  3501 sgd_solver.cpp:106] Iteration 9220, lr = 0.01
I0318 07:23:42.591853  3501 solver.cpp:228] Iteration 9240, loss = 17.5039
I0318 07:23:42.591920  3501 solver.cpp:244]     Train net output #0: loss = 17.5039 (* 1 = 17.5039 loss)
I0318 07:23:42.591934  3501 sgd_solver.cpp:106] Iteration 9240, lr = 0.01
I0318 07:23:45.894114  3501 solver.cpp:337] Iteration 9250, Testing net (#0)
I0318 07:25:40.085232  3501 solver.cpp:404]     Test net output #0: loss = 636.76 (* 1 = 636.76 loss)
I0318 07:25:40.085351  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:25:44.031846  3501 solver.cpp:228] Iteration 9260, loss = 40.0607
I0318 07:25:44.031913  3501 solver.cpp:244]     Train net output #0: loss = 40.0606 (* 1 = 40.0606 loss)
I0318 07:25:44.031927  3501 sgd_solver.cpp:106] Iteration 9260, lr = 0.01
I0318 07:25:51.294711  3501 solver.cpp:228] Iteration 9280, loss = 36.2555
I0318 07:25:51.294775  3501 solver.cpp:244]     Train net output #0: loss = 36.2555 (* 1 = 36.2555 loss)
I0318 07:25:51.294790  3501 sgd_solver.cpp:106] Iteration 9280, lr = 0.01
I0318 07:25:58.588157  3501 solver.cpp:228] Iteration 9300, loss = 42.3807
I0318 07:25:58.588229  3501 solver.cpp:244]     Train net output #0: loss = 42.3807 (* 1 = 42.3807 loss)
I0318 07:25:58.588244  3501 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I0318 07:26:05.901242  3501 solver.cpp:228] Iteration 9320, loss = 105.659
I0318 07:26:05.901316  3501 solver.cpp:244]     Train net output #0: loss = 105.659 (* 1 = 105.659 loss)
I0318 07:26:05.901331  3501 sgd_solver.cpp:106] Iteration 9320, lr = 0.01
I0318 07:26:13.227527  3501 solver.cpp:228] Iteration 9340, loss = 58.1356
I0318 07:26:13.227666  3501 solver.cpp:244]     Train net output #0: loss = 58.1355 (* 1 = 58.1355 loss)
I0318 07:26:13.227682  3501 sgd_solver.cpp:106] Iteration 9340, lr = 0.01
I0318 07:26:20.560119  3501 solver.cpp:228] Iteration 9360, loss = 39.2986
I0318 07:26:20.560185  3501 solver.cpp:244]     Train net output #0: loss = 39.2985 (* 1 = 39.2985 loss)
I0318 07:26:20.560199  3501 sgd_solver.cpp:106] Iteration 9360, lr = 0.01
I0318 07:26:27.895776  3501 solver.cpp:228] Iteration 9380, loss = 24.9677
I0318 07:26:27.895839  3501 solver.cpp:244]     Train net output #0: loss = 24.9677 (* 1 = 24.9677 loss)
I0318 07:26:27.895853  3501 sgd_solver.cpp:106] Iteration 9380, lr = 0.01
I0318 07:26:35.233350  3501 solver.cpp:228] Iteration 9400, loss = 13.455
I0318 07:26:35.233415  3501 solver.cpp:244]     Train net output #0: loss = 13.4549 (* 1 = 13.4549 loss)
I0318 07:26:35.233429  3501 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
I0318 07:26:42.578063  3501 solver.cpp:228] Iteration 9420, loss = 33.5525
I0318 07:26:42.578128  3501 solver.cpp:244]     Train net output #0: loss = 33.5524 (* 1 = 33.5524 loss)
I0318 07:26:42.578142  3501 sgd_solver.cpp:106] Iteration 9420, lr = 0.01
I0318 07:26:49.927530  3501 solver.cpp:228] Iteration 9440, loss = 34.1096
I0318 07:26:49.927700  3501 solver.cpp:244]     Train net output #0: loss = 34.1095 (* 1 = 34.1095 loss)
I0318 07:26:49.927716  3501 sgd_solver.cpp:106] Iteration 9440, lr = 0.01
I0318 07:26:57.277431  3501 solver.cpp:228] Iteration 9460, loss = 30.2
I0318 07:26:57.277495  3501 solver.cpp:244]     Train net output #0: loss = 30.2 (* 1 = 30.2 loss)
I0318 07:26:57.277509  3501 sgd_solver.cpp:106] Iteration 9460, lr = 0.01
I0318 07:27:04.611059  3501 solver.cpp:228] Iteration 9480, loss = 39.0066
I0318 07:27:04.611125  3501 solver.cpp:244]     Train net output #0: loss = 39.0065 (* 1 = 39.0065 loss)
I0318 07:27:04.611140  3501 sgd_solver.cpp:106] Iteration 9480, lr = 0.01
I0318 07:27:11.575541  3501 solver.cpp:337] Iteration 9500, Testing net (#0)
I0318 07:29:05.798765  3501 solver.cpp:404]     Test net output #0: loss = 676.289 (* 1 = 676.289 loss)
I0318 07:29:05.798897  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:29:06.134809  3501 solver.cpp:228] Iteration 9500, loss = 50.3865
I0318 07:29:06.134874  3501 solver.cpp:244]     Train net output #0: loss = 50.3865 (* 1 = 50.3865 loss)
I0318 07:29:06.134888  3501 sgd_solver.cpp:106] Iteration 9500, lr = 0.01
I0318 07:29:13.368993  3501 solver.cpp:228] Iteration 9520, loss = 118.983
I0318 07:29:13.369065  3501 solver.cpp:244]     Train net output #0: loss = 118.983 (* 1 = 118.983 loss)
I0318 07:29:13.369081  3501 sgd_solver.cpp:106] Iteration 9520, lr = 0.01
I0318 07:29:20.651024  3501 solver.cpp:228] Iteration 9540, loss = 68.5069
I0318 07:29:20.651088  3501 solver.cpp:244]     Train net output #0: loss = 68.5068 (* 1 = 68.5068 loss)
I0318 07:29:20.651103  3501 sgd_solver.cpp:106] Iteration 9540, lr = 0.01
I0318 07:29:27.954788  3501 solver.cpp:228] Iteration 9560, loss = 32.8403
I0318 07:29:27.954855  3501 solver.cpp:244]     Train net output #0: loss = 32.8402 (* 1 = 32.8402 loss)
I0318 07:29:27.954869  3501 sgd_solver.cpp:106] Iteration 9560, lr = 0.01
I0318 07:29:35.271105  3501 solver.cpp:228] Iteration 9580, loss = 14.7268
I0318 07:29:35.271169  3501 solver.cpp:244]     Train net output #0: loss = 14.7267 (* 1 = 14.7267 loss)
I0318 07:29:35.271183  3501 sgd_solver.cpp:106] Iteration 9580, lr = 0.01
I0318 07:29:42.602005  3501 solver.cpp:228] Iteration 9600, loss = 36.8284
I0318 07:29:42.602140  3501 solver.cpp:244]     Train net output #0: loss = 36.8284 (* 1 = 36.8284 loss)
I0318 07:29:42.602156  3501 sgd_solver.cpp:106] Iteration 9600, lr = 0.01
I0318 07:29:49.944768  3501 solver.cpp:228] Iteration 9620, loss = 23.9788
I0318 07:29:49.944834  3501 solver.cpp:244]     Train net output #0: loss = 23.9787 (* 1 = 23.9787 loss)
I0318 07:29:49.944847  3501 sgd_solver.cpp:106] Iteration 9620, lr = 0.01
I0318 07:29:57.278565  3501 solver.cpp:228] Iteration 9640, loss = 34.1306
I0318 07:29:57.278630  3501 solver.cpp:244]     Train net output #0: loss = 34.1306 (* 1 = 34.1306 loss)
I0318 07:29:57.278645  3501 sgd_solver.cpp:106] Iteration 9640, lr = 0.01
I0318 07:30:04.620677  3501 solver.cpp:228] Iteration 9660, loss = 47.2517
I0318 07:30:04.620745  3501 solver.cpp:244]     Train net output #0: loss = 47.2516 (* 1 = 47.2516 loss)
I0318 07:30:04.620759  3501 sgd_solver.cpp:106] Iteration 9660, lr = 0.01
I0318 07:30:11.950604  3501 solver.cpp:228] Iteration 9680, loss = 33.8031
I0318 07:30:11.950682  3501 solver.cpp:244]     Train net output #0: loss = 33.803 (* 1 = 33.803 loss)
I0318 07:30:11.950698  3501 sgd_solver.cpp:106] Iteration 9680, lr = 0.01
I0318 07:30:19.272148  3501 solver.cpp:228] Iteration 9700, loss = 92.3259
I0318 07:30:19.272346  3501 solver.cpp:244]     Train net output #0: loss = 92.3258 (* 1 = 92.3258 loss)
I0318 07:30:19.272363  3501 sgd_solver.cpp:106] Iteration 9700, lr = 0.01
I0318 07:30:26.590061  3501 solver.cpp:228] Iteration 9720, loss = 43.2512
I0318 07:30:26.590134  3501 solver.cpp:244]     Train net output #0: loss = 43.2512 (* 1 = 43.2512 loss)
I0318 07:30:26.590149  3501 sgd_solver.cpp:106] Iteration 9720, lr = 0.01
I0318 07:30:33.911751  3501 solver.cpp:228] Iteration 9740, loss = 35.6604
I0318 07:30:33.911819  3501 solver.cpp:244]     Train net output #0: loss = 35.6604 (* 1 = 35.6604 loss)
I0318 07:30:33.911834  3501 sgd_solver.cpp:106] Iteration 9740, lr = 0.01
I0318 07:30:37.205490  3501 solver.cpp:337] Iteration 9750, Testing net (#0)
I0318 07:32:31.436921  3501 solver.cpp:404]     Test net output #0: loss = 623.679 (* 1 = 623.679 loss)
I0318 07:32:31.437057  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:32:35.398371  3501 solver.cpp:228] Iteration 9760, loss = 15.7028
I0318 07:32:35.398435  3501 solver.cpp:244]     Train net output #0: loss = 15.7027 (* 1 = 15.7027 loss)
I0318 07:32:35.398449  3501 sgd_solver.cpp:106] Iteration 9760, lr = 0.01
I0318 07:32:42.683945  3501 solver.cpp:228] Iteration 9780, loss = 32.6512
I0318 07:32:42.684010  3501 solver.cpp:244]     Train net output #0: loss = 32.6512 (* 1 = 32.6512 loss)
I0318 07:32:42.684022  3501 sgd_solver.cpp:106] Iteration 9780, lr = 0.01
I0318 07:32:50.001385  3501 solver.cpp:228] Iteration 9800, loss = 35.5583
I0318 07:32:50.001452  3501 solver.cpp:244]     Train net output #0: loss = 35.5583 (* 1 = 35.5583 loss)
I0318 07:32:50.001466  3501 sgd_solver.cpp:106] Iteration 9800, lr = 0.01
I0318 07:32:57.315791  3501 solver.cpp:228] Iteration 9820, loss = 67.1931
I0318 07:32:57.315861  3501 solver.cpp:244]     Train net output #0: loss = 67.193 (* 1 = 67.193 loss)
I0318 07:32:57.315876  3501 sgd_solver.cpp:106] Iteration 9820, lr = 0.01
I0318 07:33:04.635805  3501 solver.cpp:228] Iteration 9840, loss = 29.4608
I0318 07:33:04.635946  3501 solver.cpp:244]     Train net output #0: loss = 29.4608 (* 1 = 29.4608 loss)
I0318 07:33:04.635962  3501 sgd_solver.cpp:106] Iteration 9840, lr = 0.01
I0318 07:33:11.964629  3501 solver.cpp:228] Iteration 9860, loss = 90.6451
I0318 07:33:11.964694  3501 solver.cpp:244]     Train net output #0: loss = 90.645 (* 1 = 90.645 loss)
I0318 07:33:11.964707  3501 sgd_solver.cpp:106] Iteration 9860, lr = 0.01
I0318 07:33:19.285430  3501 solver.cpp:228] Iteration 9880, loss = 97.2805
I0318 07:33:19.285495  3501 solver.cpp:244]     Train net output #0: loss = 97.2805 (* 1 = 97.2805 loss)
I0318 07:33:19.285508  3501 sgd_solver.cpp:106] Iteration 9880, lr = 0.01
I0318 07:33:26.612249  3501 solver.cpp:228] Iteration 9900, loss = 93.7045
I0318 07:33:26.612325  3501 solver.cpp:244]     Train net output #0: loss = 93.7044 (* 1 = 93.7044 loss)
I0318 07:33:26.612345  3501 sgd_solver.cpp:106] Iteration 9900, lr = 0.01
I0318 07:33:33.937212  3501 solver.cpp:228] Iteration 9920, loss = 59.0246
I0318 07:33:33.937276  3501 solver.cpp:244]     Train net output #0: loss = 59.0246 (* 1 = 59.0246 loss)
I0318 07:33:33.937290  3501 sgd_solver.cpp:106] Iteration 9920, lr = 0.01
I0318 07:33:41.266054  3501 solver.cpp:228] Iteration 9940, loss = 19.0167
I0318 07:33:41.266203  3501 solver.cpp:244]     Train net output #0: loss = 19.0166 (* 1 = 19.0166 loss)
I0318 07:33:41.266219  3501 sgd_solver.cpp:106] Iteration 9940, lr = 0.01
I0318 07:33:48.596494  3501 solver.cpp:228] Iteration 9960, loss = 17.7814
I0318 07:33:48.596560  3501 solver.cpp:244]     Train net output #0: loss = 17.7813 (* 1 = 17.7813 loss)
I0318 07:33:48.596572  3501 sgd_solver.cpp:106] Iteration 9960, lr = 0.01
I0318 07:33:55.939457  3501 solver.cpp:228] Iteration 9980, loss = 18.0478
I0318 07:33:55.939523  3501 solver.cpp:244]     Train net output #0: loss = 18.0477 (* 1 = 18.0477 loss)
I0318 07:33:55.939537  3501 sgd_solver.cpp:106] Iteration 9980, lr = 0.01
I0318 07:34:02.914460  3501 solver.cpp:454] Snapshotting to binary proto file ./caffe_alexnet_train_iter_10000.caffemodel
I0318 07:34:04.539683  3501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./caffe_alexnet_train_iter_10000.solverstate
I0318 07:34:04.941951  3501 solver.cpp:337] Iteration 10000, Testing net (#0)
I0318 07:35:59.083158  3501 solver.cpp:404]     Test net output #0: loss = 652.527 (* 1 = 652.527 loss)
I0318 07:35:59.083323  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 07:35:59.419849  3501 solver.cpp:228] Iteration 10000, loss = 66.479
I0318 07:35:59.419914  3501 solver.cpp:244]     Train net output #0: loss = 66.479 (* 1 = 66.479 loss)
I0318 07:35:59.419927  3501 sgd_solver.cpp:106] Iteration 10000, lr = 0.01
I0318 07:36:06.681319  3501 solver.cpp:228] Iteration 10020, loss = 60.929
I0318 07:36:06.681388  3501 solver.cpp:244]     Train net output #0: loss = 60.9289 (* 1 = 60.9289 loss)
I0318 07:36:06.681403  3501 sgd_solver.cpp:106] Iteration 10020, lr = 0.01
I0318 07:36:13.983893  3501 solver.cpp:228] Iteration 10040, loss = 66.0777
I0318 07:36:13.983959  3501 solver.cpp:244]     Train net output #0: loss = 66.0776 (* 1 = 66.0776 loss)
I0318 07:36:13.983973  3501 sgd_solver.cpp:106] Iteration 10040, lr = 0.01
I0318 07:36:21.294924  3501 solver.cpp:228] Iteration 10060, loss = 69.4135
I0318 07:36:21.294991  3501 solver.cpp:244]     Train net output #0: loss = 69.4134 (* 1 = 69.4134 loss)
I0318 07:36:21.295004  3501 sgd_solver.cpp:106] Iteration 10060, lr = 0.01
I0318 07:36:28.619961  3501 solver.cpp:228] Iteration 10080, loss = 38.6457
I0318 07:36:28.620038  3501 solver.cpp:244]     Train net output #0: loss = 38.6456 (* 1 = 38.6456 loss)
I0318 07:36:28.620052  3501 sgd_solver.cpp:106] Iteration 10080, lr = 0.01
I0318 07:36:35.963984  3501 solver.cpp:228] Iteration 10100, loss = 34.0422
I0318 07:36:35.964169  3501 solver.cpp:244]     Train net output #0: loss = 34.0421 (* 1 = 34.0421 loss)
I0318 07:36:35.964185  3501 sgd_solver.cpp:106] Iteration 10100, lr = 0.01
I0318 07:36:43.305392  3501 solver.cpp:228] Iteration 10120, loss = 28.3412
I0318 07:36:43.305464  3501 solver.cpp:244]     Train net output #0: loss = 28.3411 (* 1 = 28.3411 loss)
I0318 07:36:43.305480  3501 sgd_solver.cpp:106] Iteration 10120, lr = 0.01
I0318 07:36:50.643328  3501 solver.cpp:228] Iteration 10140, loss = 7.68373
I0318 07:36:50.643395  3501 solver.cpp:244]     Train net output #0: loss = 7.68363 (* 1 = 7.68363 loss)
I0318 07:36:50.643409  3501 sgd_solver.cpp:106] Iteration 10140, lr = 0.01
I0318 07:36:57.985306  3501 solver.cpp:228] Iteration 10160, loss = 22.6613
I0318 07:36:57.985373  3501 solver.cpp:244]     Train net output #0: loss = 22.6612 (* 1 = 22.6612 loss)
I0318 07:36:57.985386  3501 sgd_solver.cpp:106] Iteration 10160, lr = 0.01
I0318 07:37:05.320034  3501 solver.cpp:228] Iteration 10180, loss = 30.351
I0318 07:37:05.320099  3501 solver.cpp:244]     Train net output #0: loss = 30.3509 (* 1 = 30.3509 loss)
I0318 07:37:05.320113  3501 sgd_solver.cpp:106] Iteration 10180, lr = 0.01
I0318 07:37:12.644529  3501 solver.cpp:228] Iteration 10200, loss = 57.9109
I0318 07:37:12.644670  3501 solver.cpp:244]     Train net output #0: loss = 57.9108 (* 1 = 57.9108 loss)
I0318 07:37:12.644685  3501 sgd_solver.cpp:106] Iteration 10200, lr = 0.01
I0318 07:37:19.973073  3501 solver.cpp:228] Iteration 10220, loss = 67.4144
I0318 07:37:19.973139  3501 solver.cpp:244]     Train net output #0: loss = 67.4143 (* 1 = 67.4143 loss)
I0318 07:37:19.973152  3501 sgd_solver.cpp:106] Iteration 10220, lr = 0.01
I0318 07:37:27.295840  3501 solver.cpp:228] Iteration 10240, loss = 57.8898
I0318 07:37:27.295904  3501 solver.cpp:244]     Train net output #0: loss = 57.8897 (* 1 = 57.8897 loss)
I0318 07:37:27.295917  3501 sgd_solver.cpp:106] Iteration 10240, lr = 0.01
I0318 07:37:30.593209  3501 solver.cpp:337] Iteration 10250, Testing net (#0)
I0318 07:39:24.804335  3501 solver.cpp:404]     Test net output #0: loss = 634.302 (* 1 = 634.302 loss)
I0318 07:39:24.804505  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 07:39:28.772835  3501 solver.cpp:228] Iteration 10260, loss = 55.4709
I0318 07:39:28.772899  3501 solver.cpp:244]     Train net output #0: loss = 55.4708 (* 1 = 55.4708 loss)
I0318 07:39:28.772913  3501 sgd_solver.cpp:106] Iteration 10260, lr = 0.01
I0318 07:39:36.067358  3501 solver.cpp:228] Iteration 10280, loss = 32.2126
I0318 07:39:36.067423  3501 solver.cpp:244]     Train net output #0: loss = 32.2125 (* 1 = 32.2125 loss)
I0318 07:39:36.067437  3501 sgd_solver.cpp:106] Iteration 10280, lr = 0.01
I0318 07:39:43.381122  3501 solver.cpp:228] Iteration 10300, loss = 18.202
I0318 07:39:43.381186  3501 solver.cpp:244]     Train net output #0: loss = 18.2019 (* 1 = 18.2019 loss)
I0318 07:39:43.381201  3501 sgd_solver.cpp:106] Iteration 10300, lr = 0.01
I0318 07:39:50.711817  3501 solver.cpp:228] Iteration 10320, loss = 58.6975
I0318 07:39:50.711879  3501 solver.cpp:244]     Train net output #0: loss = 58.6974 (* 1 = 58.6974 loss)
I0318 07:39:50.711894  3501 sgd_solver.cpp:106] Iteration 10320, lr = 0.01
I0318 07:39:58.046730  3501 solver.cpp:228] Iteration 10340, loss = 50.8032
I0318 07:39:58.046880  3501 solver.cpp:244]     Train net output #0: loss = 50.8031 (* 1 = 50.8031 loss)
I0318 07:39:58.046896  3501 sgd_solver.cpp:106] Iteration 10340, lr = 0.01
I0318 07:40:05.385406  3501 solver.cpp:228] Iteration 10360, loss = 43.4094
I0318 07:40:05.385471  3501 solver.cpp:244]     Train net output #0: loss = 43.4093 (* 1 = 43.4093 loss)
I0318 07:40:05.385486  3501 sgd_solver.cpp:106] Iteration 10360, lr = 0.01
I0318 07:40:12.718958  3501 solver.cpp:228] Iteration 10380, loss = 27.3785
I0318 07:40:12.719024  3501 solver.cpp:244]     Train net output #0: loss = 27.3784 (* 1 = 27.3784 loss)
I0318 07:40:12.719038  3501 sgd_solver.cpp:106] Iteration 10380, lr = 0.01
I0318 07:40:20.051401  3501 solver.cpp:228] Iteration 10400, loss = 105.503
I0318 07:40:20.051466  3501 solver.cpp:244]     Train net output #0: loss = 105.503 (* 1 = 105.503 loss)
I0318 07:40:20.051481  3501 sgd_solver.cpp:106] Iteration 10400, lr = 0.01
I0318 07:40:27.380128  3501 solver.cpp:228] Iteration 10420, loss = 79.9213
I0318 07:40:27.380194  3501 solver.cpp:244]     Train net output #0: loss = 79.9212 (* 1 = 79.9212 loss)
I0318 07:40:27.380208  3501 sgd_solver.cpp:106] Iteration 10420, lr = 0.01
I0318 07:40:34.719806  3501 solver.cpp:228] Iteration 10440, loss = 60.0116
I0318 07:40:34.719943  3501 solver.cpp:244]     Train net output #0: loss = 60.0115 (* 1 = 60.0115 loss)
I0318 07:40:34.719959  3501 sgd_solver.cpp:106] Iteration 10440, lr = 0.01
I0318 07:40:42.057385  3501 solver.cpp:228] Iteration 10460, loss = 47.4848
I0318 07:40:42.057451  3501 solver.cpp:244]     Train net output #0: loss = 47.4847 (* 1 = 47.4847 loss)
I0318 07:40:42.057466  3501 sgd_solver.cpp:106] Iteration 10460, lr = 0.01
I0318 07:40:49.391607  3501 solver.cpp:228] Iteration 10480, loss = 21.9106
I0318 07:40:49.391671  3501 solver.cpp:244]     Train net output #0: loss = 21.9105 (* 1 = 21.9105 loss)
I0318 07:40:49.391686  3501 sgd_solver.cpp:106] Iteration 10480, lr = 0.01
I0318 07:40:56.354790  3501 solver.cpp:337] Iteration 10500, Testing net (#0)
I0318 07:42:50.578460  3501 solver.cpp:404]     Test net output #0: loss = 620.112 (* 1 = 620.112 loss)
I0318 07:42:50.578565  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:42:50.915866  3501 solver.cpp:228] Iteration 10500, loss = 26.8311
I0318 07:42:50.915931  3501 solver.cpp:244]     Train net output #0: loss = 26.831 (* 1 = 26.831 loss)
I0318 07:42:50.915944  3501 sgd_solver.cpp:106] Iteration 10500, lr = 0.01
I0318 07:42:58.186758  3501 solver.cpp:228] Iteration 10520, loss = 30.6624
I0318 07:42:58.186822  3501 solver.cpp:244]     Train net output #0: loss = 30.6623 (* 1 = 30.6623 loss)
I0318 07:42:58.186836  3501 sgd_solver.cpp:106] Iteration 10520, lr = 0.01
I0318 07:43:05.491345  3501 solver.cpp:228] Iteration 10540, loss = 53.2412
I0318 07:43:05.491410  3501 solver.cpp:244]     Train net output #0: loss = 53.2411 (* 1 = 53.2411 loss)
I0318 07:43:05.491425  3501 sgd_solver.cpp:106] Iteration 10540, lr = 0.01
I0318 07:43:12.800945  3501 solver.cpp:228] Iteration 10560, loss = 46.5001
I0318 07:43:12.801010  3501 solver.cpp:244]     Train net output #0: loss = 46.4999 (* 1 = 46.4999 loss)
I0318 07:43:12.801024  3501 sgd_solver.cpp:106] Iteration 10560, lr = 0.01
I0318 07:43:20.128933  3501 solver.cpp:228] Iteration 10580, loss = 37.2837
I0318 07:43:20.128999  3501 solver.cpp:244]     Train net output #0: loss = 37.2836 (* 1 = 37.2836 loss)
I0318 07:43:20.129014  3501 sgd_solver.cpp:106] Iteration 10580, lr = 0.01
I0318 07:43:27.452692  3501 solver.cpp:228] Iteration 10600, loss = 70.1222
I0318 07:43:27.452872  3501 solver.cpp:244]     Train net output #0: loss = 70.1221 (* 1 = 70.1221 loss)
I0318 07:43:27.452890  3501 sgd_solver.cpp:106] Iteration 10600, lr = 0.01
I0318 07:43:34.785343  3501 solver.cpp:228] Iteration 10620, loss = 32.4833
I0318 07:43:34.785413  3501 solver.cpp:244]     Train net output #0: loss = 32.4832 (* 1 = 32.4832 loss)
I0318 07:43:34.785426  3501 sgd_solver.cpp:106] Iteration 10620, lr = 0.01
I0318 07:43:42.125048  3501 solver.cpp:228] Iteration 10640, loss = 51.0808
I0318 07:43:42.125120  3501 solver.cpp:244]     Train net output #0: loss = 51.0807 (* 1 = 51.0807 loss)
I0318 07:43:42.125135  3501 sgd_solver.cpp:106] Iteration 10640, lr = 0.01
I0318 07:43:49.459383  3501 solver.cpp:228] Iteration 10660, loss = 33.5058
I0318 07:43:49.459451  3501 solver.cpp:244]     Train net output #0: loss = 33.5057 (* 1 = 33.5057 loss)
I0318 07:43:49.459467  3501 sgd_solver.cpp:106] Iteration 10660, lr = 0.01
I0318 07:43:56.811023  3501 solver.cpp:228] Iteration 10680, loss = 47.8019
I0318 07:43:56.811095  3501 solver.cpp:244]     Train net output #0: loss = 47.8018 (* 1 = 47.8018 loss)
I0318 07:43:56.811110  3501 sgd_solver.cpp:106] Iteration 10680, lr = 0.01
I0318 07:44:04.145313  3501 solver.cpp:228] Iteration 10700, loss = 33.096
I0318 07:44:04.145468  3501 solver.cpp:244]     Train net output #0: loss = 33.0958 (* 1 = 33.0958 loss)
I0318 07:44:04.145483  3501 sgd_solver.cpp:106] Iteration 10700, lr = 0.01
I0318 07:44:11.473711  3501 solver.cpp:228] Iteration 10720, loss = 59.0609
I0318 07:44:11.473775  3501 solver.cpp:244]     Train net output #0: loss = 59.0608 (* 1 = 59.0608 loss)
I0318 07:44:11.473789  3501 sgd_solver.cpp:106] Iteration 10720, lr = 0.01
I0318 07:44:18.803534  3501 solver.cpp:228] Iteration 10740, loss = 32.1519
I0318 07:44:18.803606  3501 solver.cpp:244]     Train net output #0: loss = 32.1518 (* 1 = 32.1518 loss)
I0318 07:44:18.803620  3501 sgd_solver.cpp:106] Iteration 10740, lr = 0.01
I0318 07:44:22.102954  3501 solver.cpp:337] Iteration 10750, Testing net (#0)
I0318 07:46:16.306190  3501 solver.cpp:404]     Test net output #0: loss = 636.114 (* 1 = 636.114 loss)
I0318 07:46:16.306311  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 07:46:20.256309  3501 solver.cpp:228] Iteration 10760, loss = 57.501
I0318 07:46:20.256371  3501 solver.cpp:244]     Train net output #0: loss = 57.5009 (* 1 = 57.5009 loss)
I0318 07:46:20.256386  3501 sgd_solver.cpp:106] Iteration 10760, lr = 0.01
I0318 07:46:27.529446  3501 solver.cpp:228] Iteration 10780, loss = 62.4509
I0318 07:46:27.529511  3501 solver.cpp:244]     Train net output #0: loss = 62.4508 (* 1 = 62.4508 loss)
I0318 07:46:27.529525  3501 sgd_solver.cpp:106] Iteration 10780, lr = 0.01
I0318 07:46:34.837798  3501 solver.cpp:228] Iteration 10800, loss = 50.3763
I0318 07:46:34.837864  3501 solver.cpp:244]     Train net output #0: loss = 50.3762 (* 1 = 50.3762 loss)
I0318 07:46:34.837878  3501 sgd_solver.cpp:106] Iteration 10800, lr = 0.01
I0318 07:46:42.153563  3501 solver.cpp:228] Iteration 10820, loss = 30.2441
I0318 07:46:42.153626  3501 solver.cpp:244]     Train net output #0: loss = 30.244 (* 1 = 30.244 loss)
I0318 07:46:42.153640  3501 sgd_solver.cpp:106] Iteration 10820, lr = 0.01
I0318 07:46:49.479748  3501 solver.cpp:228] Iteration 10840, loss = 22.5675
I0318 07:46:49.479907  3501 solver.cpp:244]     Train net output #0: loss = 22.5674 (* 1 = 22.5674 loss)
I0318 07:46:49.479924  3501 sgd_solver.cpp:106] Iteration 10840, lr = 0.01
I0318 07:46:56.818032  3501 solver.cpp:228] Iteration 10860, loss = 30.357
I0318 07:46:56.818097  3501 solver.cpp:244]     Train net output #0: loss = 30.3569 (* 1 = 30.3569 loss)
I0318 07:46:56.818112  3501 sgd_solver.cpp:106] Iteration 10860, lr = 0.01
I0318 07:47:04.158026  3501 solver.cpp:228] Iteration 10880, loss = 35.0382
I0318 07:47:04.158107  3501 solver.cpp:244]     Train net output #0: loss = 35.0381 (* 1 = 35.0381 loss)
I0318 07:47:04.158121  3501 sgd_solver.cpp:106] Iteration 10880, lr = 0.01
I0318 07:47:11.500628  3501 solver.cpp:228] Iteration 10900, loss = 65.3678
I0318 07:47:11.500704  3501 solver.cpp:244]     Train net output #0: loss = 65.3677 (* 1 = 65.3677 loss)
I0318 07:47:11.500718  3501 sgd_solver.cpp:106] Iteration 10900, lr = 0.01
I0318 07:47:18.839022  3501 solver.cpp:228] Iteration 10920, loss = 34.2311
I0318 07:47:18.839089  3501 solver.cpp:244]     Train net output #0: loss = 34.231 (* 1 = 34.231 loss)
I0318 07:47:18.839103  3501 sgd_solver.cpp:106] Iteration 10920, lr = 0.01
I0318 07:47:26.175269  3501 solver.cpp:228] Iteration 10940, loss = 76.4134
I0318 07:47:26.175398  3501 solver.cpp:244]     Train net output #0: loss = 76.4133 (* 1 = 76.4133 loss)
I0318 07:47:26.175415  3501 sgd_solver.cpp:106] Iteration 10940, lr = 0.01
I0318 07:47:33.513504  3501 solver.cpp:228] Iteration 10960, loss = 57.7561
I0318 07:47:33.513567  3501 solver.cpp:244]     Train net output #0: loss = 57.756 (* 1 = 57.756 loss)
I0318 07:47:33.513581  3501 sgd_solver.cpp:106] Iteration 10960, lr = 0.01
I0318 07:47:40.851758  3501 solver.cpp:228] Iteration 10980, loss = 53.7066
I0318 07:47:40.851824  3501 solver.cpp:244]     Train net output #0: loss = 53.7065 (* 1 = 53.7065 loss)
I0318 07:47:40.851837  3501 sgd_solver.cpp:106] Iteration 10980, lr = 0.01
I0318 07:47:47.823357  3501 solver.cpp:337] Iteration 11000, Testing net (#0)
I0318 07:49:42.033144  3501 solver.cpp:404]     Test net output #0: loss = 592.335 (* 1 = 592.335 loss)
I0318 07:49:42.033285  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 07:49:42.369771  3501 solver.cpp:228] Iteration 11000, loss = 35.1201
I0318 07:49:42.369835  3501 solver.cpp:244]     Train net output #0: loss = 35.12 (* 1 = 35.12 loss)
I0318 07:49:42.369850  3501 sgd_solver.cpp:106] Iteration 11000, lr = 0.01
I0318 07:49:49.606333  3501 solver.cpp:228] Iteration 11020, loss = 24.9873
I0318 07:49:49.606400  3501 solver.cpp:244]     Train net output #0: loss = 24.9872 (* 1 = 24.9872 loss)
I0318 07:49:49.606413  3501 sgd_solver.cpp:106] Iteration 11020, lr = 0.01
I0318 07:49:56.888741  3501 solver.cpp:228] Iteration 11040, loss = 33.5635
I0318 07:49:56.888808  3501 solver.cpp:244]     Train net output #0: loss = 33.5634 (* 1 = 33.5634 loss)
I0318 07:49:56.888820  3501 sgd_solver.cpp:106] Iteration 11040, lr = 0.01
I0318 07:50:04.197811  3501 solver.cpp:228] Iteration 11060, loss = 29.9855
I0318 07:50:04.197877  3501 solver.cpp:244]     Train net output #0: loss = 29.9854 (* 1 = 29.9854 loss)
I0318 07:50:04.197890  3501 sgd_solver.cpp:106] Iteration 11060, lr = 0.01
I0318 07:50:11.525447  3501 solver.cpp:228] Iteration 11080, loss = 46.4354
I0318 07:50:11.525511  3501 solver.cpp:244]     Train net output #0: loss = 46.4353 (* 1 = 46.4353 loss)
I0318 07:50:11.525524  3501 sgd_solver.cpp:106] Iteration 11080, lr = 0.01
I0318 07:50:18.863142  3501 solver.cpp:228] Iteration 11100, loss = 70.0158
I0318 07:50:18.863286  3501 solver.cpp:244]     Train net output #0: loss = 70.0157 (* 1 = 70.0157 loss)
I0318 07:50:18.863301  3501 sgd_solver.cpp:106] Iteration 11100, lr = 0.01
I0318 07:50:26.202637  3501 solver.cpp:228] Iteration 11120, loss = 58.4431
I0318 07:50:26.202706  3501 solver.cpp:244]     Train net output #0: loss = 58.443 (* 1 = 58.443 loss)
I0318 07:50:26.202723  3501 sgd_solver.cpp:106] Iteration 11120, lr = 0.01
I0318 07:50:33.538889  3501 solver.cpp:228] Iteration 11140, loss = 62.5388
I0318 07:50:33.538955  3501 solver.cpp:244]     Train net output #0: loss = 62.5387 (* 1 = 62.5387 loss)
I0318 07:50:33.538969  3501 sgd_solver.cpp:106] Iteration 11140, lr = 0.01
I0318 07:50:40.867760  3501 solver.cpp:228] Iteration 11160, loss = 38.9017
I0318 07:50:40.867825  3501 solver.cpp:244]     Train net output #0: loss = 38.9016 (* 1 = 38.9016 loss)
I0318 07:50:40.867838  3501 sgd_solver.cpp:106] Iteration 11160, lr = 0.01
I0318 07:50:48.195674  3501 solver.cpp:228] Iteration 11180, loss = 27.5242
I0318 07:50:48.195740  3501 solver.cpp:244]     Train net output #0: loss = 27.5241 (* 1 = 27.5241 loss)
I0318 07:50:48.195755  3501 sgd_solver.cpp:106] Iteration 11180, lr = 0.01
I0318 07:50:55.537389  3501 solver.cpp:228] Iteration 11200, loss = 27.8154
I0318 07:50:55.537564  3501 solver.cpp:244]     Train net output #0: loss = 27.8153 (* 1 = 27.8153 loss)
I0318 07:50:55.537581  3501 sgd_solver.cpp:106] Iteration 11200, lr = 0.01
I0318 07:51:02.868281  3501 solver.cpp:228] Iteration 11220, loss = 53.2801
I0318 07:51:02.868356  3501 solver.cpp:244]     Train net output #0: loss = 53.28 (* 1 = 53.28 loss)
I0318 07:51:02.868371  3501 sgd_solver.cpp:106] Iteration 11220, lr = 0.01
I0318 07:51:10.199064  3501 solver.cpp:228] Iteration 11240, loss = 41.815
I0318 07:51:10.199134  3501 solver.cpp:244]     Train net output #0: loss = 41.815 (* 1 = 41.815 loss)
I0318 07:51:10.199149  3501 sgd_solver.cpp:106] Iteration 11240, lr = 0.01
I0318 07:51:13.497766  3501 solver.cpp:337] Iteration 11250, Testing net (#0)
I0318 07:53:07.698936  3501 solver.cpp:404]     Test net output #0: loss = 635.874 (* 1 = 635.874 loss)
I0318 07:53:07.699067  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 07:53:11.650625  3501 solver.cpp:228] Iteration 11260, loss = 39.2052
I0318 07:53:11.650686  3501 solver.cpp:244]     Train net output #0: loss = 39.2051 (* 1 = 39.2051 loss)
I0318 07:53:11.650699  3501 sgd_solver.cpp:106] Iteration 11260, lr = 0.01
I0318 07:53:18.931320  3501 solver.cpp:228] Iteration 11280, loss = 35.8078
I0318 07:53:18.931386  3501 solver.cpp:244]     Train net output #0: loss = 35.8078 (* 1 = 35.8078 loss)
I0318 07:53:18.931401  3501 sgd_solver.cpp:106] Iteration 11280, lr = 0.01
I0318 07:53:26.242007  3501 solver.cpp:228] Iteration 11300, loss = 44.3403
I0318 07:53:26.242072  3501 solver.cpp:244]     Train net output #0: loss = 44.3402 (* 1 = 44.3402 loss)
I0318 07:53:26.242086  3501 sgd_solver.cpp:106] Iteration 11300, lr = 0.01
I0318 07:53:33.569146  3501 solver.cpp:228] Iteration 11320, loss = 46.7669
I0318 07:53:33.569211  3501 solver.cpp:244]     Train net output #0: loss = 46.7668 (* 1 = 46.7668 loss)
I0318 07:53:33.569226  3501 sgd_solver.cpp:106] Iteration 11320, lr = 0.01
I0318 07:53:40.902815  3501 solver.cpp:228] Iteration 11340, loss = 51.3321
I0318 07:53:40.902956  3501 solver.cpp:244]     Train net output #0: loss = 51.332 (* 1 = 51.332 loss)
I0318 07:53:40.902972  3501 sgd_solver.cpp:106] Iteration 11340, lr = 0.01
I0318 07:53:48.236393  3501 solver.cpp:228] Iteration 11360, loss = 32.6628
I0318 07:53:48.236459  3501 solver.cpp:244]     Train net output #0: loss = 32.6627 (* 1 = 32.6627 loss)
I0318 07:53:48.236472  3501 sgd_solver.cpp:106] Iteration 11360, lr = 0.01
I0318 07:53:55.565897  3501 solver.cpp:228] Iteration 11380, loss = 23.9375
I0318 07:53:55.565963  3501 solver.cpp:244]     Train net output #0: loss = 23.9374 (* 1 = 23.9374 loss)
I0318 07:53:55.565976  3501 sgd_solver.cpp:106] Iteration 11380, lr = 0.01
I0318 07:54:02.898914  3501 solver.cpp:228] Iteration 11400, loss = 30.4994
I0318 07:54:02.898980  3501 solver.cpp:244]     Train net output #0: loss = 30.4993 (* 1 = 30.4993 loss)
I0318 07:54:02.898994  3501 sgd_solver.cpp:106] Iteration 11400, lr = 0.01
I0318 07:54:10.230705  3501 solver.cpp:228] Iteration 11420, loss = 45.9919
I0318 07:54:10.230767  3501 solver.cpp:244]     Train net output #0: loss = 45.9918 (* 1 = 45.9918 loss)
I0318 07:54:10.230782  3501 sgd_solver.cpp:106] Iteration 11420, lr = 0.01
I0318 07:54:17.574892  3501 solver.cpp:228] Iteration 11440, loss = 29.4614
I0318 07:54:17.575076  3501 solver.cpp:244]     Train net output #0: loss = 29.4613 (* 1 = 29.4613 loss)
I0318 07:54:17.575093  3501 sgd_solver.cpp:106] Iteration 11440, lr = 0.01
I0318 07:54:24.919914  3501 solver.cpp:228] Iteration 11460, loss = 45.5555
I0318 07:54:24.919986  3501 solver.cpp:244]     Train net output #0: loss = 45.5554 (* 1 = 45.5554 loss)
I0318 07:54:24.920001  3501 sgd_solver.cpp:106] Iteration 11460, lr = 0.01
I0318 07:54:32.265784  3501 solver.cpp:228] Iteration 11480, loss = 90.1528
I0318 07:54:32.265849  3501 solver.cpp:244]     Train net output #0: loss = 90.1527 (* 1 = 90.1527 loss)
I0318 07:54:32.265862  3501 sgd_solver.cpp:106] Iteration 11480, lr = 0.01
I0318 07:54:39.240818  3501 solver.cpp:337] Iteration 11500, Testing net (#0)
I0318 07:56:33.455070  3501 solver.cpp:404]     Test net output #0: loss = 652.443 (* 1 = 652.443 loss)
I0318 07:56:33.455199  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 07:56:33.793191  3501 solver.cpp:228] Iteration 11500, loss = 76.5838
I0318 07:56:33.793256  3501 solver.cpp:244]     Train net output #0: loss = 76.5837 (* 1 = 76.5837 loss)
I0318 07:56:33.793270  3501 sgd_solver.cpp:106] Iteration 11500, lr = 0.01
I0318 07:56:41.060045  3501 solver.cpp:228] Iteration 11520, loss = 41.5118
I0318 07:56:41.060115  3501 solver.cpp:244]     Train net output #0: loss = 41.5117 (* 1 = 41.5117 loss)
I0318 07:56:41.060130  3501 sgd_solver.cpp:106] Iteration 11520, lr = 0.01
I0318 07:56:48.365010  3501 solver.cpp:228] Iteration 11540, loss = 33.6905
I0318 07:56:48.365073  3501 solver.cpp:244]     Train net output #0: loss = 33.6904 (* 1 = 33.6904 loss)
I0318 07:56:48.365087  3501 sgd_solver.cpp:106] Iteration 11540, lr = 0.01
I0318 07:56:55.686069  3501 solver.cpp:228] Iteration 11560, loss = 16.5447
I0318 07:56:55.686132  3501 solver.cpp:244]     Train net output #0: loss = 16.5446 (* 1 = 16.5446 loss)
I0318 07:56:55.686147  3501 sgd_solver.cpp:106] Iteration 11560, lr = 0.01
I0318 07:57:03.031092  3501 solver.cpp:228] Iteration 11580, loss = 47.3111
I0318 07:57:03.031152  3501 solver.cpp:244]     Train net output #0: loss = 47.311 (* 1 = 47.311 loss)
I0318 07:57:03.031167  3501 sgd_solver.cpp:106] Iteration 11580, lr = 0.01
I0318 07:57:10.363215  3501 solver.cpp:228] Iteration 11600, loss = 49.1159
I0318 07:57:10.363343  3501 solver.cpp:244]     Train net output #0: loss = 49.1158 (* 1 = 49.1158 loss)
I0318 07:57:10.363358  3501 sgd_solver.cpp:106] Iteration 11600, lr = 0.01
I0318 07:57:17.691402  3501 solver.cpp:228] Iteration 11620, loss = 46.6858
I0318 07:57:17.691469  3501 solver.cpp:244]     Train net output #0: loss = 46.6858 (* 1 = 46.6858 loss)
I0318 07:57:17.691484  3501 sgd_solver.cpp:106] Iteration 11620, lr = 0.01
I0318 07:57:25.018322  3501 solver.cpp:228] Iteration 11640, loss = 35.5323
I0318 07:57:25.018386  3501 solver.cpp:244]     Train net output #0: loss = 35.5322 (* 1 = 35.5322 loss)
I0318 07:57:25.018400  3501 sgd_solver.cpp:106] Iteration 11640, lr = 0.01
I0318 07:57:32.349427  3501 solver.cpp:228] Iteration 11660, loss = 48.1824
I0318 07:57:32.349504  3501 solver.cpp:244]     Train net output #0: loss = 48.1823 (* 1 = 48.1823 loss)
I0318 07:57:32.349521  3501 sgd_solver.cpp:106] Iteration 11660, lr = 0.01
I0318 07:57:39.694391  3501 solver.cpp:228] Iteration 11680, loss = 68.6017
I0318 07:57:39.694469  3501 solver.cpp:244]     Train net output #0: loss = 68.6017 (* 1 = 68.6017 loss)
I0318 07:57:39.694485  3501 sgd_solver.cpp:106] Iteration 11680, lr = 0.01
I0318 07:57:47.040475  3501 solver.cpp:228] Iteration 11700, loss = 58.9604
I0318 07:57:47.040603  3501 solver.cpp:244]     Train net output #0: loss = 58.9603 (* 1 = 58.9603 loss)
I0318 07:57:47.040618  3501 sgd_solver.cpp:106] Iteration 11700, lr = 0.01
I0318 07:57:54.376392  3501 solver.cpp:228] Iteration 11720, loss = 41.4011
I0318 07:57:54.376462  3501 solver.cpp:244]     Train net output #0: loss = 41.401 (* 1 = 41.401 loss)
I0318 07:57:54.376477  3501 sgd_solver.cpp:106] Iteration 11720, lr = 0.01
I0318 07:58:01.714757  3501 solver.cpp:228] Iteration 11740, loss = 29.6854
I0318 07:58:01.714828  3501 solver.cpp:244]     Train net output #0: loss = 29.6853 (* 1 = 29.6853 loss)
I0318 07:58:01.714841  3501 sgd_solver.cpp:106] Iteration 11740, lr = 0.01
I0318 07:58:05.015219  3501 solver.cpp:337] Iteration 11750, Testing net (#0)
I0318 07:59:59.197602  3501 solver.cpp:404]     Test net output #0: loss = 680.742 (* 1 = 680.742 loss)
I0318 07:59:59.197743  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:00:03.156764  3501 solver.cpp:228] Iteration 11760, loss = 38.9311
I0318 08:00:03.156824  3501 solver.cpp:244]     Train net output #0: loss = 38.931 (* 1 = 38.931 loss)
I0318 08:00:03.156838  3501 sgd_solver.cpp:106] Iteration 11760, lr = 0.01
I0318 08:00:10.436051  3501 solver.cpp:228] Iteration 11780, loss = 43.1399
I0318 08:00:10.436120  3501 solver.cpp:244]     Train net output #0: loss = 43.1399 (* 1 = 43.1399 loss)
I0318 08:00:10.436133  3501 sgd_solver.cpp:106] Iteration 11780, lr = 0.01
I0318 08:00:17.747670  3501 solver.cpp:228] Iteration 11800, loss = 30.4883
I0318 08:00:17.747735  3501 solver.cpp:244]     Train net output #0: loss = 30.4883 (* 1 = 30.4883 loss)
I0318 08:00:17.747750  3501 sgd_solver.cpp:106] Iteration 11800, lr = 0.01
I0318 08:00:25.063156  3501 solver.cpp:228] Iteration 11820, loss = 54.9179
I0318 08:00:25.063222  3501 solver.cpp:244]     Train net output #0: loss = 54.9178 (* 1 = 54.9178 loss)
I0318 08:00:25.063235  3501 sgd_solver.cpp:106] Iteration 11820, lr = 0.01
I0318 08:00:32.386834  3501 solver.cpp:228] Iteration 11840, loss = 69.787
I0318 08:00:32.386991  3501 solver.cpp:244]     Train net output #0: loss = 69.7869 (* 1 = 69.7869 loss)
I0318 08:00:32.387007  3501 sgd_solver.cpp:106] Iteration 11840, lr = 0.01
I0318 08:00:39.728065  3501 solver.cpp:228] Iteration 11860, loss = 42.7934
I0318 08:00:39.728130  3501 solver.cpp:244]     Train net output #0: loss = 42.7933 (* 1 = 42.7933 loss)
I0318 08:00:39.728143  3501 sgd_solver.cpp:106] Iteration 11860, lr = 0.01
I0318 08:00:47.071897  3501 solver.cpp:228] Iteration 11880, loss = 54.5944
I0318 08:00:47.071967  3501 solver.cpp:244]     Train net output #0: loss = 54.5943 (* 1 = 54.5943 loss)
I0318 08:00:47.071983  3501 sgd_solver.cpp:106] Iteration 11880, lr = 0.01
I0318 08:00:54.414441  3501 solver.cpp:228] Iteration 11900, loss = 29.7732
I0318 08:00:54.414507  3501 solver.cpp:244]     Train net output #0: loss = 29.7731 (* 1 = 29.7731 loss)
I0318 08:00:54.414521  3501 sgd_solver.cpp:106] Iteration 11900, lr = 0.01
I0318 08:01:01.750381  3501 solver.cpp:228] Iteration 11920, loss = 25.9127
I0318 08:01:01.750447  3501 solver.cpp:244]     Train net output #0: loss = 25.9126 (* 1 = 25.9126 loss)
I0318 08:01:01.750460  3501 sgd_solver.cpp:106] Iteration 11920, lr = 0.01
I0318 08:01:09.086503  3501 solver.cpp:228] Iteration 11940, loss = 19.9367
I0318 08:01:09.086650  3501 solver.cpp:244]     Train net output #0: loss = 19.9366 (* 1 = 19.9366 loss)
I0318 08:01:09.086666  3501 sgd_solver.cpp:106] Iteration 11940, lr = 0.01
I0318 08:01:16.417930  3501 solver.cpp:228] Iteration 11960, loss = 42.4251
I0318 08:01:16.417996  3501 solver.cpp:244]     Train net output #0: loss = 42.425 (* 1 = 42.425 loss)
I0318 08:01:16.418010  3501 sgd_solver.cpp:106] Iteration 11960, lr = 0.01
I0318 08:01:23.748080  3501 solver.cpp:228] Iteration 11980, loss = 42.6881
I0318 08:01:23.748144  3501 solver.cpp:244]     Train net output #0: loss = 42.688 (* 1 = 42.688 loss)
I0318 08:01:23.748157  3501 sgd_solver.cpp:106] Iteration 11980, lr = 0.01
I0318 08:01:30.709564  3501 solver.cpp:337] Iteration 12000, Testing net (#0)
I0318 08:03:24.907521  3501 solver.cpp:404]     Test net output #0: loss = 639.564 (* 1 = 639.564 loss)
I0318 08:03:24.907686  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 08:03:25.245586  3501 solver.cpp:228] Iteration 12000, loss = 35.8598
I0318 08:03:25.245651  3501 solver.cpp:244]     Train net output #0: loss = 35.8597 (* 1 = 35.8597 loss)
I0318 08:03:25.245664  3501 sgd_solver.cpp:106] Iteration 12000, lr = 0.01
I0318 08:03:32.513902  3501 solver.cpp:228] Iteration 12020, loss = 45.2467
I0318 08:03:32.513968  3501 solver.cpp:244]     Train net output #0: loss = 45.2466 (* 1 = 45.2466 loss)
I0318 08:03:32.513983  3501 sgd_solver.cpp:106] Iteration 12020, lr = 0.01
I0318 08:03:39.826555  3501 solver.cpp:228] Iteration 12040, loss = 61.1592
I0318 08:03:39.826619  3501 solver.cpp:244]     Train net output #0: loss = 61.1591 (* 1 = 61.1591 loss)
I0318 08:03:39.826633  3501 sgd_solver.cpp:106] Iteration 12040, lr = 0.01
I0318 08:03:47.147598  3501 solver.cpp:228] Iteration 12060, loss = 63.7721
I0318 08:03:47.147663  3501 solver.cpp:244]     Train net output #0: loss = 63.772 (* 1 = 63.772 loss)
I0318 08:03:47.147677  3501 sgd_solver.cpp:106] Iteration 12060, lr = 0.01
I0318 08:03:54.485755  3501 solver.cpp:228] Iteration 12080, loss = 35.3674
I0318 08:03:54.485821  3501 solver.cpp:244]     Train net output #0: loss = 35.3673 (* 1 = 35.3673 loss)
I0318 08:03:54.485833  3501 sgd_solver.cpp:106] Iteration 12080, lr = 0.01
I0318 08:04:01.830195  3501 solver.cpp:228] Iteration 12100, loss = 24.4728
I0318 08:04:01.830381  3501 solver.cpp:244]     Train net output #0: loss = 24.4727 (* 1 = 24.4727 loss)
I0318 08:04:01.830396  3501 sgd_solver.cpp:106] Iteration 12100, lr = 0.01
I0318 08:04:09.170892  3501 solver.cpp:228] Iteration 12120, loss = 40.5474
I0318 08:04:09.170960  3501 solver.cpp:244]     Train net output #0: loss = 40.5473 (* 1 = 40.5473 loss)
I0318 08:04:09.170974  3501 sgd_solver.cpp:106] Iteration 12120, lr = 0.01
I0318 08:04:16.509289  3501 solver.cpp:228] Iteration 12140, loss = 33.4242
I0318 08:04:16.509356  3501 solver.cpp:244]     Train net output #0: loss = 33.4241 (* 1 = 33.4241 loss)
I0318 08:04:16.509369  3501 sgd_solver.cpp:106] Iteration 12140, lr = 0.01
I0318 08:04:23.838482  3501 solver.cpp:228] Iteration 12160, loss = 49.1912
I0318 08:04:23.838548  3501 solver.cpp:244]     Train net output #0: loss = 49.1911 (* 1 = 49.1911 loss)
I0318 08:04:23.838562  3501 sgd_solver.cpp:106] Iteration 12160, lr = 0.01
I0318 08:04:31.171748  3501 solver.cpp:228] Iteration 12180, loss = 31.7745
I0318 08:04:31.171814  3501 solver.cpp:244]     Train net output #0: loss = 31.7744 (* 1 = 31.7744 loss)
I0318 08:04:31.171828  3501 sgd_solver.cpp:106] Iteration 12180, lr = 0.01
I0318 08:04:38.499892  3501 solver.cpp:228] Iteration 12200, loss = 60.652
I0318 08:04:38.500036  3501 solver.cpp:244]     Train net output #0: loss = 60.652 (* 1 = 60.652 loss)
I0318 08:04:38.500053  3501 sgd_solver.cpp:106] Iteration 12200, lr = 0.01
I0318 08:04:45.834197  3501 solver.cpp:228] Iteration 12220, loss = 59.9488
I0318 08:04:45.834270  3501 solver.cpp:244]     Train net output #0: loss = 59.9487 (* 1 = 59.9487 loss)
I0318 08:04:45.834283  3501 sgd_solver.cpp:106] Iteration 12220, lr = 0.01
I0318 08:04:53.172576  3501 solver.cpp:228] Iteration 12240, loss = 77.2036
I0318 08:04:53.172636  3501 solver.cpp:244]     Train net output #0: loss = 77.2035 (* 1 = 77.2035 loss)
I0318 08:04:53.172649  3501 sgd_solver.cpp:106] Iteration 12240, lr = 0.01
I0318 08:04:56.472667  3501 solver.cpp:337] Iteration 12250, Testing net (#0)
I0318 08:06:50.692062  3501 solver.cpp:404]     Test net output #0: loss = 650.805 (* 1 = 650.805 loss)
I0318 08:06:50.692176  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 08:06:54.644639  3501 solver.cpp:228] Iteration 12260, loss = 49.872
I0318 08:06:54.644706  3501 solver.cpp:244]     Train net output #0: loss = 49.8719 (* 1 = 49.8719 loss)
I0318 08:06:54.644719  3501 sgd_solver.cpp:106] Iteration 12260, lr = 0.01
I0318 08:07:01.932916  3501 solver.cpp:228] Iteration 12280, loss = 19.8966
I0318 08:07:01.932989  3501 solver.cpp:244]     Train net output #0: loss = 19.8965 (* 1 = 19.8965 loss)
I0318 08:07:01.933004  3501 sgd_solver.cpp:106] Iteration 12280, lr = 0.01
I0318 08:07:09.246376  3501 solver.cpp:228] Iteration 12300, loss = 32.1713
I0318 08:07:09.246449  3501 solver.cpp:244]     Train net output #0: loss = 32.1712 (* 1 = 32.1712 loss)
I0318 08:07:09.246464  3501 sgd_solver.cpp:106] Iteration 12300, lr = 0.01
I0318 08:07:16.565335  3501 solver.cpp:228] Iteration 12320, loss = 50.5948
I0318 08:07:16.565407  3501 solver.cpp:244]     Train net output #0: loss = 50.5947 (* 1 = 50.5947 loss)
I0318 08:07:16.565423  3501 sgd_solver.cpp:106] Iteration 12320, lr = 0.01
I0318 08:07:23.887153  3501 solver.cpp:228] Iteration 12340, loss = 40.7454
I0318 08:07:23.887321  3501 solver.cpp:244]     Train net output #0: loss = 40.7454 (* 1 = 40.7454 loss)
I0318 08:07:23.887339  3501 sgd_solver.cpp:106] Iteration 12340, lr = 0.01
I0318 08:07:31.224056  3501 solver.cpp:228] Iteration 12360, loss = 67.1962
I0318 08:07:31.224128  3501 solver.cpp:244]     Train net output #0: loss = 67.1962 (* 1 = 67.1962 loss)
I0318 08:07:31.224143  3501 sgd_solver.cpp:106] Iteration 12360, lr = 0.01
I0318 08:07:38.558143  3501 solver.cpp:228] Iteration 12380, loss = 54.6984
I0318 08:07:38.558210  3501 solver.cpp:244]     Train net output #0: loss = 54.6983 (* 1 = 54.6983 loss)
I0318 08:07:38.558223  3501 sgd_solver.cpp:106] Iteration 12380, lr = 0.01
I0318 08:07:45.893976  3501 solver.cpp:228] Iteration 12400, loss = 59.9404
I0318 08:07:45.894042  3501 solver.cpp:244]     Train net output #0: loss = 59.9404 (* 1 = 59.9404 loss)
I0318 08:07:45.894055  3501 sgd_solver.cpp:106] Iteration 12400, lr = 0.01
I0318 08:07:53.228826  3501 solver.cpp:228] Iteration 12420, loss = 55.3496
I0318 08:07:53.228886  3501 solver.cpp:244]     Train net output #0: loss = 55.3495 (* 1 = 55.3495 loss)
I0318 08:07:53.228900  3501 sgd_solver.cpp:106] Iteration 12420, lr = 0.01
I0318 08:08:00.553925  3501 solver.cpp:228] Iteration 12440, loss = 28.6276
I0318 08:08:00.554092  3501 solver.cpp:244]     Train net output #0: loss = 28.6276 (* 1 = 28.6276 loss)
I0318 08:08:00.554108  3501 sgd_solver.cpp:106] Iteration 12440, lr = 0.01
I0318 08:08:07.875766  3501 solver.cpp:228] Iteration 12460, loss = 45.4075
I0318 08:08:07.875835  3501 solver.cpp:244]     Train net output #0: loss = 45.4075 (* 1 = 45.4075 loss)
I0318 08:08:07.875849  3501 sgd_solver.cpp:106] Iteration 12460, lr = 0.01
I0318 08:08:15.201762  3501 solver.cpp:228] Iteration 12480, loss = 27.2396
I0318 08:08:15.201829  3501 solver.cpp:244]     Train net output #0: loss = 27.2395 (* 1 = 27.2395 loss)
I0318 08:08:15.201844  3501 sgd_solver.cpp:106] Iteration 12480, lr = 0.01
I0318 08:08:22.165208  3501 solver.cpp:337] Iteration 12500, Testing net (#0)
I0318 08:10:16.366014  3501 solver.cpp:404]     Test net output #0: loss = 607.277 (* 1 = 607.277 loss)
I0318 08:10:16.366132  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:10:16.702159  3501 solver.cpp:228] Iteration 12500, loss = 24.5422
I0318 08:10:16.702231  3501 solver.cpp:244]     Train net output #0: loss = 24.5421 (* 1 = 24.5421 loss)
I0318 08:10:16.702246  3501 sgd_solver.cpp:106] Iteration 12500, lr = 0.01
I0318 08:10:23.943529  3501 solver.cpp:228] Iteration 12520, loss = 56.4326
I0318 08:10:23.943596  3501 solver.cpp:244]     Train net output #0: loss = 56.4326 (* 1 = 56.4326 loss)
I0318 08:10:23.943610  3501 sgd_solver.cpp:106] Iteration 12520, lr = 0.01
I0318 08:10:31.233566  3501 solver.cpp:228] Iteration 12540, loss = 50.1689
I0318 08:10:31.233633  3501 solver.cpp:244]     Train net output #0: loss = 50.1689 (* 1 = 50.1689 loss)
I0318 08:10:31.233647  3501 sgd_solver.cpp:106] Iteration 12540, lr = 0.01
I0318 08:10:38.549741  3501 solver.cpp:228] Iteration 12560, loss = 53.1178
I0318 08:10:38.549808  3501 solver.cpp:244]     Train net output #0: loss = 53.1178 (* 1 = 53.1178 loss)
I0318 08:10:38.549821  3501 sgd_solver.cpp:106] Iteration 12560, lr = 0.01
I0318 08:10:45.882328  3501 solver.cpp:228] Iteration 12580, loss = 62.3421
I0318 08:10:45.882393  3501 solver.cpp:244]     Train net output #0: loss = 62.342 (* 1 = 62.342 loss)
I0318 08:10:45.882406  3501 sgd_solver.cpp:106] Iteration 12580, lr = 0.01
I0318 08:10:53.225653  3501 solver.cpp:228] Iteration 12600, loss = 49.7628
I0318 08:10:53.225841  3501 solver.cpp:244]     Train net output #0: loss = 49.7627 (* 1 = 49.7627 loss)
I0318 08:10:53.225859  3501 sgd_solver.cpp:106] Iteration 12600, lr = 0.01
I0318 08:11:00.568867  3501 solver.cpp:228] Iteration 12620, loss = 40.9842
I0318 08:11:00.568933  3501 solver.cpp:244]     Train net output #0: loss = 40.9842 (* 1 = 40.9842 loss)
I0318 08:11:00.568945  3501 sgd_solver.cpp:106] Iteration 12620, lr = 0.01
I0318 08:11:07.902518  3501 solver.cpp:228] Iteration 12640, loss = 34.2147
I0318 08:11:07.902586  3501 solver.cpp:244]     Train net output #0: loss = 34.2146 (* 1 = 34.2146 loss)
I0318 08:11:07.902600  3501 sgd_solver.cpp:106] Iteration 12640, lr = 0.01
I0318 08:11:15.230268  3501 solver.cpp:228] Iteration 12660, loss = 33.7018
I0318 08:11:15.230334  3501 solver.cpp:244]     Train net output #0: loss = 33.7017 (* 1 = 33.7017 loss)
I0318 08:11:15.230348  3501 sgd_solver.cpp:106] Iteration 12660, lr = 0.01
I0318 08:11:22.561386  3501 solver.cpp:228] Iteration 12680, loss = 63.0274
I0318 08:11:22.561450  3501 solver.cpp:244]     Train net output #0: loss = 63.0274 (* 1 = 63.0274 loss)
I0318 08:11:22.561465  3501 sgd_solver.cpp:106] Iteration 12680, lr = 0.01
I0318 08:11:29.890328  3501 solver.cpp:228] Iteration 12700, loss = 62.0163
I0318 08:11:29.890480  3501 solver.cpp:244]     Train net output #0: loss = 62.0162 (* 1 = 62.0162 loss)
I0318 08:11:29.890496  3501 sgd_solver.cpp:106] Iteration 12700, lr = 0.01
I0318 08:11:37.225330  3501 solver.cpp:228] Iteration 12720, loss = 30.8996
I0318 08:11:37.225395  3501 solver.cpp:244]     Train net output #0: loss = 30.8995 (* 1 = 30.8995 loss)
I0318 08:11:37.225409  3501 sgd_solver.cpp:106] Iteration 12720, lr = 0.01
I0318 08:11:44.558540  3501 solver.cpp:228] Iteration 12740, loss = 53.5029
I0318 08:11:44.558606  3501 solver.cpp:244]     Train net output #0: loss = 53.5028 (* 1 = 53.5028 loss)
I0318 08:11:44.558620  3501 sgd_solver.cpp:106] Iteration 12740, lr = 0.01
I0318 08:11:47.856207  3501 solver.cpp:337] Iteration 12750, Testing net (#0)
I0318 08:13:42.099407  3501 solver.cpp:404]     Test net output #0: loss = 645.945 (* 1 = 645.945 loss)
I0318 08:13:42.099534  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:13:46.054857  3501 solver.cpp:228] Iteration 12760, loss = 72.3518
I0318 08:13:46.054927  3501 solver.cpp:244]     Train net output #0: loss = 72.3518 (* 1 = 72.3518 loss)
I0318 08:13:46.054941  3501 sgd_solver.cpp:106] Iteration 12760, lr = 0.01
I0318 08:13:53.336036  3501 solver.cpp:228] Iteration 12780, loss = 50.8856
I0318 08:13:53.336100  3501 solver.cpp:244]     Train net output #0: loss = 50.8855 (* 1 = 50.8855 loss)
I0318 08:13:53.336113  3501 sgd_solver.cpp:106] Iteration 12780, lr = 0.01
I0318 08:14:00.645855  3501 solver.cpp:228] Iteration 12800, loss = 40.0058
I0318 08:14:00.645920  3501 solver.cpp:244]     Train net output #0: loss = 40.0058 (* 1 = 40.0058 loss)
I0318 08:14:00.645932  3501 sgd_solver.cpp:106] Iteration 12800, lr = 0.01
I0318 08:14:07.956442  3501 solver.cpp:228] Iteration 12820, loss = 26.8449
I0318 08:14:07.956512  3501 solver.cpp:244]     Train net output #0: loss = 26.8448 (* 1 = 26.8448 loss)
I0318 08:14:07.956527  3501 sgd_solver.cpp:106] Iteration 12820, lr = 0.01
I0318 08:14:15.278255  3501 solver.cpp:228] Iteration 12840, loss = 27.1662
I0318 08:14:15.278378  3501 solver.cpp:244]     Train net output #0: loss = 27.1661 (* 1 = 27.1661 loss)
I0318 08:14:15.278394  3501 sgd_solver.cpp:106] Iteration 12840, lr = 0.01
I0318 08:14:22.613736  3501 solver.cpp:228] Iteration 12860, loss = 33.3754
I0318 08:14:22.613801  3501 solver.cpp:244]     Train net output #0: loss = 33.3753 (* 1 = 33.3753 loss)
I0318 08:14:22.613816  3501 sgd_solver.cpp:106] Iteration 12860, lr = 0.01
I0318 08:14:29.954077  3501 solver.cpp:228] Iteration 12880, loss = 70.5847
I0318 08:14:29.954143  3501 solver.cpp:244]     Train net output #0: loss = 70.5847 (* 1 = 70.5847 loss)
I0318 08:14:29.954156  3501 sgd_solver.cpp:106] Iteration 12880, lr = 0.01
I0318 08:14:37.294679  3501 solver.cpp:228] Iteration 12900, loss = 36.4845
I0318 08:14:37.294745  3501 solver.cpp:244]     Train net output #0: loss = 36.4844 (* 1 = 36.4844 loss)
I0318 08:14:37.294759  3501 sgd_solver.cpp:106] Iteration 12900, lr = 0.01
I0318 08:14:44.630096  3501 solver.cpp:228] Iteration 12920, loss = 54.9476
I0318 08:14:44.630164  3501 solver.cpp:244]     Train net output #0: loss = 54.9476 (* 1 = 54.9476 loss)
I0318 08:14:44.630179  3501 sgd_solver.cpp:106] Iteration 12920, lr = 0.01
I0318 08:14:51.963001  3501 solver.cpp:228] Iteration 12940, loss = 89.4205
I0318 08:14:51.963188  3501 solver.cpp:244]     Train net output #0: loss = 89.4205 (* 1 = 89.4205 loss)
I0318 08:14:51.963205  3501 sgd_solver.cpp:106] Iteration 12940, lr = 0.01
I0318 08:14:59.307276  3501 solver.cpp:228] Iteration 12960, loss = 28.0051
I0318 08:14:59.307340  3501 solver.cpp:244]     Train net output #0: loss = 28.0051 (* 1 = 28.0051 loss)
I0318 08:14:59.307354  3501 sgd_solver.cpp:106] Iteration 12960, lr = 0.01
I0318 08:15:06.646399  3501 solver.cpp:228] Iteration 12980, loss = 31.1446
I0318 08:15:06.646466  3501 solver.cpp:244]     Train net output #0: loss = 31.1446 (* 1 = 31.1446 loss)
I0318 08:15:06.646479  3501 sgd_solver.cpp:106] Iteration 12980, lr = 0.01
I0318 08:15:13.613059  3501 solver.cpp:337] Iteration 13000, Testing net (#0)
I0318 08:17:07.808334  3501 solver.cpp:404]     Test net output #0: loss = 590.098 (* 1 = 590.098 loss)
I0318 08:17:07.808454  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:17:08.145066  3501 solver.cpp:228] Iteration 13000, loss = 11.3507
I0318 08:17:08.145133  3501 solver.cpp:244]     Train net output #0: loss = 11.3507 (* 1 = 11.3507 loss)
I0318 08:17:08.145148  3501 sgd_solver.cpp:106] Iteration 13000, lr = 0.01
I0318 08:17:15.398109  3501 solver.cpp:228] Iteration 13020, loss = 27.2994
I0318 08:17:15.398175  3501 solver.cpp:244]     Train net output #0: loss = 27.2994 (* 1 = 27.2994 loss)
I0318 08:17:15.398187  3501 sgd_solver.cpp:106] Iteration 13020, lr = 0.01
I0318 08:17:22.687629  3501 solver.cpp:228] Iteration 13040, loss = 42.6541
I0318 08:17:22.687697  3501 solver.cpp:244]     Train net output #0: loss = 42.654 (* 1 = 42.654 loss)
I0318 08:17:22.687711  3501 sgd_solver.cpp:106] Iteration 13040, lr = 0.01
I0318 08:17:30.010557  3501 solver.cpp:228] Iteration 13060, loss = 43.992
I0318 08:17:30.010622  3501 solver.cpp:244]     Train net output #0: loss = 43.992 (* 1 = 43.992 loss)
I0318 08:17:30.010637  3501 sgd_solver.cpp:106] Iteration 13060, lr = 0.01
I0318 08:17:37.329807  3501 solver.cpp:228] Iteration 13080, loss = 41.3937
I0318 08:17:37.329871  3501 solver.cpp:244]     Train net output #0: loss = 41.3937 (* 1 = 41.3937 loss)
I0318 08:17:37.329885  3501 sgd_solver.cpp:106] Iteration 13080, lr = 0.01
I0318 08:17:44.655284  3501 solver.cpp:228] Iteration 13100, loss = 52.5873
I0318 08:17:44.655413  3501 solver.cpp:244]     Train net output #0: loss = 52.5872 (* 1 = 52.5872 loss)
I0318 08:17:44.655437  3501 sgd_solver.cpp:106] Iteration 13100, lr = 0.01
I0318 08:17:51.990417  3501 solver.cpp:228] Iteration 13120, loss = 45.6374
I0318 08:17:51.990481  3501 solver.cpp:244]     Train net output #0: loss = 45.6374 (* 1 = 45.6374 loss)
I0318 08:17:51.990495  3501 sgd_solver.cpp:106] Iteration 13120, lr = 0.01
I0318 08:17:59.322837  3501 solver.cpp:228] Iteration 13140, loss = 96.7421
I0318 08:17:59.322902  3501 solver.cpp:244]     Train net output #0: loss = 96.7421 (* 1 = 96.7421 loss)
I0318 08:17:59.322916  3501 sgd_solver.cpp:106] Iteration 13140, lr = 0.01
I0318 08:18:06.656613  3501 solver.cpp:228] Iteration 13160, loss = 39.9333
I0318 08:18:06.656678  3501 solver.cpp:244]     Train net output #0: loss = 39.9332 (* 1 = 39.9332 loss)
I0318 08:18:06.656692  3501 sgd_solver.cpp:106] Iteration 13160, lr = 0.01
I0318 08:18:13.993492  3501 solver.cpp:228] Iteration 13180, loss = 23.0802
I0318 08:18:13.993556  3501 solver.cpp:244]     Train net output #0: loss = 23.0801 (* 1 = 23.0801 loss)
I0318 08:18:13.993571  3501 sgd_solver.cpp:106] Iteration 13180, lr = 0.01
I0318 08:18:21.321616  3501 solver.cpp:228] Iteration 13200, loss = 28.7693
I0318 08:18:21.321797  3501 solver.cpp:244]     Train net output #0: loss = 28.7692 (* 1 = 28.7692 loss)
I0318 08:18:21.321812  3501 sgd_solver.cpp:106] Iteration 13200, lr = 0.01
I0318 08:18:28.653534  3501 solver.cpp:228] Iteration 13220, loss = 31.9749
I0318 08:18:28.653599  3501 solver.cpp:244]     Train net output #0: loss = 31.9748 (* 1 = 31.9748 loss)
I0318 08:18:28.653612  3501 sgd_solver.cpp:106] Iteration 13220, lr = 0.01
I0318 08:18:35.996057  3501 solver.cpp:228] Iteration 13240, loss = 60.4196
I0318 08:18:35.996130  3501 solver.cpp:244]     Train net output #0: loss = 60.4196 (* 1 = 60.4196 loss)
I0318 08:18:35.996145  3501 sgd_solver.cpp:106] Iteration 13240, lr = 0.01
I0318 08:18:39.303344  3501 solver.cpp:337] Iteration 13250, Testing net (#0)
I0318 08:20:33.518263  3501 solver.cpp:404]     Test net output #0: loss = 583.883 (* 1 = 583.883 loss)
I0318 08:20:33.518429  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 08:20:37.463593  3501 solver.cpp:228] Iteration 13260, loss = 71.3122
I0318 08:20:37.463659  3501 solver.cpp:244]     Train net output #0: loss = 71.3121 (* 1 = 71.3121 loss)
I0318 08:20:37.463671  3501 sgd_solver.cpp:106] Iteration 13260, lr = 0.01
I0318 08:20:44.728554  3501 solver.cpp:228] Iteration 13280, loss = 50.9678
I0318 08:20:44.728612  3501 solver.cpp:244]     Train net output #0: loss = 50.9677 (* 1 = 50.9677 loss)
I0318 08:20:44.728626  3501 sgd_solver.cpp:106] Iteration 13280, lr = 0.01
I0318 08:20:52.026510  3501 solver.cpp:228] Iteration 13300, loss = 93.936
I0318 08:20:52.026576  3501 solver.cpp:244]     Train net output #0: loss = 93.9359 (* 1 = 93.9359 loss)
I0318 08:20:52.026592  3501 sgd_solver.cpp:106] Iteration 13300, lr = 0.01
I0318 08:20:59.338287  3501 solver.cpp:228] Iteration 13320, loss = 72.1218
I0318 08:20:59.338353  3501 solver.cpp:244]     Train net output #0: loss = 72.1217 (* 1 = 72.1217 loss)
I0318 08:20:59.338367  3501 sgd_solver.cpp:106] Iteration 13320, lr = 0.01
I0318 08:21:06.659787  3501 solver.cpp:228] Iteration 13340, loss = 45.0677
I0318 08:21:06.659935  3501 solver.cpp:244]     Train net output #0: loss = 45.0676 (* 1 = 45.0676 loss)
I0318 08:21:06.659950  3501 sgd_solver.cpp:106] Iteration 13340, lr = 0.01
I0318 08:21:13.996311  3501 solver.cpp:228] Iteration 13360, loss = 13.1765
I0318 08:21:13.996379  3501 solver.cpp:244]     Train net output #0: loss = 13.1764 (* 1 = 13.1764 loss)
I0318 08:21:13.996394  3501 sgd_solver.cpp:106] Iteration 13360, lr = 0.01
I0318 08:21:21.332787  3501 solver.cpp:228] Iteration 13380, loss = 28.7089
I0318 08:21:21.332852  3501 solver.cpp:244]     Train net output #0: loss = 28.7088 (* 1 = 28.7088 loss)
I0318 08:21:21.332866  3501 sgd_solver.cpp:106] Iteration 13380, lr = 0.01
I0318 08:21:28.669972  3501 solver.cpp:228] Iteration 13400, loss = 36.6109
I0318 08:21:28.670038  3501 solver.cpp:244]     Train net output #0: loss = 36.6109 (* 1 = 36.6109 loss)
I0318 08:21:28.670053  3501 sgd_solver.cpp:106] Iteration 13400, lr = 0.01
I0318 08:21:36.001029  3501 solver.cpp:228] Iteration 13420, loss = 73.2551
I0318 08:21:36.001103  3501 solver.cpp:244]     Train net output #0: loss = 73.2551 (* 1 = 73.2551 loss)
I0318 08:21:36.001118  3501 sgd_solver.cpp:106] Iteration 13420, lr = 0.01
I0318 08:21:43.326589  3501 solver.cpp:228] Iteration 13440, loss = 32.5121
I0318 08:21:43.326719  3501 solver.cpp:244]     Train net output #0: loss = 32.512 (* 1 = 32.512 loss)
I0318 08:21:43.326733  3501 sgd_solver.cpp:106] Iteration 13440, lr = 0.01
I0318 08:21:50.658149  3501 solver.cpp:228] Iteration 13460, loss = 63.8761
I0318 08:21:50.658219  3501 solver.cpp:244]     Train net output #0: loss = 63.876 (* 1 = 63.876 loss)
I0318 08:21:50.658234  3501 sgd_solver.cpp:106] Iteration 13460, lr = 0.01
I0318 08:21:57.987560  3501 solver.cpp:228] Iteration 13480, loss = 40.1543
I0318 08:21:57.987625  3501 solver.cpp:244]     Train net output #0: loss = 40.1542 (* 1 = 40.1542 loss)
I0318 08:21:57.987639  3501 sgd_solver.cpp:106] Iteration 13480, lr = 0.01
I0318 08:22:04.947911  3501 solver.cpp:337] Iteration 13500, Testing net (#0)
I0318 08:23:59.151878  3501 solver.cpp:404]     Test net output #0: loss = 612.646 (* 1 = 612.646 loss)
I0318 08:23:59.152051  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 08:23:59.489351  3501 solver.cpp:228] Iteration 13500, loss = 61.6842
I0318 08:23:59.489423  3501 solver.cpp:244]     Train net output #0: loss = 61.6842 (* 1 = 61.6842 loss)
I0318 08:23:59.489437  3501 sgd_solver.cpp:106] Iteration 13500, lr = 0.01
I0318 08:24:06.734315  3501 solver.cpp:228] Iteration 13520, loss = 36.1279
I0318 08:24:06.734383  3501 solver.cpp:244]     Train net output #0: loss = 36.1278 (* 1 = 36.1278 loss)
I0318 08:24:06.734397  3501 sgd_solver.cpp:106] Iteration 13520, lr = 0.01
I0318 08:24:14.027209  3501 solver.cpp:228] Iteration 13540, loss = 30.9594
I0318 08:24:14.027273  3501 solver.cpp:244]     Train net output #0: loss = 30.9593 (* 1 = 30.9593 loss)
I0318 08:24:14.027287  3501 sgd_solver.cpp:106] Iteration 13540, lr = 0.01
I0318 08:24:21.342028  3501 solver.cpp:228] Iteration 13560, loss = 36.216
I0318 08:24:21.342094  3501 solver.cpp:244]     Train net output #0: loss = 36.2159 (* 1 = 36.2159 loss)
I0318 08:24:21.342113  3501 sgd_solver.cpp:106] Iteration 13560, lr = 0.01
I0318 08:24:28.656693  3501 solver.cpp:228] Iteration 13580, loss = 24.4682
I0318 08:24:28.656759  3501 solver.cpp:244]     Train net output #0: loss = 24.4681 (* 1 = 24.4681 loss)
I0318 08:24:28.656774  3501 sgd_solver.cpp:106] Iteration 13580, lr = 0.01
I0318 08:24:35.978344  3501 solver.cpp:228] Iteration 13600, loss = 51.8208
I0318 08:24:35.978494  3501 solver.cpp:244]     Train net output #0: loss = 51.8207 (* 1 = 51.8207 loss)
I0318 08:24:35.978509  3501 sgd_solver.cpp:106] Iteration 13600, lr = 0.01
I0318 08:24:43.300055  3501 solver.cpp:228] Iteration 13620, loss = 25.3688
I0318 08:24:43.300122  3501 solver.cpp:244]     Train net output #0: loss = 25.3687 (* 1 = 25.3687 loss)
I0318 08:24:43.300137  3501 sgd_solver.cpp:106] Iteration 13620, lr = 0.01
I0318 08:24:50.624074  3501 solver.cpp:228] Iteration 13640, loss = 47.334
I0318 08:24:50.624140  3501 solver.cpp:244]     Train net output #0: loss = 47.3339 (* 1 = 47.3339 loss)
I0318 08:24:50.624155  3501 sgd_solver.cpp:106] Iteration 13640, lr = 0.01
I0318 08:24:57.958081  3501 solver.cpp:228] Iteration 13660, loss = 85.9546
I0318 08:24:57.958145  3501 solver.cpp:244]     Train net output #0: loss = 85.9545 (* 1 = 85.9545 loss)
I0318 08:24:57.958160  3501 sgd_solver.cpp:106] Iteration 13660, lr = 0.01
I0318 08:25:05.295326  3501 solver.cpp:228] Iteration 13680, loss = 63.3689
I0318 08:25:05.295397  3501 solver.cpp:244]     Train net output #0: loss = 63.3688 (* 1 = 63.3688 loss)
I0318 08:25:05.295413  3501 sgd_solver.cpp:106] Iteration 13680, lr = 0.01
I0318 08:25:12.635859  3501 solver.cpp:228] Iteration 13700, loss = 51.1988
I0318 08:25:12.636013  3501 solver.cpp:244]     Train net output #0: loss = 51.1987 (* 1 = 51.1987 loss)
I0318 08:25:12.636028  3501 sgd_solver.cpp:106] Iteration 13700, lr = 0.01
I0318 08:25:19.978751  3501 solver.cpp:228] Iteration 13720, loss = 27.944
I0318 08:25:19.978821  3501 solver.cpp:244]     Train net output #0: loss = 27.9439 (* 1 = 27.9439 loss)
I0318 08:25:19.978834  3501 sgd_solver.cpp:106] Iteration 13720, lr = 0.01
I0318 08:25:27.327508  3501 solver.cpp:228] Iteration 13740, loss = 23.8262
I0318 08:25:27.327576  3501 solver.cpp:244]     Train net output #0: loss = 23.8261 (* 1 = 23.8261 loss)
I0318 08:25:27.327590  3501 sgd_solver.cpp:106] Iteration 13740, lr = 0.01
I0318 08:25:30.630455  3501 solver.cpp:337] Iteration 13750, Testing net (#0)
I0318 08:27:24.809670  3501 solver.cpp:404]     Test net output #0: loss = 653.683 (* 1 = 653.683 loss)
I0318 08:27:24.809785  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:27:28.760732  3501 solver.cpp:228] Iteration 13760, loss = 26.3062
I0318 08:27:28.760797  3501 solver.cpp:244]     Train net output #0: loss = 26.3061 (* 1 = 26.3061 loss)
I0318 08:27:28.760812  3501 sgd_solver.cpp:106] Iteration 13760, lr = 0.01
I0318 08:27:36.022696  3501 solver.cpp:228] Iteration 13780, loss = 63.7501
I0318 08:27:36.022761  3501 solver.cpp:244]     Train net output #0: loss = 63.75 (* 1 = 63.75 loss)
I0318 08:27:36.022775  3501 sgd_solver.cpp:106] Iteration 13780, lr = 0.01
I0318 08:27:43.327381  3501 solver.cpp:228] Iteration 13800, loss = 23.4942
I0318 08:27:43.327447  3501 solver.cpp:244]     Train net output #0: loss = 23.4942 (* 1 = 23.4942 loss)
I0318 08:27:43.327461  3501 sgd_solver.cpp:106] Iteration 13800, lr = 0.01
I0318 08:27:50.647891  3501 solver.cpp:228] Iteration 13820, loss = 51.6663
I0318 08:27:50.647955  3501 solver.cpp:244]     Train net output #0: loss = 51.6662 (* 1 = 51.6662 loss)
I0318 08:27:50.647969  3501 sgd_solver.cpp:106] Iteration 13820, lr = 0.01
I0318 08:27:57.973503  3501 solver.cpp:228] Iteration 13840, loss = 65.1204
I0318 08:27:57.973649  3501 solver.cpp:244]     Train net output #0: loss = 65.1203 (* 1 = 65.1203 loss)
I0318 08:27:57.973664  3501 sgd_solver.cpp:106] Iteration 13840, lr = 0.01
I0318 08:28:05.305846  3501 solver.cpp:228] Iteration 13860, loss = 52.5065
I0318 08:28:05.305912  3501 solver.cpp:244]     Train net output #0: loss = 52.5064 (* 1 = 52.5064 loss)
I0318 08:28:05.305927  3501 sgd_solver.cpp:106] Iteration 13860, lr = 0.01
I0318 08:28:12.648905  3501 solver.cpp:228] Iteration 13880, loss = 19.6137
I0318 08:28:12.648977  3501 solver.cpp:244]     Train net output #0: loss = 19.6136 (* 1 = 19.6136 loss)
I0318 08:28:12.648993  3501 sgd_solver.cpp:106] Iteration 13880, lr = 0.01
I0318 08:28:19.986984  3501 solver.cpp:228] Iteration 13900, loss = 26.3954
I0318 08:28:19.987049  3501 solver.cpp:244]     Train net output #0: loss = 26.3953 (* 1 = 26.3953 loss)
I0318 08:28:19.987063  3501 sgd_solver.cpp:106] Iteration 13900, lr = 0.01
I0318 08:28:27.325809  3501 solver.cpp:228] Iteration 13920, loss = 48.7782
I0318 08:28:27.325881  3501 solver.cpp:244]     Train net output #0: loss = 48.7781 (* 1 = 48.7781 loss)
I0318 08:28:27.325894  3501 sgd_solver.cpp:106] Iteration 13920, lr = 0.01
I0318 08:28:34.656560  3501 solver.cpp:228] Iteration 13940, loss = 24.8082
I0318 08:28:34.656713  3501 solver.cpp:244]     Train net output #0: loss = 24.8081 (* 1 = 24.8081 loss)
I0318 08:28:34.656728  3501 sgd_solver.cpp:106] Iteration 13940, lr = 0.01
I0318 08:28:41.989594  3501 solver.cpp:228] Iteration 13960, loss = 44.9644
I0318 08:28:41.989667  3501 solver.cpp:244]     Train net output #0: loss = 44.9643 (* 1 = 44.9643 loss)
I0318 08:28:41.989682  3501 sgd_solver.cpp:106] Iteration 13960, lr = 0.01
I0318 08:28:49.331452  3501 solver.cpp:228] Iteration 13980, loss = 29.3623
I0318 08:28:49.331518  3501 solver.cpp:244]     Train net output #0: loss = 29.3622 (* 1 = 29.3622 loss)
I0318 08:28:49.331532  3501 sgd_solver.cpp:106] Iteration 13980, lr = 0.01
I0318 08:28:56.299082  3501 solver.cpp:337] Iteration 14000, Testing net (#0)
I0318 08:30:50.519346  3501 solver.cpp:404]     Test net output #0: loss = 669.328 (* 1 = 669.328 loss)
I0318 08:30:50.519471  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 08:30:50.855645  3501 solver.cpp:228] Iteration 14000, loss = 96.0964
I0318 08:30:50.855712  3501 solver.cpp:244]     Train net output #0: loss = 96.0962 (* 1 = 96.0962 loss)
I0318 08:30:50.855726  3501 sgd_solver.cpp:106] Iteration 14000, lr = 0.01
I0318 08:30:58.105240  3501 solver.cpp:228] Iteration 14020, loss = 75.427
I0318 08:30:58.105307  3501 solver.cpp:244]     Train net output #0: loss = 75.4269 (* 1 = 75.4269 loss)
I0318 08:30:58.105321  3501 sgd_solver.cpp:106] Iteration 14020, lr = 0.01
I0318 08:31:05.409368  3501 solver.cpp:228] Iteration 14040, loss = 68.6384
I0318 08:31:05.409432  3501 solver.cpp:244]     Train net output #0: loss = 68.6383 (* 1 = 68.6383 loss)
I0318 08:31:05.409446  3501 sgd_solver.cpp:106] Iteration 14040, lr = 0.01
I0318 08:31:12.739243  3501 solver.cpp:228] Iteration 14060, loss = 29.7055
I0318 08:31:12.739310  3501 solver.cpp:244]     Train net output #0: loss = 29.7054 (* 1 = 29.7054 loss)
I0318 08:31:12.739325  3501 sgd_solver.cpp:106] Iteration 14060, lr = 0.01
I0318 08:31:20.069267  3501 solver.cpp:228] Iteration 14080, loss = 32.5846
I0318 08:31:20.069334  3501 solver.cpp:244]     Train net output #0: loss = 32.5845 (* 1 = 32.5845 loss)
I0318 08:31:20.069347  3501 sgd_solver.cpp:106] Iteration 14080, lr = 0.01
I0318 08:31:27.400663  3501 solver.cpp:228] Iteration 14100, loss = 44.162
I0318 08:31:27.400847  3501 solver.cpp:244]     Train net output #0: loss = 44.1619 (* 1 = 44.1619 loss)
I0318 08:31:27.400863  3501 sgd_solver.cpp:106] Iteration 14100, lr = 0.01
I0318 08:31:34.730360  3501 solver.cpp:228] Iteration 14120, loss = 32.4265
I0318 08:31:34.730425  3501 solver.cpp:244]     Train net output #0: loss = 32.4264 (* 1 = 32.4264 loss)
I0318 08:31:34.730440  3501 sgd_solver.cpp:106] Iteration 14120, lr = 0.01
I0318 08:31:42.070853  3501 solver.cpp:228] Iteration 14140, loss = 50.9567
I0318 08:31:42.070921  3501 solver.cpp:244]     Train net output #0: loss = 50.9566 (* 1 = 50.9566 loss)
I0318 08:31:42.070935  3501 sgd_solver.cpp:106] Iteration 14140, lr = 0.01
I0318 08:31:49.411345  3501 solver.cpp:228] Iteration 14160, loss = 23.5217
I0318 08:31:49.411411  3501 solver.cpp:244]     Train net output #0: loss = 23.5216 (* 1 = 23.5216 loss)
I0318 08:31:49.411424  3501 sgd_solver.cpp:106] Iteration 14160, lr = 0.01
I0318 08:31:56.757782  3501 solver.cpp:228] Iteration 14180, loss = 33.0498
I0318 08:31:56.757848  3501 solver.cpp:244]     Train net output #0: loss = 33.0497 (* 1 = 33.0497 loss)
I0318 08:31:56.757863  3501 sgd_solver.cpp:106] Iteration 14180, lr = 0.01
I0318 08:32:04.101088  3501 solver.cpp:228] Iteration 14200, loss = 49.4272
I0318 08:32:04.101225  3501 solver.cpp:244]     Train net output #0: loss = 49.4271 (* 1 = 49.4271 loss)
I0318 08:32:04.101239  3501 sgd_solver.cpp:106] Iteration 14200, lr = 0.01
I0318 08:32:11.440377  3501 solver.cpp:228] Iteration 14220, loss = 36.4041
I0318 08:32:11.440443  3501 solver.cpp:244]     Train net output #0: loss = 36.404 (* 1 = 36.404 loss)
I0318 08:32:11.440455  3501 sgd_solver.cpp:106] Iteration 14220, lr = 0.01
I0318 08:32:18.775365  3501 solver.cpp:228] Iteration 14240, loss = 34.0052
I0318 08:32:18.775431  3501 solver.cpp:244]     Train net output #0: loss = 34.005 (* 1 = 34.005 loss)
I0318 08:32:18.775444  3501 sgd_solver.cpp:106] Iteration 14240, lr = 0.01
I0318 08:32:22.076771  3501 solver.cpp:337] Iteration 14250, Testing net (#0)
I0318 08:34:16.304711  3501 solver.cpp:404]     Test net output #0: loss = 615.123 (* 1 = 615.123 loss)
I0318 08:34:16.304787  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 08:34:20.259979  3501 solver.cpp:228] Iteration 14260, loss = 72.2634
I0318 08:34:20.260056  3501 solver.cpp:244]     Train net output #0: loss = 72.2633 (* 1 = 72.2633 loss)
I0318 08:34:20.260071  3501 sgd_solver.cpp:106] Iteration 14260, lr = 0.01
I0318 08:34:27.539891  3501 solver.cpp:228] Iteration 14280, loss = 48.5425
I0318 08:34:27.539968  3501 solver.cpp:244]     Train net output #0: loss = 48.5424 (* 1 = 48.5424 loss)
I0318 08:34:27.539983  3501 sgd_solver.cpp:106] Iteration 14280, lr = 0.01
I0318 08:34:34.862632  3501 solver.cpp:228] Iteration 14300, loss = 23.8665
I0318 08:34:34.862696  3501 solver.cpp:244]     Train net output #0: loss = 23.8664 (* 1 = 23.8664 loss)
I0318 08:34:34.862711  3501 sgd_solver.cpp:106] Iteration 14300, lr = 0.01
I0318 08:34:42.187114  3501 solver.cpp:228] Iteration 14320, loss = 63.0317
I0318 08:34:42.187183  3501 solver.cpp:244]     Train net output #0: loss = 63.0316 (* 1 = 63.0316 loss)
I0318 08:34:42.187197  3501 sgd_solver.cpp:106] Iteration 14320, lr = 0.01
I0318 08:34:49.523890  3501 solver.cpp:228] Iteration 14340, loss = 48.0249
I0318 08:34:49.524065  3501 solver.cpp:244]     Train net output #0: loss = 48.0248 (* 1 = 48.0248 loss)
I0318 08:34:49.524080  3501 sgd_solver.cpp:106] Iteration 14340, lr = 0.01
I0318 08:34:56.858750  3501 solver.cpp:228] Iteration 14360, loss = 75.4839
I0318 08:34:56.858815  3501 solver.cpp:244]     Train net output #0: loss = 75.4838 (* 1 = 75.4838 loss)
I0318 08:34:56.858829  3501 sgd_solver.cpp:106] Iteration 14360, lr = 0.01
I0318 08:35:04.199019  3501 solver.cpp:228] Iteration 14380, loss = 53.7964
I0318 08:35:04.199086  3501 solver.cpp:244]     Train net output #0: loss = 53.7963 (* 1 = 53.7963 loss)
I0318 08:35:04.199100  3501 sgd_solver.cpp:106] Iteration 14380, lr = 0.01
I0318 08:35:11.536383  3501 solver.cpp:228] Iteration 14400, loss = 49.0074
I0318 08:35:11.536449  3501 solver.cpp:244]     Train net output #0: loss = 49.0073 (* 1 = 49.0073 loss)
I0318 08:35:11.536463  3501 sgd_solver.cpp:106] Iteration 14400, lr = 0.01
I0318 08:35:18.866907  3501 solver.cpp:228] Iteration 14420, loss = 37.3237
I0318 08:35:18.866972  3501 solver.cpp:244]     Train net output #0: loss = 37.3236 (* 1 = 37.3236 loss)
I0318 08:35:18.866986  3501 sgd_solver.cpp:106] Iteration 14420, lr = 0.01
I0318 08:35:26.204708  3501 solver.cpp:228] Iteration 14440, loss = 26.4983
I0318 08:35:26.204845  3501 solver.cpp:244]     Train net output #0: loss = 26.4982 (* 1 = 26.4982 loss)
I0318 08:35:26.204860  3501 sgd_solver.cpp:106] Iteration 14440, lr = 0.01
I0318 08:35:33.538939  3501 solver.cpp:228] Iteration 14460, loss = 33.7266
I0318 08:35:33.539005  3501 solver.cpp:244]     Train net output #0: loss = 33.7265 (* 1 = 33.7265 loss)
I0318 08:35:33.539018  3501 sgd_solver.cpp:106] Iteration 14460, lr = 0.01
I0318 08:35:40.877511  3501 solver.cpp:228] Iteration 14480, loss = 36.2498
I0318 08:35:40.877580  3501 solver.cpp:244]     Train net output #0: loss = 36.2496 (* 1 = 36.2496 loss)
I0318 08:35:40.877595  3501 sgd_solver.cpp:106] Iteration 14480, lr = 0.01
I0318 08:35:47.854676  3501 solver.cpp:337] Iteration 14500, Testing net (#0)
I0318 08:37:42.064929  3501 solver.cpp:404]     Test net output #0: loss = 633.404 (* 1 = 633.404 loss)
I0318 08:37:42.065050  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 08:37:42.403093  3501 solver.cpp:228] Iteration 14500, loss = 25.1043
I0318 08:37:42.403162  3501 solver.cpp:244]     Train net output #0: loss = 25.1042 (* 1 = 25.1042 loss)
I0318 08:37:42.403175  3501 sgd_solver.cpp:106] Iteration 14500, lr = 0.01
I0318 08:37:49.678319  3501 solver.cpp:228] Iteration 14520, loss = 35.4284
I0318 08:37:49.678386  3501 solver.cpp:244]     Train net output #0: loss = 35.4283 (* 1 = 35.4283 loss)
I0318 08:37:49.678403  3501 sgd_solver.cpp:106] Iteration 14520, lr = 0.01
I0318 08:37:56.984501  3501 solver.cpp:228] Iteration 14540, loss = 38.0539
I0318 08:37:56.984565  3501 solver.cpp:244]     Train net output #0: loss = 38.0538 (* 1 = 38.0538 loss)
I0318 08:37:56.984580  3501 sgd_solver.cpp:106] Iteration 14540, lr = 0.01
I0318 08:38:04.297904  3501 solver.cpp:228] Iteration 14560, loss = 42.6105
I0318 08:38:04.297971  3501 solver.cpp:244]     Train net output #0: loss = 42.6104 (* 1 = 42.6104 loss)
I0318 08:38:04.297984  3501 sgd_solver.cpp:106] Iteration 14560, lr = 0.01
I0318 08:38:11.621052  3501 solver.cpp:228] Iteration 14580, loss = 45.0543
I0318 08:38:11.621121  3501 solver.cpp:244]     Train net output #0: loss = 45.0542 (* 1 = 45.0542 loss)
I0318 08:38:11.621135  3501 sgd_solver.cpp:106] Iteration 14580, lr = 0.01
I0318 08:38:18.945433  3501 solver.cpp:228] Iteration 14600, loss = 28.2894
I0318 08:38:18.945588  3501 solver.cpp:244]     Train net output #0: loss = 28.2892 (* 1 = 28.2892 loss)
I0318 08:38:18.945603  3501 sgd_solver.cpp:106] Iteration 14600, lr = 0.01
I0318 08:38:26.273059  3501 solver.cpp:228] Iteration 14620, loss = 25.9107
I0318 08:38:26.273126  3501 solver.cpp:244]     Train net output #0: loss = 25.9106 (* 1 = 25.9106 loss)
I0318 08:38:26.273140  3501 sgd_solver.cpp:106] Iteration 14620, lr = 0.01
I0318 08:38:33.599916  3501 solver.cpp:228] Iteration 14640, loss = 25.1783
I0318 08:38:33.599982  3501 solver.cpp:244]     Train net output #0: loss = 25.1781 (* 1 = 25.1781 loss)
I0318 08:38:33.599994  3501 sgd_solver.cpp:106] Iteration 14640, lr = 0.01
I0318 08:38:40.927902  3501 solver.cpp:228] Iteration 14660, loss = 37.1283
I0318 08:38:40.927968  3501 solver.cpp:244]     Train net output #0: loss = 37.1281 (* 1 = 37.1281 loss)
I0318 08:38:40.927983  3501 sgd_solver.cpp:106] Iteration 14660, lr = 0.01
I0318 08:38:48.258034  3501 solver.cpp:228] Iteration 14680, loss = 47.1763
I0318 08:38:48.258100  3501 solver.cpp:244]     Train net output #0: loss = 47.1761 (* 1 = 47.1761 loss)
I0318 08:38:48.258113  3501 sgd_solver.cpp:106] Iteration 14680, lr = 0.01
I0318 08:38:55.586639  3501 solver.cpp:228] Iteration 14700, loss = 36.2418
I0318 08:38:55.586843  3501 solver.cpp:244]     Train net output #0: loss = 36.2416 (* 1 = 36.2416 loss)
I0318 08:38:55.586859  3501 sgd_solver.cpp:106] Iteration 14700, lr = 0.01
I0318 08:39:02.922188  3501 solver.cpp:228] Iteration 14720, loss = 79.448
I0318 08:39:02.922255  3501 solver.cpp:244]     Train net output #0: loss = 79.4479 (* 1 = 79.4479 loss)
I0318 08:39:02.922268  3501 sgd_solver.cpp:106] Iteration 14720, lr = 0.01
I0318 08:39:10.268488  3501 solver.cpp:228] Iteration 14740, loss = 54.8688
I0318 08:39:10.268558  3501 solver.cpp:244]     Train net output #0: loss = 54.8686 (* 1 = 54.8686 loss)
I0318 08:39:10.268571  3501 sgd_solver.cpp:106] Iteration 14740, lr = 0.01
I0318 08:39:13.569332  3501 solver.cpp:337] Iteration 14750, Testing net (#0)
I0318 08:41:07.804690  3501 solver.cpp:404]     Test net output #0: loss = 607.317 (* 1 = 607.317 loss)
I0318 08:41:07.804824  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:41:11.763636  3501 solver.cpp:228] Iteration 14760, loss = 64.4352
I0318 08:41:11.763708  3501 solver.cpp:244]     Train net output #0: loss = 64.435 (* 1 = 64.435 loss)
I0318 08:41:11.763723  3501 sgd_solver.cpp:106] Iteration 14760, lr = 0.01
I0318 08:41:19.049594  3501 solver.cpp:228] Iteration 14780, loss = 35.5717
I0318 08:41:19.049667  3501 solver.cpp:244]     Train net output #0: loss = 35.5715 (* 1 = 35.5715 loss)
I0318 08:41:19.049682  3501 sgd_solver.cpp:106] Iteration 14780, lr = 0.01
I0318 08:41:26.366798  3501 solver.cpp:228] Iteration 14800, loss = 31.5973
I0318 08:41:26.366869  3501 solver.cpp:244]     Train net output #0: loss = 31.5971 (* 1 = 31.5971 loss)
I0318 08:41:26.366884  3501 sgd_solver.cpp:106] Iteration 14800, lr = 0.01
I0318 08:41:33.695953  3501 solver.cpp:228] Iteration 14820, loss = 12.1087
I0318 08:41:33.696019  3501 solver.cpp:244]     Train net output #0: loss = 12.1085 (* 1 = 12.1085 loss)
I0318 08:41:33.696033  3501 sgd_solver.cpp:106] Iteration 14820, lr = 0.01
I0318 08:41:41.021140  3501 solver.cpp:228] Iteration 14840, loss = 28.2847
I0318 08:41:41.021291  3501 solver.cpp:244]     Train net output #0: loss = 28.2845 (* 1 = 28.2845 loss)
I0318 08:41:41.021307  3501 sgd_solver.cpp:106] Iteration 14840, lr = 0.01
I0318 08:41:48.348316  3501 solver.cpp:228] Iteration 14860, loss = 50.3095
I0318 08:41:48.348379  3501 solver.cpp:244]     Train net output #0: loss = 50.3094 (* 1 = 50.3094 loss)
I0318 08:41:48.348393  3501 sgd_solver.cpp:106] Iteration 14860, lr = 0.01
I0318 08:41:55.691392  3501 solver.cpp:228] Iteration 14880, loss = 32.1006
I0318 08:41:55.691458  3501 solver.cpp:244]     Train net output #0: loss = 32.1004 (* 1 = 32.1004 loss)
I0318 08:41:55.691473  3501 sgd_solver.cpp:106] Iteration 14880, lr = 0.01
I0318 08:42:03.040163  3501 solver.cpp:228] Iteration 14900, loss = 63.9436
I0318 08:42:03.040225  3501 solver.cpp:244]     Train net output #0: loss = 63.9435 (* 1 = 63.9435 loss)
I0318 08:42:03.040240  3501 sgd_solver.cpp:106] Iteration 14900, lr = 0.01
I0318 08:42:10.388658  3501 solver.cpp:228] Iteration 14920, loss = 69.5292
I0318 08:42:10.388722  3501 solver.cpp:244]     Train net output #0: loss = 69.529 (* 1 = 69.529 loss)
I0318 08:42:10.388736  3501 sgd_solver.cpp:106] Iteration 14920, lr = 0.01
I0318 08:42:17.732741  3501 solver.cpp:228] Iteration 14940, loss = 56.3826
I0318 08:42:17.732945  3501 solver.cpp:244]     Train net output #0: loss = 56.3825 (* 1 = 56.3825 loss)
I0318 08:42:17.732960  3501 sgd_solver.cpp:106] Iteration 14940, lr = 0.01
I0318 08:42:25.068509  3501 solver.cpp:228] Iteration 14960, loss = 44.1412
I0318 08:42:25.068577  3501 solver.cpp:244]     Train net output #0: loss = 44.1411 (* 1 = 44.1411 loss)
I0318 08:42:25.068590  3501 sgd_solver.cpp:106] Iteration 14960, lr = 0.01
I0318 08:42:32.409603  3501 solver.cpp:228] Iteration 14980, loss = 9.83419
I0318 08:42:32.409669  3501 solver.cpp:244]     Train net output #0: loss = 9.83406 (* 1 = 9.83406 loss)
I0318 08:42:32.409682  3501 sgd_solver.cpp:106] Iteration 14980, lr = 0.01
I0318 08:42:39.383558  3501 solver.cpp:454] Snapshotting to binary proto file ./caffe_alexnet_train_iter_15000.caffemodel
I0318 08:42:41.015648  3501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./caffe_alexnet_train_iter_15000.solverstate
I0318 08:42:41.420812  3501 solver.cpp:337] Iteration 15000, Testing net (#0)
I0318 08:44:35.588996  3501 solver.cpp:404]     Test net output #0: loss = 638.833 (* 1 = 638.833 loss)
I0318 08:44:35.589109  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:44:35.925998  3501 solver.cpp:228] Iteration 15000, loss = 32.222
I0318 08:44:35.926064  3501 solver.cpp:244]     Train net output #0: loss = 32.2219 (* 1 = 32.2219 loss)
I0318 08:44:35.926077  3501 sgd_solver.cpp:106] Iteration 15000, lr = 0.01
I0318 08:44:43.166287  3501 solver.cpp:228] Iteration 15020, loss = 30.814
I0318 08:44:43.166347  3501 solver.cpp:244]     Train net output #0: loss = 30.8139 (* 1 = 30.8139 loss)
I0318 08:44:43.166362  3501 sgd_solver.cpp:106] Iteration 15020, lr = 0.01
I0318 08:44:50.443357  3501 solver.cpp:228] Iteration 15040, loss = 50.2774
I0318 08:44:50.443430  3501 solver.cpp:244]     Train net output #0: loss = 50.2773 (* 1 = 50.2773 loss)
I0318 08:44:50.443446  3501 sgd_solver.cpp:106] Iteration 15040, lr = 0.01
I0318 08:44:57.775703  3501 solver.cpp:228] Iteration 15060, loss = 49.5037
I0318 08:44:57.775784  3501 solver.cpp:244]     Train net output #0: loss = 49.5036 (* 1 = 49.5036 loss)
I0318 08:44:57.775800  3501 sgd_solver.cpp:106] Iteration 15060, lr = 0.01
I0318 08:45:05.108563  3501 solver.cpp:228] Iteration 15080, loss = 51.3176
I0318 08:45:05.108635  3501 solver.cpp:244]     Train net output #0: loss = 51.3175 (* 1 = 51.3175 loss)
I0318 08:45:05.108650  3501 sgd_solver.cpp:106] Iteration 15080, lr = 0.01
I0318 08:45:12.448869  3501 solver.cpp:228] Iteration 15100, loss = 71.0263
I0318 08:45:12.449023  3501 solver.cpp:244]     Train net output #0: loss = 71.0261 (* 1 = 71.0261 loss)
I0318 08:45:12.449038  3501 sgd_solver.cpp:106] Iteration 15100, lr = 0.01
I0318 08:45:19.778743  3501 solver.cpp:228] Iteration 15120, loss = 53.8909
I0318 08:45:19.778810  3501 solver.cpp:244]     Train net output #0: loss = 53.8908 (* 1 = 53.8908 loss)
I0318 08:45:19.778825  3501 sgd_solver.cpp:106] Iteration 15120, lr = 0.01
I0318 08:45:27.120383  3501 solver.cpp:228] Iteration 15140, loss = 62.0828
I0318 08:45:27.120448  3501 solver.cpp:244]     Train net output #0: loss = 62.0827 (* 1 = 62.0827 loss)
I0318 08:45:27.120467  3501 sgd_solver.cpp:106] Iteration 15140, lr = 0.01
I0318 08:45:34.457361  3501 solver.cpp:228] Iteration 15160, loss = 33.0554
I0318 08:45:34.457428  3501 solver.cpp:244]     Train net output #0: loss = 33.0553 (* 1 = 33.0553 loss)
I0318 08:45:34.457443  3501 sgd_solver.cpp:106] Iteration 15160, lr = 0.01
I0318 08:45:41.793118  3501 solver.cpp:228] Iteration 15180, loss = 23.5353
I0318 08:45:41.793184  3501 solver.cpp:244]     Train net output #0: loss = 23.5352 (* 1 = 23.5352 loss)
I0318 08:45:41.793203  3501 sgd_solver.cpp:106] Iteration 15180, lr = 0.01
I0318 08:45:49.123486  3501 solver.cpp:228] Iteration 15200, loss = 26.5757
I0318 08:45:49.123680  3501 solver.cpp:244]     Train net output #0: loss = 26.5756 (* 1 = 26.5756 loss)
I0318 08:45:49.123695  3501 sgd_solver.cpp:106] Iteration 15200, lr = 0.01
I0318 08:45:56.457820  3501 solver.cpp:228] Iteration 15220, loss = 45.6746
I0318 08:45:56.457885  3501 solver.cpp:244]     Train net output #0: loss = 45.6745 (* 1 = 45.6745 loss)
I0318 08:45:56.457900  3501 sgd_solver.cpp:106] Iteration 15220, lr = 0.01
I0318 08:46:03.794101  3501 solver.cpp:228] Iteration 15240, loss = 48.687
I0318 08:46:03.794167  3501 solver.cpp:244]     Train net output #0: loss = 48.6869 (* 1 = 48.6869 loss)
I0318 08:46:03.794181  3501 sgd_solver.cpp:106] Iteration 15240, lr = 0.01
I0318 08:46:07.094961  3501 solver.cpp:337] Iteration 15250, Testing net (#0)
I0318 08:48:01.315592  3501 solver.cpp:404]     Test net output #0: loss = 645.533 (* 1 = 645.533 loss)
I0318 08:48:01.315676  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 08:48:05.277122  3501 solver.cpp:228] Iteration 15260, loss = 47.187
I0318 08:48:05.277186  3501 solver.cpp:244]     Train net output #0: loss = 47.1869 (* 1 = 47.1869 loss)
I0318 08:48:05.277199  3501 sgd_solver.cpp:106] Iteration 15260, lr = 0.01
I0318 08:48:12.573369  3501 solver.cpp:228] Iteration 15280, loss = 40.4021
I0318 08:48:12.573436  3501 solver.cpp:244]     Train net output #0: loss = 40.4019 (* 1 = 40.4019 loss)
I0318 08:48:12.573451  3501 sgd_solver.cpp:106] Iteration 15280, lr = 0.01
I0318 08:48:19.894852  3501 solver.cpp:228] Iteration 15300, loss = 33.2236
I0318 08:48:19.894919  3501 solver.cpp:244]     Train net output #0: loss = 33.2235 (* 1 = 33.2235 loss)
I0318 08:48:19.894933  3501 sgd_solver.cpp:106] Iteration 15300, lr = 0.01
I0318 08:48:27.235272  3501 solver.cpp:228] Iteration 15320, loss = 64.1491
I0318 08:48:27.235337  3501 solver.cpp:244]     Train net output #0: loss = 64.149 (* 1 = 64.149 loss)
I0318 08:48:27.235352  3501 sgd_solver.cpp:106] Iteration 15320, lr = 0.01
I0318 08:48:34.577656  3501 solver.cpp:228] Iteration 15340, loss = 29.0516
I0318 08:48:34.577797  3501 solver.cpp:244]     Train net output #0: loss = 29.0515 (* 1 = 29.0515 loss)
I0318 08:48:34.577813  3501 sgd_solver.cpp:106] Iteration 15340, lr = 0.01
I0318 08:48:41.919064  3501 solver.cpp:228] Iteration 15360, loss = 34.8281
I0318 08:48:41.919131  3501 solver.cpp:244]     Train net output #0: loss = 34.8279 (* 1 = 34.8279 loss)
I0318 08:48:41.919145  3501 sgd_solver.cpp:106] Iteration 15360, lr = 0.01
I0318 08:48:49.261703  3501 solver.cpp:228] Iteration 15380, loss = 24.8457
I0318 08:48:49.261771  3501 solver.cpp:244]     Train net output #0: loss = 24.8455 (* 1 = 24.8455 loss)
I0318 08:48:49.261786  3501 sgd_solver.cpp:106] Iteration 15380, lr = 0.01
I0318 08:48:56.602064  3501 solver.cpp:228] Iteration 15400, loss = 50.6936
I0318 08:48:56.602131  3501 solver.cpp:244]     Train net output #0: loss = 50.6935 (* 1 = 50.6935 loss)
I0318 08:48:56.602144  3501 sgd_solver.cpp:106] Iteration 15400, lr = 0.01
I0318 08:49:03.944599  3501 solver.cpp:228] Iteration 15420, loss = 33.0418
I0318 08:49:03.944667  3501 solver.cpp:244]     Train net output #0: loss = 33.0417 (* 1 = 33.0417 loss)
I0318 08:49:03.944681  3501 sgd_solver.cpp:106] Iteration 15420, lr = 0.01
I0318 08:49:11.291117  3501 solver.cpp:228] Iteration 15440, loss = 52.2212
I0318 08:49:11.291273  3501 solver.cpp:244]     Train net output #0: loss = 52.221 (* 1 = 52.221 loss)
I0318 08:49:11.291288  3501 sgd_solver.cpp:106] Iteration 15440, lr = 0.01
I0318 08:49:18.631402  3501 solver.cpp:228] Iteration 15460, loss = 49.9455
I0318 08:49:18.631469  3501 solver.cpp:244]     Train net output #0: loss = 49.9454 (* 1 = 49.9454 loss)
I0318 08:49:18.631482  3501 sgd_solver.cpp:106] Iteration 15460, lr = 0.01
I0318 08:49:25.968358  3501 solver.cpp:228] Iteration 15480, loss = 51.4095
I0318 08:49:25.968425  3501 solver.cpp:244]     Train net output #0: loss = 51.4094 (* 1 = 51.4094 loss)
I0318 08:49:25.968438  3501 sgd_solver.cpp:106] Iteration 15480, lr = 0.01
I0318 08:49:32.940524  3501 solver.cpp:337] Iteration 15500, Testing net (#0)
I0318 08:51:27.149564  3501 solver.cpp:404]     Test net output #0: loss = 626.672 (* 1 = 626.672 loss)
I0318 08:51:27.149729  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 08:51:27.487606  3501 solver.cpp:228] Iteration 15500, loss = 26.5729
I0318 08:51:27.487670  3501 solver.cpp:244]     Train net output #0: loss = 26.5728 (* 1 = 26.5728 loss)
I0318 08:51:27.487684  3501 sgd_solver.cpp:106] Iteration 15500, lr = 0.01
I0318 08:51:34.743392  3501 solver.cpp:228] Iteration 15520, loss = 39.8964
I0318 08:51:34.743459  3501 solver.cpp:244]     Train net output #0: loss = 39.8963 (* 1 = 39.8963 loss)
I0318 08:51:34.743472  3501 sgd_solver.cpp:106] Iteration 15520, lr = 0.01
I0318 08:51:42.045706  3501 solver.cpp:228] Iteration 15540, loss = 29.2648
I0318 08:51:42.045773  3501 solver.cpp:244]     Train net output #0: loss = 29.2647 (* 1 = 29.2647 loss)
I0318 08:51:42.045788  3501 sgd_solver.cpp:106] Iteration 15540, lr = 0.01
I0318 08:51:49.365802  3501 solver.cpp:228] Iteration 15560, loss = 19.2612
I0318 08:51:49.365866  3501 solver.cpp:244]     Train net output #0: loss = 19.2611 (* 1 = 19.2611 loss)
I0318 08:51:49.365880  3501 sgd_solver.cpp:106] Iteration 15560, lr = 0.01
I0318 08:51:56.683895  3501 solver.cpp:228] Iteration 15580, loss = 59.7636
I0318 08:51:56.683960  3501 solver.cpp:244]     Train net output #0: loss = 59.7635 (* 1 = 59.7635 loss)
I0318 08:51:56.683974  3501 sgd_solver.cpp:106] Iteration 15580, lr = 0.01
I0318 08:52:04.016613  3501 solver.cpp:228] Iteration 15600, loss = 39.7948
I0318 08:52:04.016765  3501 solver.cpp:244]     Train net output #0: loss = 39.7947 (* 1 = 39.7947 loss)
I0318 08:52:04.016780  3501 sgd_solver.cpp:106] Iteration 15600, lr = 0.01
I0318 08:52:11.345448  3501 solver.cpp:228] Iteration 15620, loss = 73.7057
I0318 08:52:11.345515  3501 solver.cpp:244]     Train net output #0: loss = 73.7056 (* 1 = 73.7056 loss)
I0318 08:52:11.345527  3501 sgd_solver.cpp:106] Iteration 15620, lr = 0.01
I0318 08:52:18.689280  3501 solver.cpp:228] Iteration 15640, loss = 90.7377
I0318 08:52:18.689354  3501 solver.cpp:244]     Train net output #0: loss = 90.7376 (* 1 = 90.7376 loss)
I0318 08:52:18.689369  3501 sgd_solver.cpp:106] Iteration 15640, lr = 0.01
I0318 08:52:26.035058  3501 solver.cpp:228] Iteration 15660, loss = 34.859
I0318 08:52:26.035126  3501 solver.cpp:244]     Train net output #0: loss = 34.8589 (* 1 = 34.8589 loss)
I0318 08:52:26.035140  3501 sgd_solver.cpp:106] Iteration 15660, lr = 0.01
I0318 08:52:33.372731  3501 solver.cpp:228] Iteration 15680, loss = 31.0456
I0318 08:52:33.372795  3501 solver.cpp:244]     Train net output #0: loss = 31.0455 (* 1 = 31.0455 loss)
I0318 08:52:33.372809  3501 sgd_solver.cpp:106] Iteration 15680, lr = 0.01
I0318 08:52:40.711935  3501 solver.cpp:228] Iteration 15700, loss = 25.3095
I0318 08:52:40.712064  3501 solver.cpp:244]     Train net output #0: loss = 25.3094 (* 1 = 25.3094 loss)
I0318 08:52:40.712080  3501 sgd_solver.cpp:106] Iteration 15700, lr = 0.01
I0318 08:52:48.043720  3501 solver.cpp:228] Iteration 15720, loss = 35.5418
I0318 08:52:48.043786  3501 solver.cpp:244]     Train net output #0: loss = 35.5416 (* 1 = 35.5416 loss)
I0318 08:52:48.043802  3501 sgd_solver.cpp:106] Iteration 15720, lr = 0.01
I0318 08:52:55.379783  3501 solver.cpp:228] Iteration 15740, loss = 34.5638
I0318 08:52:55.379849  3501 solver.cpp:244]     Train net output #0: loss = 34.5637 (* 1 = 34.5637 loss)
I0318 08:52:55.379863  3501 sgd_solver.cpp:106] Iteration 15740, lr = 0.01
I0318 08:52:58.682227  3501 solver.cpp:337] Iteration 15750, Testing net (#0)
I0318 08:54:52.913380  3501 solver.cpp:404]     Test net output #0: loss = 620.179 (* 1 = 620.179 loss)
I0318 08:54:52.913494  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:54:56.866327  3501 solver.cpp:228] Iteration 15760, loss = 66.0306
I0318 08:54:56.866390  3501 solver.cpp:244]     Train net output #0: loss = 66.0305 (* 1 = 66.0305 loss)
I0318 08:54:56.866405  3501 sgd_solver.cpp:106] Iteration 15760, lr = 0.01
I0318 08:55:04.149679  3501 solver.cpp:228] Iteration 15780, loss = 26.9891
I0318 08:55:04.149747  3501 solver.cpp:244]     Train net output #0: loss = 26.989 (* 1 = 26.989 loss)
I0318 08:55:04.149761  3501 sgd_solver.cpp:106] Iteration 15780, lr = 0.01
I0318 08:55:11.456331  3501 solver.cpp:228] Iteration 15800, loss = 59.5997
I0318 08:55:11.456398  3501 solver.cpp:244]     Train net output #0: loss = 59.5995 (* 1 = 59.5995 loss)
I0318 08:55:11.456413  3501 sgd_solver.cpp:106] Iteration 15800, lr = 0.01
I0318 08:55:18.772173  3501 solver.cpp:228] Iteration 15820, loss = 60.8816
I0318 08:55:18.772250  3501 solver.cpp:244]     Train net output #0: loss = 60.8815 (* 1 = 60.8815 loss)
I0318 08:55:18.772265  3501 sgd_solver.cpp:106] Iteration 15820, lr = 0.01
I0318 08:55:26.107199  3501 solver.cpp:228] Iteration 15840, loss = 28.8196
I0318 08:55:26.107395  3501 solver.cpp:244]     Train net output #0: loss = 28.8195 (* 1 = 28.8195 loss)
I0318 08:55:26.107411  3501 sgd_solver.cpp:106] Iteration 15840, lr = 0.01
I0318 08:55:33.437515  3501 solver.cpp:228] Iteration 15860, loss = 43.5893
I0318 08:55:33.437583  3501 solver.cpp:244]     Train net output #0: loss = 43.5891 (* 1 = 43.5891 loss)
I0318 08:55:33.437598  3501 sgd_solver.cpp:106] Iteration 15860, lr = 0.01
I0318 08:55:40.768023  3501 solver.cpp:228] Iteration 15880, loss = 22.3175
I0318 08:55:40.768092  3501 solver.cpp:244]     Train net output #0: loss = 22.3174 (* 1 = 22.3174 loss)
I0318 08:55:40.768106  3501 sgd_solver.cpp:106] Iteration 15880, lr = 0.01
I0318 08:55:48.101449  3501 solver.cpp:228] Iteration 15900, loss = 30.9438
I0318 08:55:48.101512  3501 solver.cpp:244]     Train net output #0: loss = 30.9437 (* 1 = 30.9437 loss)
I0318 08:55:48.101526  3501 sgd_solver.cpp:106] Iteration 15900, lr = 0.01
I0318 08:55:55.429289  3501 solver.cpp:228] Iteration 15920, loss = 34.9216
I0318 08:55:55.429361  3501 solver.cpp:244]     Train net output #0: loss = 34.9214 (* 1 = 34.9214 loss)
I0318 08:55:55.429375  3501 sgd_solver.cpp:106] Iteration 15920, lr = 0.01
I0318 08:56:02.757230  3501 solver.cpp:228] Iteration 15940, loss = 45.4057
I0318 08:56:02.757388  3501 solver.cpp:244]     Train net output #0: loss = 45.4056 (* 1 = 45.4056 loss)
I0318 08:56:02.757405  3501 sgd_solver.cpp:106] Iteration 15940, lr = 0.01
I0318 08:56:10.092736  3501 solver.cpp:228] Iteration 15960, loss = 43.475
I0318 08:56:10.092802  3501 solver.cpp:244]     Train net output #0: loss = 43.4749 (* 1 = 43.4749 loss)
I0318 08:56:10.092815  3501 sgd_solver.cpp:106] Iteration 15960, lr = 0.01
I0318 08:56:17.433573  3501 solver.cpp:228] Iteration 15980, loss = 34.2318
I0318 08:56:17.433639  3501 solver.cpp:244]     Train net output #0: loss = 34.2317 (* 1 = 34.2317 loss)
I0318 08:56:17.433651  3501 sgd_solver.cpp:106] Iteration 15980, lr = 0.01
I0318 08:56:24.399101  3501 solver.cpp:337] Iteration 16000, Testing net (#0)
I0318 08:58:18.592774  3501 solver.cpp:404]     Test net output #0: loss = 646.605 (* 1 = 646.605 loss)
I0318 08:58:18.592855  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 08:58:18.928841  3501 solver.cpp:228] Iteration 16000, loss = 84.8819
I0318 08:58:18.928908  3501 solver.cpp:244]     Train net output #0: loss = 84.8818 (* 1 = 84.8818 loss)
I0318 08:58:18.928922  3501 sgd_solver.cpp:106] Iteration 16000, lr = 0.01
I0318 08:58:26.204495  3501 solver.cpp:228] Iteration 16020, loss = 53.03
I0318 08:58:26.204560  3501 solver.cpp:244]     Train net output #0: loss = 53.0298 (* 1 = 53.0298 loss)
I0318 08:58:26.204574  3501 sgd_solver.cpp:106] Iteration 16020, lr = 0.01
I0318 08:58:33.516703  3501 solver.cpp:228] Iteration 16040, loss = 44.1205
I0318 08:58:33.516775  3501 solver.cpp:244]     Train net output #0: loss = 44.1203 (* 1 = 44.1203 loss)
I0318 08:58:33.516790  3501 sgd_solver.cpp:106] Iteration 16040, lr = 0.01
I0318 08:58:40.839046  3501 solver.cpp:228] Iteration 16060, loss = 27.622
I0318 08:58:40.839112  3501 solver.cpp:244]     Train net output #0: loss = 27.6219 (* 1 = 27.6219 loss)
I0318 08:58:40.839125  3501 sgd_solver.cpp:106] Iteration 16060, lr = 0.01
I0318 08:58:48.163069  3501 solver.cpp:228] Iteration 16080, loss = 32.3749
I0318 08:58:48.163130  3501 solver.cpp:244]     Train net output #0: loss = 32.3748 (* 1 = 32.3748 loss)
I0318 08:58:48.163144  3501 sgd_solver.cpp:106] Iteration 16080, lr = 0.01
I0318 08:58:55.502279  3501 solver.cpp:228] Iteration 16100, loss = 45.4785
I0318 08:58:55.502444  3501 solver.cpp:244]     Train net output #0: loss = 45.4783 (* 1 = 45.4783 loss)
I0318 08:58:55.502460  3501 sgd_solver.cpp:106] Iteration 16100, lr = 0.01
I0318 08:59:02.840883  3501 solver.cpp:228] Iteration 16120, loss = 27.4024
I0318 08:59:02.840951  3501 solver.cpp:244]     Train net output #0: loss = 27.4022 (* 1 = 27.4022 loss)
I0318 08:59:02.840965  3501 sgd_solver.cpp:106] Iteration 16120, lr = 0.01
I0318 08:59:10.181826  3501 solver.cpp:228] Iteration 16140, loss = 40.8562
I0318 08:59:10.181892  3501 solver.cpp:244]     Train net output #0: loss = 40.856 (* 1 = 40.856 loss)
I0318 08:59:10.181906  3501 sgd_solver.cpp:106] Iteration 16140, lr = 0.01
I0318 08:59:17.528291  3501 solver.cpp:228] Iteration 16160, loss = 34.1438
I0318 08:59:17.528373  3501 solver.cpp:244]     Train net output #0: loss = 34.1437 (* 1 = 34.1437 loss)
I0318 08:59:17.528386  3501 sgd_solver.cpp:106] Iteration 16160, lr = 0.01
I0318 08:59:24.879428  3501 solver.cpp:228] Iteration 16180, loss = 90.0884
I0318 08:59:24.879492  3501 solver.cpp:244]     Train net output #0: loss = 90.0882 (* 1 = 90.0882 loss)
I0318 08:59:24.879505  3501 sgd_solver.cpp:106] Iteration 16180, lr = 0.01
I0318 08:59:32.214707  3501 solver.cpp:228] Iteration 16200, loss = 68.0863
I0318 08:59:32.214854  3501 solver.cpp:244]     Train net output #0: loss = 68.0861 (* 1 = 68.0861 loss)
I0318 08:59:32.214870  3501 sgd_solver.cpp:106] Iteration 16200, lr = 0.01
I0318 08:59:39.545547  3501 solver.cpp:228] Iteration 16220, loss = 49.0992
I0318 08:59:39.545610  3501 solver.cpp:244]     Train net output #0: loss = 49.0991 (* 1 = 49.0991 loss)
I0318 08:59:39.545629  3501 sgd_solver.cpp:106] Iteration 16220, lr = 0.01
I0318 08:59:46.886410  3501 solver.cpp:228] Iteration 16240, loss = 18.366
I0318 08:59:46.886476  3501 solver.cpp:244]     Train net output #0: loss = 18.3658 (* 1 = 18.3658 loss)
I0318 08:59:46.886489  3501 sgd_solver.cpp:106] Iteration 16240, lr = 0.01
I0318 08:59:50.191366  3501 solver.cpp:337] Iteration 16250, Testing net (#0)
I0318 09:01:44.451856  3501 solver.cpp:404]     Test net output #0: loss = 631.647 (* 1 = 631.647 loss)
I0318 09:01:44.451973  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:01:48.406846  3501 solver.cpp:228] Iteration 16260, loss = 24.4123
I0318 09:01:48.406905  3501 solver.cpp:244]     Train net output #0: loss = 24.4122 (* 1 = 24.4122 loss)
I0318 09:01:48.406919  3501 sgd_solver.cpp:106] Iteration 16260, lr = 0.01
I0318 09:01:55.685009  3501 solver.cpp:228] Iteration 16280, loss = 45.9576
I0318 09:01:55.685075  3501 solver.cpp:244]     Train net output #0: loss = 45.9575 (* 1 = 45.9575 loss)
I0318 09:01:55.685091  3501 sgd_solver.cpp:106] Iteration 16280, lr = 0.01
I0318 09:02:02.990411  3501 solver.cpp:228] Iteration 16300, loss = 53.6922
I0318 09:02:02.990479  3501 solver.cpp:244]     Train net output #0: loss = 53.6921 (* 1 = 53.6921 loss)
I0318 09:02:02.990494  3501 sgd_solver.cpp:106] Iteration 16300, lr = 0.01
I0318 09:02:10.309203  3501 solver.cpp:228] Iteration 16320, loss = 64.003
I0318 09:02:10.309268  3501 solver.cpp:244]     Train net output #0: loss = 64.0028 (* 1 = 64.0028 loss)
I0318 09:02:10.309283  3501 sgd_solver.cpp:106] Iteration 16320, lr = 0.01
I0318 09:02:17.630146  3501 solver.cpp:228] Iteration 16340, loss = 38.0912
I0318 09:02:17.630265  3501 solver.cpp:244]     Train net output #0: loss = 38.0911 (* 1 = 38.0911 loss)
I0318 09:02:17.630280  3501 sgd_solver.cpp:106] Iteration 16340, lr = 0.01
I0318 09:02:24.962927  3501 solver.cpp:228] Iteration 16360, loss = 84.7287
I0318 09:02:24.962985  3501 solver.cpp:244]     Train net output #0: loss = 84.7286 (* 1 = 84.7286 loss)
I0318 09:02:24.962999  3501 sgd_solver.cpp:106] Iteration 16360, lr = 0.01
I0318 09:02:32.307718  3501 solver.cpp:228] Iteration 16380, loss = 37.9039
I0318 09:02:32.307776  3501 solver.cpp:244]     Train net output #0: loss = 37.9038 (* 1 = 37.9038 loss)
I0318 09:02:32.307790  3501 sgd_solver.cpp:106] Iteration 16380, lr = 0.01
I0318 09:02:39.645690  3501 solver.cpp:228] Iteration 16400, loss = 28.6961
I0318 09:02:39.645750  3501 solver.cpp:244]     Train net output #0: loss = 28.6959 (* 1 = 28.6959 loss)
I0318 09:02:39.645764  3501 sgd_solver.cpp:106] Iteration 16400, lr = 0.01
I0318 09:02:46.988991  3501 solver.cpp:228] Iteration 16420, loss = 7.92984
I0318 09:02:46.989056  3501 solver.cpp:244]     Train net output #0: loss = 7.92967 (* 1 = 7.92967 loss)
I0318 09:02:46.989069  3501 sgd_solver.cpp:106] Iteration 16420, lr = 0.01
I0318 09:02:54.326058  3501 solver.cpp:228] Iteration 16440, loss = 23.7283
I0318 09:02:54.326256  3501 solver.cpp:244]     Train net output #0: loss = 23.7282 (* 1 = 23.7282 loss)
I0318 09:02:54.326270  3501 sgd_solver.cpp:106] Iteration 16440, lr = 0.01
I0318 09:03:01.666236  3501 solver.cpp:228] Iteration 16460, loss = 33.7696
I0318 09:03:01.666311  3501 solver.cpp:244]     Train net output #0: loss = 33.7695 (* 1 = 33.7695 loss)
I0318 09:03:01.666326  3501 sgd_solver.cpp:106] Iteration 16460, lr = 0.01
I0318 09:03:08.999444  3501 solver.cpp:228] Iteration 16480, loss = 53.3435
I0318 09:03:08.999508  3501 solver.cpp:244]     Train net output #0: loss = 53.3433 (* 1 = 53.3433 loss)
I0318 09:03:08.999522  3501 sgd_solver.cpp:106] Iteration 16480, lr = 0.01
I0318 09:03:15.970423  3501 solver.cpp:337] Iteration 16500, Testing net (#0)
I0318 09:05:10.165503  3501 solver.cpp:404]     Test net output #0: loss = 640.14 (* 1 = 640.14 loss)
I0318 09:05:10.165642  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 09:05:10.501747  3501 solver.cpp:228] Iteration 16500, loss = 47.0858
I0318 09:05:10.501811  3501 solver.cpp:244]     Train net output #0: loss = 47.0857 (* 1 = 47.0857 loss)
I0318 09:05:10.501826  3501 sgd_solver.cpp:106] Iteration 16500, lr = 0.01
I0318 09:05:17.733220  3501 solver.cpp:228] Iteration 16520, loss = 55.5415
I0318 09:05:17.733286  3501 solver.cpp:244]     Train net output #0: loss = 55.5414 (* 1 = 55.5414 loss)
I0318 09:05:17.733300  3501 sgd_solver.cpp:106] Iteration 16520, lr = 0.01
I0318 09:05:25.025486  3501 solver.cpp:228] Iteration 16540, loss = 51.3973
I0318 09:05:25.025554  3501 solver.cpp:244]     Train net output #0: loss = 51.3971 (* 1 = 51.3971 loss)
I0318 09:05:25.025568  3501 sgd_solver.cpp:106] Iteration 16540, lr = 0.01
I0318 09:05:32.334134  3501 solver.cpp:228] Iteration 16560, loss = 27.2594
I0318 09:05:32.334211  3501 solver.cpp:244]     Train net output #0: loss = 27.2593 (* 1 = 27.2593 loss)
I0318 09:05:32.334226  3501 sgd_solver.cpp:106] Iteration 16560, lr = 0.01
I0318 09:05:39.653640  3501 solver.cpp:228] Iteration 16580, loss = 35.3995
I0318 09:05:39.653705  3501 solver.cpp:244]     Train net output #0: loss = 35.3994 (* 1 = 35.3994 loss)
I0318 09:05:39.653719  3501 sgd_solver.cpp:106] Iteration 16580, lr = 0.01
I0318 09:05:46.983108  3501 solver.cpp:228] Iteration 16600, loss = 26.8539
I0318 09:05:46.983244  3501 solver.cpp:244]     Train net output #0: loss = 26.8538 (* 1 = 26.8538 loss)
I0318 09:05:46.983259  3501 sgd_solver.cpp:106] Iteration 16600, lr = 0.01
I0318 09:05:54.315979  3501 solver.cpp:228] Iteration 16620, loss = 21.175
I0318 09:05:54.316056  3501 solver.cpp:244]     Train net output #0: loss = 21.1748 (* 1 = 21.1748 loss)
I0318 09:05:54.316071  3501 sgd_solver.cpp:106] Iteration 16620, lr = 0.01
I0318 09:06:01.653585  3501 solver.cpp:228] Iteration 16640, loss = 50.5391
I0318 09:06:01.653659  3501 solver.cpp:244]     Train net output #0: loss = 50.5389 (* 1 = 50.5389 loss)
I0318 09:06:01.653674  3501 sgd_solver.cpp:106] Iteration 16640, lr = 0.01
I0318 09:06:08.989548  3501 solver.cpp:228] Iteration 16660, loss = 30.5945
I0318 09:06:08.989614  3501 solver.cpp:244]     Train net output #0: loss = 30.5943 (* 1 = 30.5943 loss)
I0318 09:06:08.989629  3501 sgd_solver.cpp:106] Iteration 16660, lr = 0.01
I0318 09:06:16.323717  3501 solver.cpp:228] Iteration 16680, loss = 47.3315
I0318 09:06:16.323787  3501 solver.cpp:244]     Train net output #0: loss = 47.3313 (* 1 = 47.3313 loss)
I0318 09:06:16.323802  3501 sgd_solver.cpp:106] Iteration 16680, lr = 0.01
I0318 09:06:23.665733  3501 solver.cpp:228] Iteration 16700, loss = 51.3789
I0318 09:06:23.665915  3501 solver.cpp:244]     Train net output #0: loss = 51.3788 (* 1 = 51.3788 loss)
I0318 09:06:23.665930  3501 sgd_solver.cpp:106] Iteration 16700, lr = 0.01
I0318 09:06:31.003396  3501 solver.cpp:228] Iteration 16720, loss = 40.5195
I0318 09:06:31.003470  3501 solver.cpp:244]     Train net output #0: loss = 40.5194 (* 1 = 40.5194 loss)
I0318 09:06:31.003484  3501 sgd_solver.cpp:106] Iteration 16720, lr = 0.01
I0318 09:06:38.340939  3501 solver.cpp:228] Iteration 16740, loss = 57.4867
I0318 09:06:38.341004  3501 solver.cpp:244]     Train net output #0: loss = 57.4865 (* 1 = 57.4865 loss)
I0318 09:06:38.341018  3501 sgd_solver.cpp:106] Iteration 16740, lr = 0.01
I0318 09:06:41.643651  3501 solver.cpp:337] Iteration 16750, Testing net (#0)
I0318 09:08:35.846586  3501 solver.cpp:404]     Test net output #0: loss = 610.604 (* 1 = 610.604 loss)
I0318 09:08:35.846714  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 09:08:39.803302  3501 solver.cpp:228] Iteration 16760, loss = 59.207
I0318 09:08:39.803366  3501 solver.cpp:244]     Train net output #0: loss = 59.2069 (* 1 = 59.2069 loss)
I0318 09:08:39.803380  3501 sgd_solver.cpp:106] Iteration 16760, lr = 0.01
I0318 09:08:47.097898  3501 solver.cpp:228] Iteration 16780, loss = 17.0475
I0318 09:08:47.097970  3501 solver.cpp:244]     Train net output #0: loss = 17.0473 (* 1 = 17.0473 loss)
I0318 09:08:47.097985  3501 sgd_solver.cpp:106] Iteration 16780, lr = 0.01
I0318 09:08:54.421212  3501 solver.cpp:228] Iteration 16800, loss = 38.0239
I0318 09:08:54.421280  3501 solver.cpp:244]     Train net output #0: loss = 38.0238 (* 1 = 38.0238 loss)
I0318 09:08:54.421294  3501 sgd_solver.cpp:106] Iteration 16800, lr = 0.01
I0318 09:09:01.752588  3501 solver.cpp:228] Iteration 16820, loss = 31.5302
I0318 09:09:01.752671  3501 solver.cpp:244]     Train net output #0: loss = 31.53 (* 1 = 31.53 loss)
I0318 09:09:01.752687  3501 sgd_solver.cpp:106] Iteration 16820, lr = 0.01
I0318 09:09:09.091289  3501 solver.cpp:228] Iteration 16840, loss = 30.7087
I0318 09:09:09.091408  3501 solver.cpp:244]     Train net output #0: loss = 30.7085 (* 1 = 30.7085 loss)
I0318 09:09:09.091421  3501 sgd_solver.cpp:106] Iteration 16840, lr = 0.01
I0318 09:09:16.426888  3501 solver.cpp:228] Iteration 16860, loss = 62.9515
I0318 09:09:16.426954  3501 solver.cpp:244]     Train net output #0: loss = 62.9513 (* 1 = 62.9513 loss)
I0318 09:09:16.426967  3501 sgd_solver.cpp:106] Iteration 16860, lr = 0.01
I0318 09:09:23.760447  3501 solver.cpp:228] Iteration 16880, loss = 71.4287
I0318 09:09:23.760515  3501 solver.cpp:244]     Train net output #0: loss = 71.4286 (* 1 = 71.4286 loss)
I0318 09:09:23.760529  3501 sgd_solver.cpp:106] Iteration 16880, lr = 0.01
I0318 09:09:31.089607  3501 solver.cpp:228] Iteration 16900, loss = 53.6342
I0318 09:09:31.089675  3501 solver.cpp:244]     Train net output #0: loss = 53.634 (* 1 = 53.634 loss)
I0318 09:09:31.089689  3501 sgd_solver.cpp:106] Iteration 16900, lr = 0.01
I0318 09:09:38.423791  3501 solver.cpp:228] Iteration 16920, loss = 60.4406
I0318 09:09:38.423857  3501 solver.cpp:244]     Train net output #0: loss = 60.4404 (* 1 = 60.4404 loss)
I0318 09:09:38.423871  3501 sgd_solver.cpp:106] Iteration 16920, lr = 0.01
I0318 09:09:45.759618  3501 solver.cpp:228] Iteration 16940, loss = 45.4892
I0318 09:09:45.759760  3501 solver.cpp:244]     Train net output #0: loss = 45.4891 (* 1 = 45.4891 loss)
I0318 09:09:45.759775  3501 sgd_solver.cpp:106] Iteration 16940, lr = 0.01
I0318 09:09:53.092377  3501 solver.cpp:228] Iteration 16960, loss = 6.37235
I0318 09:09:53.092437  3501 solver.cpp:244]     Train net output #0: loss = 6.3722 (* 1 = 6.3722 loss)
I0318 09:09:53.092450  3501 sgd_solver.cpp:106] Iteration 16960, lr = 0.01
I0318 09:10:00.424365  3501 solver.cpp:228] Iteration 16980, loss = 42.8693
I0318 09:10:00.424437  3501 solver.cpp:244]     Train net output #0: loss = 42.8691 (* 1 = 42.8691 loss)
I0318 09:10:00.424453  3501 sgd_solver.cpp:106] Iteration 16980, lr = 0.01
I0318 09:10:07.390420  3501 solver.cpp:337] Iteration 17000, Testing net (#0)
I0318 09:12:01.583793  3501 solver.cpp:404]     Test net output #0: loss = 651.625 (* 1 = 651.625 loss)
I0318 09:12:01.583945  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:12:01.920166  3501 solver.cpp:228] Iteration 17000, loss = 23.2271
I0318 09:12:01.920233  3501 solver.cpp:244]     Train net output #0: loss = 23.2269 (* 1 = 23.2269 loss)
I0318 09:12:01.920248  3501 sgd_solver.cpp:106] Iteration 17000, lr = 0.01
I0318 09:12:09.159404  3501 solver.cpp:228] Iteration 17020, loss = 35.1868
I0318 09:12:09.159476  3501 solver.cpp:244]     Train net output #0: loss = 35.1866 (* 1 = 35.1866 loss)
I0318 09:12:09.159490  3501 sgd_solver.cpp:106] Iteration 17020, lr = 0.01
I0318 09:12:16.467983  3501 solver.cpp:228] Iteration 17040, loss = 54.1402
I0318 09:12:16.468050  3501 solver.cpp:244]     Train net output #0: loss = 54.14 (* 1 = 54.14 loss)
I0318 09:12:16.468062  3501 sgd_solver.cpp:106] Iteration 17040, lr = 0.01
I0318 09:12:23.799968  3501 solver.cpp:228] Iteration 17060, loss = 45.8931
I0318 09:12:23.800038  3501 solver.cpp:244]     Train net output #0: loss = 45.893 (* 1 = 45.893 loss)
I0318 09:12:23.800053  3501 sgd_solver.cpp:106] Iteration 17060, lr = 0.01
I0318 09:12:31.138533  3501 solver.cpp:228] Iteration 17080, loss = 81.633
I0318 09:12:31.138602  3501 solver.cpp:244]     Train net output #0: loss = 81.6328 (* 1 = 81.6328 loss)
I0318 09:12:31.138617  3501 sgd_solver.cpp:106] Iteration 17080, lr = 0.01
I0318 09:12:38.480722  3501 solver.cpp:228] Iteration 17100, loss = 41.5333
I0318 09:12:38.480865  3501 solver.cpp:244]     Train net output #0: loss = 41.5332 (* 1 = 41.5332 loss)
I0318 09:12:38.480880  3501 sgd_solver.cpp:106] Iteration 17100, lr = 0.01
I0318 09:12:45.820732  3501 solver.cpp:228] Iteration 17120, loss = 28.092
I0318 09:12:45.820801  3501 solver.cpp:244]     Train net output #0: loss = 28.0919 (* 1 = 28.0919 loss)
I0318 09:12:45.820813  3501 sgd_solver.cpp:106] Iteration 17120, lr = 0.01
I0318 09:12:53.152940  3501 solver.cpp:228] Iteration 17140, loss = 20.9137
I0318 09:12:53.153002  3501 solver.cpp:244]     Train net output #0: loss = 20.9135 (* 1 = 20.9135 loss)
I0318 09:12:53.153015  3501 sgd_solver.cpp:106] Iteration 17140, lr = 0.01
I0318 09:13:00.493182  3501 solver.cpp:228] Iteration 17160, loss = 20.1484
I0318 09:13:00.493249  3501 solver.cpp:244]     Train net output #0: loss = 20.1482 (* 1 = 20.1482 loss)
I0318 09:13:00.493265  3501 sgd_solver.cpp:106] Iteration 17160, lr = 0.01
I0318 09:13:07.845820  3501 solver.cpp:228] Iteration 17180, loss = 27.813
I0318 09:13:07.845890  3501 solver.cpp:244]     Train net output #0: loss = 27.8129 (* 1 = 27.8129 loss)
I0318 09:13:07.845903  3501 sgd_solver.cpp:106] Iteration 17180, lr = 0.01
I0318 09:13:15.188769  3501 solver.cpp:228] Iteration 17200, loss = 27.0281
I0318 09:13:15.188904  3501 solver.cpp:244]     Train net output #0: loss = 27.0279 (* 1 = 27.0279 loss)
I0318 09:13:15.188918  3501 sgd_solver.cpp:106] Iteration 17200, lr = 0.01
I0318 09:13:22.524287  3501 solver.cpp:228] Iteration 17220, loss = 42.3217
I0318 09:13:22.524365  3501 solver.cpp:244]     Train net output #0: loss = 42.3216 (* 1 = 42.3216 loss)
I0318 09:13:22.524379  3501 sgd_solver.cpp:106] Iteration 17220, lr = 0.01
I0318 09:13:29.862462  3501 solver.cpp:228] Iteration 17240, loss = 47.4009
I0318 09:13:29.862529  3501 solver.cpp:244]     Train net output #0: loss = 47.4007 (* 1 = 47.4007 loss)
I0318 09:13:29.862543  3501 sgd_solver.cpp:106] Iteration 17240, lr = 0.01
I0318 09:13:33.165693  3501 solver.cpp:337] Iteration 17250, Testing net (#0)
I0318 09:15:27.367344  3501 solver.cpp:404]     Test net output #0: loss = 635.177 (* 1 = 635.177 loss)
I0318 09:15:27.367506  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 09:15:31.324281  3501 solver.cpp:228] Iteration 17260, loss = 48.6554
I0318 09:15:31.324358  3501 solver.cpp:244]     Train net output #0: loss = 48.6553 (* 1 = 48.6553 loss)
I0318 09:15:31.324371  3501 sgd_solver.cpp:106] Iteration 17260, lr = 0.01
I0318 09:15:38.606828  3501 solver.cpp:228] Iteration 17280, loss = 28.9846
I0318 09:15:38.606896  3501 solver.cpp:244]     Train net output #0: loss = 28.9845 (* 1 = 28.9845 loss)
I0318 09:15:38.606909  3501 sgd_solver.cpp:106] Iteration 17280, lr = 0.01
I0318 09:15:45.915379  3501 solver.cpp:228] Iteration 17300, loss = 32.7945
I0318 09:15:45.915443  3501 solver.cpp:244]     Train net output #0: loss = 32.7944 (* 1 = 32.7944 loss)
I0318 09:15:45.915457  3501 sgd_solver.cpp:106] Iteration 17300, lr = 0.01
I0318 09:15:53.243427  3501 solver.cpp:228] Iteration 17320, loss = 33.2759
I0318 09:15:53.243489  3501 solver.cpp:244]     Train net output #0: loss = 33.2758 (* 1 = 33.2758 loss)
I0318 09:15:53.243504  3501 sgd_solver.cpp:106] Iteration 17320, lr = 0.01
I0318 09:16:00.574373  3501 solver.cpp:228] Iteration 17340, loss = 26.6781
I0318 09:16:00.574532  3501 solver.cpp:244]     Train net output #0: loss = 26.678 (* 1 = 26.678 loss)
I0318 09:16:00.574548  3501 sgd_solver.cpp:106] Iteration 17340, lr = 0.01
I0318 09:16:07.908965  3501 solver.cpp:228] Iteration 17360, loss = 34.0427
I0318 09:16:07.909034  3501 solver.cpp:244]     Train net output #0: loss = 34.0426 (* 1 = 34.0426 loss)
I0318 09:16:07.909047  3501 sgd_solver.cpp:106] Iteration 17360, lr = 0.01
I0318 09:16:15.246809  3501 solver.cpp:228] Iteration 17380, loss = 28.8258
I0318 09:16:15.246873  3501 solver.cpp:244]     Train net output #0: loss = 28.8256 (* 1 = 28.8256 loss)
I0318 09:16:15.246887  3501 sgd_solver.cpp:106] Iteration 17380, lr = 0.01
I0318 09:16:22.581156  3501 solver.cpp:228] Iteration 17400, loss = 38.8639
I0318 09:16:22.581231  3501 solver.cpp:244]     Train net output #0: loss = 38.8638 (* 1 = 38.8638 loss)
I0318 09:16:22.581248  3501 sgd_solver.cpp:106] Iteration 17400, lr = 0.01
I0318 09:16:29.925681  3501 solver.cpp:228] Iteration 17420, loss = 50.4622
I0318 09:16:29.925755  3501 solver.cpp:244]     Train net output #0: loss = 50.462 (* 1 = 50.462 loss)
I0318 09:16:29.925770  3501 sgd_solver.cpp:106] Iteration 17420, lr = 0.01
I0318 09:16:37.258358  3501 solver.cpp:228] Iteration 17440, loss = 49.1011
I0318 09:16:37.258496  3501 solver.cpp:244]     Train net output #0: loss = 49.1009 (* 1 = 49.1009 loss)
I0318 09:16:37.258510  3501 sgd_solver.cpp:106] Iteration 17440, lr = 0.01
I0318 09:16:44.578724  3501 solver.cpp:228] Iteration 17460, loss = 34.8249
I0318 09:16:44.578793  3501 solver.cpp:244]     Train net output #0: loss = 34.8248 (* 1 = 34.8248 loss)
I0318 09:16:44.578805  3501 sgd_solver.cpp:106] Iteration 17460, lr = 0.01
I0318 09:16:51.913059  3501 solver.cpp:228] Iteration 17480, loss = 31.0682
I0318 09:16:51.913128  3501 solver.cpp:244]     Train net output #0: loss = 31.068 (* 1 = 31.068 loss)
I0318 09:16:51.913142  3501 sgd_solver.cpp:106] Iteration 17480, lr = 0.01
I0318 09:16:58.879993  3501 solver.cpp:337] Iteration 17500, Testing net (#0)
I0318 09:18:53.075191  3501 solver.cpp:404]     Test net output #0: loss = 643.997 (* 1 = 643.997 loss)
I0318 09:18:53.075263  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 09:18:53.412775  3501 solver.cpp:228] Iteration 17500, loss = 22.3343
I0318 09:18:53.412839  3501 solver.cpp:244]     Train net output #0: loss = 22.3342 (* 1 = 22.3342 loss)
I0318 09:18:53.412853  3501 sgd_solver.cpp:106] Iteration 17500, lr = 0.01
I0318 09:19:00.646153  3501 solver.cpp:228] Iteration 17520, loss = 35.6074
I0318 09:19:00.646220  3501 solver.cpp:244]     Train net output #0: loss = 35.6073 (* 1 = 35.6073 loss)
I0318 09:19:00.646234  3501 sgd_solver.cpp:106] Iteration 17520, lr = 0.01
I0318 09:19:07.940830  3501 solver.cpp:228] Iteration 17540, loss = 21.9329
I0318 09:19:07.940896  3501 solver.cpp:244]     Train net output #0: loss = 21.9327 (* 1 = 21.9327 loss)
I0318 09:19:07.940909  3501 sgd_solver.cpp:106] Iteration 17540, lr = 0.01
I0318 09:19:15.256945  3501 solver.cpp:228] Iteration 17560, loss = 81.7137
I0318 09:19:15.257011  3501 solver.cpp:244]     Train net output #0: loss = 81.7135 (* 1 = 81.7135 loss)
I0318 09:19:15.257025  3501 sgd_solver.cpp:106] Iteration 17560, lr = 0.01
I0318 09:19:22.589560  3501 solver.cpp:228] Iteration 17580, loss = 27.1006
I0318 09:19:22.589629  3501 solver.cpp:244]     Train net output #0: loss = 27.1005 (* 1 = 27.1005 loss)
I0318 09:19:22.589644  3501 sgd_solver.cpp:106] Iteration 17580, lr = 0.01
I0318 09:19:29.920661  3501 solver.cpp:228] Iteration 17600, loss = 60.572
I0318 09:19:29.920845  3501 solver.cpp:244]     Train net output #0: loss = 60.5718 (* 1 = 60.5718 loss)
I0318 09:19:29.920859  3501 sgd_solver.cpp:106] Iteration 17600, lr = 0.01
I0318 09:19:37.258342  3501 solver.cpp:228] Iteration 17620, loss = 50.0484
I0318 09:19:37.258406  3501 solver.cpp:244]     Train net output #0: loss = 50.0483 (* 1 = 50.0483 loss)
I0318 09:19:37.258420  3501 sgd_solver.cpp:106] Iteration 17620, lr = 0.01
I0318 09:19:44.598220  3501 solver.cpp:228] Iteration 17640, loss = 50.883
I0318 09:19:44.598289  3501 solver.cpp:244]     Train net output #0: loss = 50.8829 (* 1 = 50.8829 loss)
I0318 09:19:44.598304  3501 sgd_solver.cpp:106] Iteration 17640, lr = 0.01
I0318 09:19:51.939121  3501 solver.cpp:228] Iteration 17660, loss = 33.2183
I0318 09:19:51.939191  3501 solver.cpp:244]     Train net output #0: loss = 33.2182 (* 1 = 33.2182 loss)
I0318 09:19:51.939206  3501 sgd_solver.cpp:106] Iteration 17660, lr = 0.01
I0318 09:19:59.281903  3501 solver.cpp:228] Iteration 17680, loss = 8.85489
I0318 09:19:59.281968  3501 solver.cpp:244]     Train net output #0: loss = 8.85475 (* 1 = 8.85475 loss)
I0318 09:19:59.281981  3501 sgd_solver.cpp:106] Iteration 17680, lr = 0.01
I0318 09:20:06.626068  3501 solver.cpp:228] Iteration 17700, loss = 19.1467
I0318 09:20:06.626224  3501 solver.cpp:244]     Train net output #0: loss = 19.1465 (* 1 = 19.1465 loss)
I0318 09:20:06.626238  3501 sgd_solver.cpp:106] Iteration 17700, lr = 0.01
I0318 09:20:13.961668  3501 solver.cpp:228] Iteration 17720, loss = 17.4708
I0318 09:20:13.961731  3501 solver.cpp:244]     Train net output #0: loss = 17.4706 (* 1 = 17.4706 loss)
I0318 09:20:13.961745  3501 sgd_solver.cpp:106] Iteration 17720, lr = 0.01
I0318 09:20:21.297411  3501 solver.cpp:228] Iteration 17740, loss = 31.5844
I0318 09:20:21.297477  3501 solver.cpp:244]     Train net output #0: loss = 31.5842 (* 1 = 31.5842 loss)
I0318 09:20:21.297492  3501 sgd_solver.cpp:106] Iteration 17740, lr = 0.01
I0318 09:20:24.602941  3501 solver.cpp:337] Iteration 17750, Testing net (#0)
I0318 09:22:18.841339  3501 solver.cpp:404]     Test net output #0: loss = 616.672 (* 1 = 616.672 loss)
I0318 09:22:18.841418  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 09:22:22.799521  3501 solver.cpp:228] Iteration 17760, loss = 33.994
I0318 09:22:22.799589  3501 solver.cpp:244]     Train net output #0: loss = 33.9938 (* 1 = 33.9938 loss)
I0318 09:22:22.799603  3501 sgd_solver.cpp:106] Iteration 17760, lr = 0.01
I0318 09:22:30.083593  3501 solver.cpp:228] Iteration 17780, loss = 50.6396
I0318 09:22:30.083659  3501 solver.cpp:244]     Train net output #0: loss = 50.6395 (* 1 = 50.6395 loss)
I0318 09:22:30.083673  3501 sgd_solver.cpp:106] Iteration 17780, lr = 0.01
I0318 09:22:37.405767  3501 solver.cpp:228] Iteration 17800, loss = 60.4131
I0318 09:22:37.405833  3501 solver.cpp:244]     Train net output #0: loss = 60.4129 (* 1 = 60.4129 loss)
I0318 09:22:37.405848  3501 sgd_solver.cpp:106] Iteration 17800, lr = 0.01
I0318 09:22:44.738662  3501 solver.cpp:228] Iteration 17820, loss = 59.9009
I0318 09:22:44.738728  3501 solver.cpp:244]     Train net output #0: loss = 59.9008 (* 1 = 59.9008 loss)
I0318 09:22:44.738741  3501 sgd_solver.cpp:106] Iteration 17820, lr = 0.01
I0318 09:22:52.071864  3501 solver.cpp:228] Iteration 17840, loss = 30.7131
I0318 09:22:52.072043  3501 solver.cpp:244]     Train net output #0: loss = 30.713 (* 1 = 30.713 loss)
I0318 09:22:52.072058  3501 sgd_solver.cpp:106] Iteration 17840, lr = 0.01
I0318 09:22:59.417685  3501 solver.cpp:228] Iteration 17860, loss = 44.0069
I0318 09:22:59.417752  3501 solver.cpp:244]     Train net output #0: loss = 44.0067 (* 1 = 44.0067 loss)
I0318 09:22:59.417767  3501 sgd_solver.cpp:106] Iteration 17860, lr = 0.01
I0318 09:23:06.759171  3501 solver.cpp:228] Iteration 17880, loss = 13.1047
I0318 09:23:06.759238  3501 solver.cpp:244]     Train net output #0: loss = 13.1045 (* 1 = 13.1045 loss)
I0318 09:23:06.759253  3501 sgd_solver.cpp:106] Iteration 17880, lr = 0.01
I0318 09:23:14.110904  3501 solver.cpp:228] Iteration 17900, loss = 35.2719
I0318 09:23:14.110972  3501 solver.cpp:244]     Train net output #0: loss = 35.2717 (* 1 = 35.2717 loss)
I0318 09:23:14.110986  3501 sgd_solver.cpp:106] Iteration 17900, lr = 0.01
I0318 09:23:21.458258  3501 solver.cpp:228] Iteration 17920, loss = 43.7253
I0318 09:23:21.458323  3501 solver.cpp:244]     Train net output #0: loss = 43.7252 (* 1 = 43.7252 loss)
I0318 09:23:21.458338  3501 sgd_solver.cpp:106] Iteration 17920, lr = 0.01
I0318 09:23:28.808765  3501 solver.cpp:228] Iteration 17940, loss = 49.3312
I0318 09:23:28.808894  3501 solver.cpp:244]     Train net output #0: loss = 49.3311 (* 1 = 49.3311 loss)
I0318 09:23:28.808909  3501 sgd_solver.cpp:106] Iteration 17940, lr = 0.01
I0318 09:23:36.156486  3501 solver.cpp:228] Iteration 17960, loss = 45.2987
I0318 09:23:36.156554  3501 solver.cpp:244]     Train net output #0: loss = 45.2986 (* 1 = 45.2986 loss)
I0318 09:23:36.156568  3501 sgd_solver.cpp:106] Iteration 17960, lr = 0.01
I0318 09:23:43.504142  3501 solver.cpp:228] Iteration 17980, loss = 72.1407
I0318 09:23:43.504209  3501 solver.cpp:244]     Train net output #0: loss = 72.1406 (* 1 = 72.1406 loss)
I0318 09:23:43.504222  3501 sgd_solver.cpp:106] Iteration 17980, lr = 0.01
I0318 09:23:50.475591  3501 solver.cpp:337] Iteration 18000, Testing net (#0)
I0318 09:25:44.656198  3501 solver.cpp:404]     Test net output #0: loss = 660.987 (* 1 = 660.987 loss)
I0318 09:25:44.656317  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:25:44.992254  3501 solver.cpp:228] Iteration 18000, loss = 60.1304
I0318 09:25:44.992331  3501 solver.cpp:244]     Train net output #0: loss = 60.1302 (* 1 = 60.1302 loss)
I0318 09:25:44.992344  3501 sgd_solver.cpp:106] Iteration 18000, lr = 0.01
I0318 09:25:52.248495  3501 solver.cpp:228] Iteration 18020, loss = 51.3256
I0318 09:25:52.248560  3501 solver.cpp:244]     Train net output #0: loss = 51.3255 (* 1 = 51.3255 loss)
I0318 09:25:52.248574  3501 sgd_solver.cpp:106] Iteration 18020, lr = 0.01
I0318 09:25:59.559159  3501 solver.cpp:228] Iteration 18040, loss = 23.5523
I0318 09:25:59.559226  3501 solver.cpp:244]     Train net output #0: loss = 23.5522 (* 1 = 23.5522 loss)
I0318 09:25:59.559238  3501 sgd_solver.cpp:106] Iteration 18040, lr = 0.01
I0318 09:26:06.884997  3501 solver.cpp:228] Iteration 18060, loss = 33.9072
I0318 09:26:06.885067  3501 solver.cpp:244]     Train net output #0: loss = 33.9071 (* 1 = 33.9071 loss)
I0318 09:26:06.885082  3501 sgd_solver.cpp:106] Iteration 18060, lr = 0.01
I0318 09:26:14.222167  3501 solver.cpp:228] Iteration 18080, loss = 32.9438
I0318 09:26:14.222232  3501 solver.cpp:244]     Train net output #0: loss = 32.9436 (* 1 = 32.9436 loss)
I0318 09:26:14.222246  3501 sgd_solver.cpp:106] Iteration 18080, lr = 0.01
I0318 09:26:21.566617  3501 solver.cpp:228] Iteration 18100, loss = 73.2181
I0318 09:26:21.566716  3501 solver.cpp:244]     Train net output #0: loss = 73.2179 (* 1 = 73.2179 loss)
I0318 09:26:21.566731  3501 sgd_solver.cpp:106] Iteration 18100, lr = 0.01
I0318 09:26:28.904433  3501 solver.cpp:228] Iteration 18120, loss = 47.4486
I0318 09:26:28.904503  3501 solver.cpp:244]     Train net output #0: loss = 47.4485 (* 1 = 47.4485 loss)
I0318 09:26:28.904517  3501 sgd_solver.cpp:106] Iteration 18120, lr = 0.01
I0318 09:26:36.245626  3501 solver.cpp:228] Iteration 18140, loss = 30.1758
I0318 09:26:36.245693  3501 solver.cpp:244]     Train net output #0: loss = 30.1756 (* 1 = 30.1756 loss)
I0318 09:26:36.245707  3501 sgd_solver.cpp:106] Iteration 18140, lr = 0.01
I0318 09:26:43.580530  3501 solver.cpp:228] Iteration 18160, loss = 45.7301
I0318 09:26:43.580597  3501 solver.cpp:244]     Train net output #0: loss = 45.73 (* 1 = 45.73 loss)
I0318 09:26:43.580610  3501 sgd_solver.cpp:106] Iteration 18160, lr = 0.01
I0318 09:26:50.920702  3501 solver.cpp:228] Iteration 18180, loss = 38.2372
I0318 09:26:50.920778  3501 solver.cpp:244]     Train net output #0: loss = 38.2371 (* 1 = 38.2371 loss)
I0318 09:26:50.920794  3501 sgd_solver.cpp:106] Iteration 18180, lr = 0.01
I0318 09:26:58.253336  3501 solver.cpp:228] Iteration 18200, loss = 52.9645
I0318 09:26:58.253547  3501 solver.cpp:244]     Train net output #0: loss = 52.9644 (* 1 = 52.9644 loss)
I0318 09:26:58.253564  3501 sgd_solver.cpp:106] Iteration 18200, lr = 0.01
I0318 09:27:05.593505  3501 solver.cpp:228] Iteration 18220, loss = 29.9295
I0318 09:27:05.593571  3501 solver.cpp:244]     Train net output #0: loss = 29.9294 (* 1 = 29.9294 loss)
I0318 09:27:05.593585  3501 sgd_solver.cpp:106] Iteration 18220, lr = 0.01
I0318 09:27:12.929747  3501 solver.cpp:228] Iteration 18240, loss = 31.1847
I0318 09:27:12.929812  3501 solver.cpp:244]     Train net output #0: loss = 31.1846 (* 1 = 31.1846 loss)
I0318 09:27:12.929826  3501 sgd_solver.cpp:106] Iteration 18240, lr = 0.01
I0318 09:27:16.228729  3501 solver.cpp:337] Iteration 18250, Testing net (#0)
I0318 09:29:10.447628  3501 solver.cpp:404]     Test net output #0: loss = 606.491 (* 1 = 606.491 loss)
I0318 09:29:10.447757  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:29:14.409487  3501 solver.cpp:228] Iteration 18260, loss = 23.0292
I0318 09:29:14.409553  3501 solver.cpp:244]     Train net output #0: loss = 23.029 (* 1 = 23.029 loss)
I0318 09:29:14.409566  3501 sgd_solver.cpp:106] Iteration 18260, lr = 0.01
I0318 09:29:21.699748  3501 solver.cpp:228] Iteration 18280, loss = 46.2391
I0318 09:29:21.699820  3501 solver.cpp:244]     Train net output #0: loss = 46.239 (* 1 = 46.239 loss)
I0318 09:29:21.699833  3501 sgd_solver.cpp:106] Iteration 18280, lr = 0.01
I0318 09:29:29.009413  3501 solver.cpp:228] Iteration 18300, loss = 71.1805
I0318 09:29:29.009479  3501 solver.cpp:244]     Train net output #0: loss = 71.1803 (* 1 = 71.1803 loss)
I0318 09:29:29.009493  3501 sgd_solver.cpp:106] Iteration 18300, lr = 0.01
I0318 09:29:36.335415  3501 solver.cpp:228] Iteration 18320, loss = 40.958
I0318 09:29:36.335481  3501 solver.cpp:244]     Train net output #0: loss = 40.9578 (* 1 = 40.9578 loss)
I0318 09:29:36.335495  3501 sgd_solver.cpp:106] Iteration 18320, lr = 0.01
I0318 09:29:43.659626  3501 solver.cpp:228] Iteration 18340, loss = 52.4214
I0318 09:29:43.659775  3501 solver.cpp:244]     Train net output #0: loss = 52.4212 (* 1 = 52.4212 loss)
I0318 09:29:43.659790  3501 sgd_solver.cpp:106] Iteration 18340, lr = 0.01
I0318 09:29:50.992244  3501 solver.cpp:228] Iteration 18360, loss = 46.9245
I0318 09:29:50.992322  3501 solver.cpp:244]     Train net output #0: loss = 46.9244 (* 1 = 46.9244 loss)
I0318 09:29:50.992337  3501 sgd_solver.cpp:106] Iteration 18360, lr = 0.01
I0318 09:29:58.327438  3501 solver.cpp:228] Iteration 18380, loss = 70.2763
I0318 09:29:58.327504  3501 solver.cpp:244]     Train net output #0: loss = 70.2762 (* 1 = 70.2762 loss)
I0318 09:29:58.327522  3501 sgd_solver.cpp:106] Iteration 18380, lr = 0.01
I0318 09:30:05.665583  3501 solver.cpp:228] Iteration 18400, loss = 34.0948
I0318 09:30:05.665652  3501 solver.cpp:244]     Train net output #0: loss = 34.0946 (* 1 = 34.0946 loss)
I0318 09:30:05.665665  3501 sgd_solver.cpp:106] Iteration 18400, lr = 0.01
I0318 09:30:13.009016  3501 solver.cpp:228] Iteration 18420, loss = 20.8123
I0318 09:30:13.009084  3501 solver.cpp:244]     Train net output #0: loss = 20.8121 (* 1 = 20.8121 loss)
I0318 09:30:13.009099  3501 sgd_solver.cpp:106] Iteration 18420, lr = 0.01
I0318 09:30:20.355525  3501 solver.cpp:228] Iteration 18440, loss = 19.4599
I0318 09:30:20.355706  3501 solver.cpp:244]     Train net output #0: loss = 19.4597 (* 1 = 19.4597 loss)
I0318 09:30:20.355721  3501 sgd_solver.cpp:106] Iteration 18440, lr = 0.01
I0318 09:30:27.698745  3501 solver.cpp:228] Iteration 18460, loss = 31.2174
I0318 09:30:27.698817  3501 solver.cpp:244]     Train net output #0: loss = 31.2172 (* 1 = 31.2172 loss)
I0318 09:30:27.698829  3501 sgd_solver.cpp:106] Iteration 18460, lr = 0.01
I0318 09:30:35.033612  3501 solver.cpp:228] Iteration 18480, loss = 44.0271
I0318 09:30:35.033680  3501 solver.cpp:244]     Train net output #0: loss = 44.027 (* 1 = 44.027 loss)
I0318 09:30:35.033694  3501 sgd_solver.cpp:106] Iteration 18480, lr = 0.01
I0318 09:30:41.996508  3501 solver.cpp:337] Iteration 18500, Testing net (#0)
I0318 09:32:36.206920  3501 solver.cpp:404]     Test net output #0: loss = 591.214 (* 1 = 591.214 loss)
I0318 09:32:36.207041  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903919 (* 1 = 0.903919 loss)
I0318 09:32:36.543730  3501 solver.cpp:228] Iteration 18500, loss = 51.8068
I0318 09:32:36.543794  3501 solver.cpp:244]     Train net output #0: loss = 51.8066 (* 1 = 51.8066 loss)
I0318 09:32:36.543807  3501 sgd_solver.cpp:106] Iteration 18500, lr = 0.01
I0318 09:32:43.789183  3501 solver.cpp:228] Iteration 18520, loss = 64.0688
I0318 09:32:43.789250  3501 solver.cpp:244]     Train net output #0: loss = 64.0686 (* 1 = 64.0686 loss)
I0318 09:32:43.789264  3501 sgd_solver.cpp:106] Iteration 18520, lr = 0.01
I0318 09:32:51.088871  3501 solver.cpp:228] Iteration 18540, loss = 54.1568
I0318 09:32:51.088937  3501 solver.cpp:244]     Train net output #0: loss = 54.1567 (* 1 = 54.1567 loss)
I0318 09:32:51.088951  3501 sgd_solver.cpp:106] Iteration 18540, lr = 0.01
I0318 09:32:58.406951  3501 solver.cpp:228] Iteration 18560, loss = 32.5974
I0318 09:32:58.407017  3501 solver.cpp:244]     Train net output #0: loss = 32.5972 (* 1 = 32.5972 loss)
I0318 09:32:58.407032  3501 sgd_solver.cpp:106] Iteration 18560, lr = 0.01
I0318 09:33:05.730969  3501 solver.cpp:228] Iteration 18580, loss = 58.5688
I0318 09:33:05.731036  3501 solver.cpp:244]     Train net output #0: loss = 58.5686 (* 1 = 58.5686 loss)
I0318 09:33:05.731051  3501 sgd_solver.cpp:106] Iteration 18580, lr = 0.01
I0318 09:33:13.060420  3501 solver.cpp:228] Iteration 18600, loss = 35.9443
I0318 09:33:13.060550  3501 solver.cpp:244]     Train net output #0: loss = 35.9442 (* 1 = 35.9442 loss)
I0318 09:33:13.060564  3501 sgd_solver.cpp:106] Iteration 18600, lr = 0.01
I0318 09:33:20.398146  3501 solver.cpp:228] Iteration 18620, loss = 43.8872
I0318 09:33:20.398208  3501 solver.cpp:244]     Train net output #0: loss = 43.8871 (* 1 = 43.8871 loss)
I0318 09:33:20.398221  3501 sgd_solver.cpp:106] Iteration 18620, lr = 0.01
I0318 09:33:27.738268  3501 solver.cpp:228] Iteration 18640, loss = 31.935
I0318 09:33:27.738335  3501 solver.cpp:244]     Train net output #0: loss = 31.9348 (* 1 = 31.9348 loss)
I0318 09:33:27.738349  3501 sgd_solver.cpp:106] Iteration 18640, lr = 0.01
I0318 09:33:35.078630  3501 solver.cpp:228] Iteration 18660, loss = 25.2362
I0318 09:33:35.078698  3501 solver.cpp:244]     Train net output #0: loss = 25.2361 (* 1 = 25.2361 loss)
I0318 09:33:35.078712  3501 sgd_solver.cpp:106] Iteration 18660, lr = 0.01
I0318 09:33:42.419859  3501 solver.cpp:228] Iteration 18680, loss = 29.2154
I0318 09:33:42.419925  3501 solver.cpp:244]     Train net output #0: loss = 29.2152 (* 1 = 29.2152 loss)
I0318 09:33:42.419939  3501 sgd_solver.cpp:106] Iteration 18680, lr = 0.01
I0318 09:33:49.755969  3501 solver.cpp:228] Iteration 18700, loss = 43.5829
I0318 09:33:49.756115  3501 solver.cpp:244]     Train net output #0: loss = 43.5827 (* 1 = 43.5827 loss)
I0318 09:33:49.756129  3501 sgd_solver.cpp:106] Iteration 18700, lr = 0.01
I0318 09:33:57.092651  3501 solver.cpp:228] Iteration 18720, loss = 59.8793
I0318 09:33:57.092717  3501 solver.cpp:244]     Train net output #0: loss = 59.8792 (* 1 = 59.8792 loss)
I0318 09:33:57.092731  3501 sgd_solver.cpp:106] Iteration 18720, lr = 0.01
I0318 09:34:04.427747  3501 solver.cpp:228] Iteration 18740, loss = 30.9288
I0318 09:34:04.427814  3501 solver.cpp:244]     Train net output #0: loss = 30.9287 (* 1 = 30.9287 loss)
I0318 09:34:04.427829  3501 sgd_solver.cpp:106] Iteration 18740, lr = 0.01
I0318 09:34:07.727721  3501 solver.cpp:337] Iteration 18750, Testing net (#0)
I0318 09:36:01.933769  3501 solver.cpp:404]     Test net output #0: loss = 677.572 (* 1 = 677.572 loss)
I0318 09:36:01.933933  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 09:36:05.884021  3501 solver.cpp:228] Iteration 18760, loss = 23.9565
I0318 09:36:05.884088  3501 solver.cpp:244]     Train net output #0: loss = 23.9563 (* 1 = 23.9563 loss)
I0318 09:36:05.884100  3501 sgd_solver.cpp:106] Iteration 18760, lr = 0.01
I0318 09:36:13.150009  3501 solver.cpp:228] Iteration 18780, loss = 13.5918
I0318 09:36:13.150071  3501 solver.cpp:244]     Train net output #0: loss = 13.5916 (* 1 = 13.5916 loss)
I0318 09:36:13.150084  3501 sgd_solver.cpp:106] Iteration 18780, lr = 0.01
I0318 09:36:20.451308  3501 solver.cpp:228] Iteration 18800, loss = 25.9312
I0318 09:36:20.451369  3501 solver.cpp:244]     Train net output #0: loss = 25.9311 (* 1 = 25.9311 loss)
I0318 09:36:20.451382  3501 sgd_solver.cpp:106] Iteration 18800, lr = 0.01
I0318 09:36:27.777559  3501 solver.cpp:228] Iteration 18820, loss = 59.0955
I0318 09:36:27.777626  3501 solver.cpp:244]     Train net output #0: loss = 59.0953 (* 1 = 59.0953 loss)
I0318 09:36:27.777639  3501 sgd_solver.cpp:106] Iteration 18820, lr = 0.01
I0318 09:36:35.113814  3501 solver.cpp:228] Iteration 18840, loss = 58.8488
I0318 09:36:35.113967  3501 solver.cpp:244]     Train net output #0: loss = 58.8486 (* 1 = 58.8486 loss)
I0318 09:36:35.113982  3501 sgd_solver.cpp:106] Iteration 18840, lr = 0.01
I0318 09:36:42.454339  3501 solver.cpp:228] Iteration 18860, loss = 33.2723
I0318 09:36:42.454403  3501 solver.cpp:244]     Train net output #0: loss = 33.2722 (* 1 = 33.2722 loss)
I0318 09:36:42.454417  3501 sgd_solver.cpp:106] Iteration 18860, lr = 0.01
I0318 09:36:49.783043  3501 solver.cpp:228] Iteration 18880, loss = 59.1392
I0318 09:36:49.783109  3501 solver.cpp:244]     Train net output #0: loss = 59.1391 (* 1 = 59.1391 loss)
I0318 09:36:49.783123  3501 sgd_solver.cpp:106] Iteration 18880, lr = 0.01
I0318 09:36:57.109935  3501 solver.cpp:228] Iteration 18900, loss = 62.7533
I0318 09:36:57.110002  3501 solver.cpp:244]     Train net output #0: loss = 62.7532 (* 1 = 62.7532 loss)
I0318 09:36:57.110018  3501 sgd_solver.cpp:106] Iteration 18900, lr = 0.01
I0318 09:37:04.445013  3501 solver.cpp:228] Iteration 18920, loss = 46.0709
I0318 09:37:04.445080  3501 solver.cpp:244]     Train net output #0: loss = 46.0707 (* 1 = 46.0707 loss)
I0318 09:37:04.445093  3501 sgd_solver.cpp:106] Iteration 18920, lr = 0.01
I0318 09:37:11.782898  3501 solver.cpp:228] Iteration 18940, loss = 26.9698
I0318 09:37:11.783046  3501 solver.cpp:244]     Train net output #0: loss = 26.9697 (* 1 = 26.9697 loss)
I0318 09:37:11.783061  3501 sgd_solver.cpp:106] Iteration 18940, lr = 0.01
I0318 09:37:19.119120  3501 solver.cpp:228] Iteration 18960, loss = 25.1166
I0318 09:37:19.119194  3501 solver.cpp:244]     Train net output #0: loss = 25.1164 (* 1 = 25.1164 loss)
I0318 09:37:19.119209  3501 sgd_solver.cpp:106] Iteration 18960, lr = 0.01
I0318 09:37:26.464238  3501 solver.cpp:228] Iteration 18980, loss = 41.4572
I0318 09:37:26.464323  3501 solver.cpp:244]     Train net output #0: loss = 41.457 (* 1 = 41.457 loss)
I0318 09:37:26.464339  3501 sgd_solver.cpp:106] Iteration 18980, lr = 0.01
I0318 09:37:33.437177  3501 solver.cpp:337] Iteration 19000, Testing net (#0)
I0318 09:39:27.654520  3501 solver.cpp:404]     Test net output #0: loss = 630.769 (* 1 = 630.769 loss)
I0318 09:39:27.654633  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:39:27.992056  3501 solver.cpp:228] Iteration 19000, loss = 45.576
I0318 09:39:27.992121  3501 solver.cpp:244]     Train net output #0: loss = 45.5759 (* 1 = 45.5759 loss)
I0318 09:39:27.992136  3501 sgd_solver.cpp:106] Iteration 19000, lr = 0.01
I0318 09:39:35.240867  3501 solver.cpp:228] Iteration 19020, loss = 37.668
I0318 09:39:35.240933  3501 solver.cpp:244]     Train net output #0: loss = 37.6678 (* 1 = 37.6678 loss)
I0318 09:39:35.240947  3501 sgd_solver.cpp:106] Iteration 19020, lr = 0.01
I0318 09:39:42.537547  3501 solver.cpp:228] Iteration 19040, loss = 54.3993
I0318 09:39:42.537614  3501 solver.cpp:244]     Train net output #0: loss = 54.3992 (* 1 = 54.3992 loss)
I0318 09:39:42.537628  3501 sgd_solver.cpp:106] Iteration 19040, lr = 0.01
I0318 09:39:49.860805  3501 solver.cpp:228] Iteration 19060, loss = 61.795
I0318 09:39:49.860874  3501 solver.cpp:244]     Train net output #0: loss = 61.7948 (* 1 = 61.7948 loss)
I0318 09:39:49.860888  3501 sgd_solver.cpp:106] Iteration 19060, lr = 0.01
I0318 09:39:57.191220  3501 solver.cpp:228] Iteration 19080, loss = 32.6844
I0318 09:39:57.191285  3501 solver.cpp:244]     Train net output #0: loss = 32.6842 (* 1 = 32.6842 loss)
I0318 09:39:57.191300  3501 sgd_solver.cpp:106] Iteration 19080, lr = 0.01
I0318 09:40:04.518589  3501 solver.cpp:228] Iteration 19100, loss = 29.5062
I0318 09:40:04.518731  3501 solver.cpp:244]     Train net output #0: loss = 29.506 (* 1 = 29.506 loss)
I0318 09:40:04.518746  3501 sgd_solver.cpp:106] Iteration 19100, lr = 0.01
I0318 09:40:11.856585  3501 solver.cpp:228] Iteration 19120, loss = 9.42258
I0318 09:40:11.856649  3501 solver.cpp:244]     Train net output #0: loss = 9.42243 (* 1 = 9.42243 loss)
I0318 09:40:11.856663  3501 sgd_solver.cpp:106] Iteration 19120, lr = 0.01
I0318 09:40:19.193147  3501 solver.cpp:228] Iteration 19140, loss = 45.463
I0318 09:40:19.193213  3501 solver.cpp:244]     Train net output #0: loss = 45.4629 (* 1 = 45.4629 loss)
I0318 09:40:19.193228  3501 sgd_solver.cpp:106] Iteration 19140, lr = 0.01
I0318 09:40:26.534111  3501 solver.cpp:228] Iteration 19160, loss = 23.8911
I0318 09:40:26.534179  3501 solver.cpp:244]     Train net output #0: loss = 23.891 (* 1 = 23.891 loss)
I0318 09:40:26.534193  3501 sgd_solver.cpp:106] Iteration 19160, lr = 0.01
I0318 09:40:33.879843  3501 solver.cpp:228] Iteration 19180, loss = 35.9596
I0318 09:40:33.879917  3501 solver.cpp:244]     Train net output #0: loss = 35.9594 (* 1 = 35.9594 loss)
I0318 09:40:33.879932  3501 sgd_solver.cpp:106] Iteration 19180, lr = 0.01
I0318 09:40:41.223117  3501 solver.cpp:228] Iteration 19200, loss = 39.0443
I0318 09:40:41.223254  3501 solver.cpp:244]     Train net output #0: loss = 39.0441 (* 1 = 39.0441 loss)
I0318 09:40:41.223268  3501 sgd_solver.cpp:106] Iteration 19200, lr = 0.01
I0318 09:40:48.558423  3501 solver.cpp:228] Iteration 19220, loss = 33.7137
I0318 09:40:48.558490  3501 solver.cpp:244]     Train net output #0: loss = 33.7136 (* 1 = 33.7136 loss)
I0318 09:40:48.558503  3501 sgd_solver.cpp:106] Iteration 19220, lr = 0.01
I0318 09:40:55.897120  3501 solver.cpp:228] Iteration 19240, loss = 61.8763
I0318 09:40:55.897187  3501 solver.cpp:244]     Train net output #0: loss = 61.8761 (* 1 = 61.8761 loss)
I0318 09:40:55.897202  3501 sgd_solver.cpp:106] Iteration 19240, lr = 0.01
I0318 09:40:59.202769  3501 solver.cpp:337] Iteration 19250, Testing net (#0)
I0318 09:42:53.422646  3501 solver.cpp:404]     Test net output #0: loss = 622.544 (* 1 = 622.544 loss)
I0318 09:42:53.422765  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:42:57.383128  3501 solver.cpp:228] Iteration 19260, loss = 41.2747
I0318 09:42:57.383196  3501 solver.cpp:244]     Train net output #0: loss = 41.2745 (* 1 = 41.2745 loss)
I0318 09:42:57.383209  3501 sgd_solver.cpp:106] Iteration 19260, lr = 0.01
I0318 09:43:04.658756  3501 solver.cpp:228] Iteration 19280, loss = 25.7602
I0318 09:43:04.658824  3501 solver.cpp:244]     Train net output #0: loss = 25.76 (* 1 = 25.76 loss)
I0318 09:43:04.658838  3501 sgd_solver.cpp:106] Iteration 19280, lr = 0.01
I0318 09:43:11.962754  3501 solver.cpp:228] Iteration 19300, loss = 11.2889
I0318 09:43:11.962826  3501 solver.cpp:244]     Train net output #0: loss = 11.2887 (* 1 = 11.2887 loss)
I0318 09:43:11.962841  3501 sgd_solver.cpp:106] Iteration 19300, lr = 0.01
I0318 09:43:19.284072  3501 solver.cpp:228] Iteration 19320, loss = 21.5119
I0318 09:43:19.284138  3501 solver.cpp:244]     Train net output #0: loss = 21.5118 (* 1 = 21.5118 loss)
I0318 09:43:19.284152  3501 sgd_solver.cpp:106] Iteration 19320, lr = 0.01
I0318 09:43:26.614938  3501 solver.cpp:228] Iteration 19340, loss = 34.922
I0318 09:43:26.615126  3501 solver.cpp:244]     Train net output #0: loss = 34.9219 (* 1 = 34.9219 loss)
I0318 09:43:26.615140  3501 sgd_solver.cpp:106] Iteration 19340, lr = 0.01
I0318 09:43:33.945097  3501 solver.cpp:228] Iteration 19360, loss = 28.2037
I0318 09:43:33.945163  3501 solver.cpp:244]     Train net output #0: loss = 28.2035 (* 1 = 28.2035 loss)
I0318 09:43:33.945178  3501 sgd_solver.cpp:106] Iteration 19360, lr = 0.01
I0318 09:43:41.279554  3501 solver.cpp:228] Iteration 19380, loss = 52.1472
I0318 09:43:41.279625  3501 solver.cpp:244]     Train net output #0: loss = 52.147 (* 1 = 52.147 loss)
I0318 09:43:41.279639  3501 sgd_solver.cpp:106] Iteration 19380, lr = 0.01
I0318 09:43:48.614980  3501 solver.cpp:228] Iteration 19400, loss = 70.0241
I0318 09:43:48.615048  3501 solver.cpp:244]     Train net output #0: loss = 70.024 (* 1 = 70.024 loss)
I0318 09:43:48.615062  3501 sgd_solver.cpp:106] Iteration 19400, lr = 0.01
I0318 09:43:55.950127  3501 solver.cpp:228] Iteration 19420, loss = 58.1813
I0318 09:43:55.950194  3501 solver.cpp:244]     Train net output #0: loss = 58.1811 (* 1 = 58.1811 loss)
I0318 09:43:55.950208  3501 sgd_solver.cpp:106] Iteration 19420, lr = 0.01
I0318 09:44:03.289656  3501 solver.cpp:228] Iteration 19440, loss = 43.0884
I0318 09:44:03.289790  3501 solver.cpp:244]     Train net output #0: loss = 43.0883 (* 1 = 43.0883 loss)
I0318 09:44:03.289805  3501 sgd_solver.cpp:106] Iteration 19440, lr = 0.01
I0318 09:44:10.621538  3501 solver.cpp:228] Iteration 19460, loss = 42.0976
I0318 09:44:10.621605  3501 solver.cpp:244]     Train net output #0: loss = 42.0974 (* 1 = 42.0974 loss)
I0318 09:44:10.621619  3501 sgd_solver.cpp:106] Iteration 19460, lr = 0.01
I0318 09:44:17.957393  3501 solver.cpp:228] Iteration 19480, loss = 27.7151
I0318 09:44:17.957459  3501 solver.cpp:244]     Train net output #0: loss = 27.715 (* 1 = 27.715 loss)
I0318 09:44:17.957474  3501 sgd_solver.cpp:106] Iteration 19480, lr = 0.01
I0318 09:44:24.926373  3501 solver.cpp:337] Iteration 19500, Testing net (#0)
I0318 09:46:19.167748  3501 solver.cpp:404]     Test net output #0: loss = 660.622 (* 1 = 660.622 loss)
I0318 09:46:19.167881  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:46:19.505165  3501 solver.cpp:228] Iteration 19500, loss = 27.3023
I0318 09:46:19.505231  3501 solver.cpp:244]     Train net output #0: loss = 27.3022 (* 1 = 27.3022 loss)
I0318 09:46:19.505245  3501 sgd_solver.cpp:106] Iteration 19500, lr = 0.01
I0318 09:46:26.750489  3501 solver.cpp:228] Iteration 19520, loss = 26.9396
I0318 09:46:26.750558  3501 solver.cpp:244]     Train net output #0: loss = 26.9395 (* 1 = 26.9395 loss)
I0318 09:46:26.750572  3501 sgd_solver.cpp:106] Iteration 19520, lr = 0.01
I0318 09:46:34.044811  3501 solver.cpp:228] Iteration 19540, loss = 39.4622
I0318 09:46:34.044878  3501 solver.cpp:244]     Train net output #0: loss = 39.462 (* 1 = 39.462 loss)
I0318 09:46:34.044891  3501 sgd_solver.cpp:106] Iteration 19540, lr = 0.01
I0318 09:46:41.376030  3501 solver.cpp:228] Iteration 19560, loss = 52.1982
I0318 09:46:41.376096  3501 solver.cpp:244]     Train net output #0: loss = 52.198 (* 1 = 52.198 loss)
I0318 09:46:41.376111  3501 sgd_solver.cpp:106] Iteration 19560, lr = 0.01
I0318 09:46:48.710443  3501 solver.cpp:228] Iteration 19580, loss = 34.1096
I0318 09:46:48.710511  3501 solver.cpp:244]     Train net output #0: loss = 34.1094 (* 1 = 34.1094 loss)
I0318 09:46:48.710525  3501 sgd_solver.cpp:106] Iteration 19580, lr = 0.01
I0318 09:46:56.049669  3501 solver.cpp:228] Iteration 19600, loss = 60.1607
I0318 09:46:56.049845  3501 solver.cpp:244]     Train net output #0: loss = 60.1605 (* 1 = 60.1605 loss)
I0318 09:46:56.049861  3501 sgd_solver.cpp:106] Iteration 19600, lr = 0.01
I0318 09:47:03.391659  3501 solver.cpp:228] Iteration 19620, loss = 77.0445
I0318 09:47:03.391726  3501 solver.cpp:244]     Train net output #0: loss = 77.0444 (* 1 = 77.0444 loss)
I0318 09:47:03.391741  3501 sgd_solver.cpp:106] Iteration 19620, lr = 0.01
I0318 09:47:10.728829  3501 solver.cpp:228] Iteration 19640, loss = 40.9672
I0318 09:47:10.728902  3501 solver.cpp:244]     Train net output #0: loss = 40.9671 (* 1 = 40.9671 loss)
I0318 09:47:10.728917  3501 sgd_solver.cpp:106] Iteration 19640, lr = 0.01
I0318 09:47:18.065482  3501 solver.cpp:228] Iteration 19660, loss = 14.8631
I0318 09:47:18.065548  3501 solver.cpp:244]     Train net output #0: loss = 14.863 (* 1 = 14.863 loss)
I0318 09:47:18.065562  3501 sgd_solver.cpp:106] Iteration 19660, lr = 0.01
I0318 09:47:25.411636  3501 solver.cpp:228] Iteration 19680, loss = 37.8386
I0318 09:47:25.411701  3501 solver.cpp:244]     Train net output #0: loss = 37.8385 (* 1 = 37.8385 loss)
I0318 09:47:25.411715  3501 sgd_solver.cpp:106] Iteration 19680, lr = 0.01
I0318 09:47:32.752553  3501 solver.cpp:228] Iteration 19700, loss = 37.7532
I0318 09:47:32.752708  3501 solver.cpp:244]     Train net output #0: loss = 37.753 (* 1 = 37.753 loss)
I0318 09:47:32.752723  3501 sgd_solver.cpp:106] Iteration 19700, lr = 0.01
I0318 09:47:40.088950  3501 solver.cpp:228] Iteration 19720, loss = 45.7434
I0318 09:47:40.089020  3501 solver.cpp:244]     Train net output #0: loss = 45.7433 (* 1 = 45.7433 loss)
I0318 09:47:40.089033  3501 sgd_solver.cpp:106] Iteration 19720, lr = 0.01
I0318 09:47:47.427276  3501 solver.cpp:228] Iteration 19740, loss = 31.8534
I0318 09:47:47.427343  3501 solver.cpp:244]     Train net output #0: loss = 31.8532 (* 1 = 31.8532 loss)
I0318 09:47:47.427357  3501 sgd_solver.cpp:106] Iteration 19740, lr = 0.01
I0318 09:47:50.729749  3501 solver.cpp:337] Iteration 19750, Testing net (#0)
I0318 09:49:44.950099  3501 solver.cpp:404]     Test net output #0: loss = 632.45 (* 1 = 632.45 loss)
I0318 09:49:44.950178  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 09:49:48.916925  3501 solver.cpp:228] Iteration 19760, loss = 63.8908
I0318 09:49:48.916991  3501 solver.cpp:244]     Train net output #0: loss = 63.8907 (* 1 = 63.8907 loss)
I0318 09:49:48.917006  3501 sgd_solver.cpp:106] Iteration 19760, lr = 0.01
I0318 09:49:56.203611  3501 solver.cpp:228] Iteration 19780, loss = 36.4673
I0318 09:49:56.203677  3501 solver.cpp:244]     Train net output #0: loss = 36.4672 (* 1 = 36.4672 loss)
I0318 09:49:56.203691  3501 sgd_solver.cpp:106] Iteration 19780, lr = 0.01
I0318 09:50:03.508343  3501 solver.cpp:228] Iteration 19800, loss = 56.9764
I0318 09:50:03.508414  3501 solver.cpp:244]     Train net output #0: loss = 56.9762 (* 1 = 56.9762 loss)
I0318 09:50:03.508429  3501 sgd_solver.cpp:106] Iteration 19800, lr = 0.01
I0318 09:50:10.822885  3501 solver.cpp:228] Iteration 19820, loss = 58.7464
I0318 09:50:10.822952  3501 solver.cpp:244]     Train net output #0: loss = 58.7462 (* 1 = 58.7462 loss)
I0318 09:50:10.822965  3501 sgd_solver.cpp:106] Iteration 19820, lr = 0.01
I0318 09:50:18.159546  3501 solver.cpp:228] Iteration 19840, loss = 23.7184
I0318 09:50:18.159680  3501 solver.cpp:244]     Train net output #0: loss = 23.7183 (* 1 = 23.7183 loss)
I0318 09:50:18.159695  3501 sgd_solver.cpp:106] Iteration 19840, lr = 0.01
I0318 09:50:25.490383  3501 solver.cpp:228] Iteration 19860, loss = 49.8948
I0318 09:50:25.490449  3501 solver.cpp:244]     Train net output #0: loss = 49.8946 (* 1 = 49.8946 loss)
I0318 09:50:25.490463  3501 sgd_solver.cpp:106] Iteration 19860, lr = 0.01
I0318 09:50:32.825639  3501 solver.cpp:228] Iteration 19880, loss = 25.181
I0318 09:50:32.825705  3501 solver.cpp:244]     Train net output #0: loss = 25.1808 (* 1 = 25.1808 loss)
I0318 09:50:32.825717  3501 sgd_solver.cpp:106] Iteration 19880, lr = 0.01
I0318 09:50:40.160471  3501 solver.cpp:228] Iteration 19900, loss = 19.2732
I0318 09:50:40.160539  3501 solver.cpp:244]     Train net output #0: loss = 19.273 (* 1 = 19.273 loss)
I0318 09:50:40.160553  3501 sgd_solver.cpp:106] Iteration 19900, lr = 0.01
I0318 09:50:47.496716  3501 solver.cpp:228] Iteration 19920, loss = 27.6208
I0318 09:50:47.496783  3501 solver.cpp:244]     Train net output #0: loss = 27.6207 (* 1 = 27.6207 loss)
I0318 09:50:47.496798  3501 sgd_solver.cpp:106] Iteration 19920, lr = 0.01
I0318 09:50:54.822805  3501 solver.cpp:228] Iteration 19940, loss = 78.6663
I0318 09:50:54.822976  3501 solver.cpp:244]     Train net output #0: loss = 78.6662 (* 1 = 78.6662 loss)
I0318 09:50:54.822991  3501 sgd_solver.cpp:106] Iteration 19940, lr = 0.01
I0318 09:51:02.153053  3501 solver.cpp:228] Iteration 19960, loss = 80.6612
I0318 09:51:02.153123  3501 solver.cpp:244]     Train net output #0: loss = 80.661 (* 1 = 80.661 loss)
I0318 09:51:02.153138  3501 sgd_solver.cpp:106] Iteration 19960, lr = 0.01
I0318 09:51:09.484002  3501 solver.cpp:228] Iteration 19980, loss = 46.5278
I0318 09:51:09.484067  3501 solver.cpp:244]     Train net output #0: loss = 46.5276 (* 1 = 46.5276 loss)
I0318 09:51:09.484081  3501 sgd_solver.cpp:106] Iteration 19980, lr = 0.01
I0318 09:51:16.441386  3501 solver.cpp:454] Snapshotting to binary proto file ./caffe_alexnet_train_iter_20000.caffemodel
I0318 09:51:18.071573  3501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./caffe_alexnet_train_iter_20000.solverstate
I0318 09:51:18.478323  3501 solver.cpp:337] Iteration 20000, Testing net (#0)
I0318 09:53:12.656980  3501 solver.cpp:404]     Test net output #0: loss = 586.977 (* 1 = 586.977 loss)
I0318 09:53:12.657075  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 09:53:12.995390  3501 solver.cpp:228] Iteration 20000, loss = 35.6645
I0318 09:53:12.995477  3501 solver.cpp:244]     Train net output #0: loss = 35.6644 (* 1 = 35.6644 loss)
I0318 09:53:12.995493  3501 sgd_solver.cpp:106] Iteration 20000, lr = 0.01
I0318 09:53:20.261534  3501 solver.cpp:228] Iteration 20020, loss = 36.7928
I0318 09:53:20.261602  3501 solver.cpp:244]     Train net output #0: loss = 36.7927 (* 1 = 36.7927 loss)
I0318 09:53:20.261616  3501 sgd_solver.cpp:106] Iteration 20020, lr = 0.01
I0318 09:53:27.568598  3501 solver.cpp:228] Iteration 20040, loss = 21.7729
I0318 09:53:27.568670  3501 solver.cpp:244]     Train net output #0: loss = 21.7727 (* 1 = 21.7727 loss)
I0318 09:53:27.568687  3501 sgd_solver.cpp:106] Iteration 20040, lr = 0.01
I0318 09:53:34.883302  3501 solver.cpp:228] Iteration 20060, loss = 26.5423
I0318 09:53:34.883368  3501 solver.cpp:244]     Train net output #0: loss = 26.5422 (* 1 = 26.5422 loss)
I0318 09:53:34.883383  3501 sgd_solver.cpp:106] Iteration 20060, lr = 0.01
I0318 09:53:42.213732  3501 solver.cpp:228] Iteration 20080, loss = 16.2767
I0318 09:53:42.213796  3501 solver.cpp:244]     Train net output #0: loss = 16.2766 (* 1 = 16.2766 loss)
I0318 09:53:42.213810  3501 sgd_solver.cpp:106] Iteration 20080, lr = 0.01
I0318 09:53:49.552433  3501 solver.cpp:228] Iteration 20100, loss = 38.4153
I0318 09:53:49.552575  3501 solver.cpp:244]     Train net output #0: loss = 38.4151 (* 1 = 38.4151 loss)
I0318 09:53:49.552589  3501 sgd_solver.cpp:106] Iteration 20100, lr = 0.01
I0318 09:53:56.902979  3501 solver.cpp:228] Iteration 20120, loss = 56.1502
I0318 09:53:56.903049  3501 solver.cpp:244]     Train net output #0: loss = 56.1501 (* 1 = 56.1501 loss)
I0318 09:53:56.903064  3501 sgd_solver.cpp:106] Iteration 20120, lr = 0.01
I0318 09:54:04.239346  3501 solver.cpp:228] Iteration 20140, loss = 68.9607
I0318 09:54:04.239413  3501 solver.cpp:244]     Train net output #0: loss = 68.9606 (* 1 = 68.9606 loss)
I0318 09:54:04.239428  3501 sgd_solver.cpp:106] Iteration 20140, lr = 0.01
I0318 09:54:11.578383  3501 solver.cpp:228] Iteration 20160, loss = 66.2047
I0318 09:54:11.578450  3501 solver.cpp:244]     Train net output #0: loss = 66.2046 (* 1 = 66.2046 loss)
I0318 09:54:11.578464  3501 sgd_solver.cpp:106] Iteration 20160, lr = 0.01
I0318 09:54:18.910959  3501 solver.cpp:228] Iteration 20180, loss = 34.2789
I0318 09:54:18.911025  3501 solver.cpp:244]     Train net output #0: loss = 34.2788 (* 1 = 34.2788 loss)
I0318 09:54:18.911037  3501 sgd_solver.cpp:106] Iteration 20180, lr = 0.01
I0318 09:54:26.248039  3501 solver.cpp:228] Iteration 20200, loss = 9.84015
I0318 09:54:26.248224  3501 solver.cpp:244]     Train net output #0: loss = 9.84 (* 1 = 9.84 loss)
I0318 09:54:26.248237  3501 sgd_solver.cpp:106] Iteration 20200, lr = 0.01
I0318 09:54:33.583525  3501 solver.cpp:228] Iteration 20220, loss = 32.0782
I0318 09:54:33.583590  3501 solver.cpp:244]     Train net output #0: loss = 32.0781 (* 1 = 32.0781 loss)
I0318 09:54:33.583605  3501 sgd_solver.cpp:106] Iteration 20220, lr = 0.01
I0318 09:54:40.912397  3501 solver.cpp:228] Iteration 20240, loss = 35.0205
I0318 09:54:40.912461  3501 solver.cpp:244]     Train net output #0: loss = 35.0204 (* 1 = 35.0204 loss)
I0318 09:54:40.912475  3501 sgd_solver.cpp:106] Iteration 20240, lr = 0.01
I0318 09:54:44.213930  3501 solver.cpp:337] Iteration 20250, Testing net (#0)
I0318 09:56:38.414602  3501 solver.cpp:404]     Test net output #0: loss = 615.195 (* 1 = 615.195 loss)
I0318 09:56:38.414736  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 09:56:42.373265  3501 solver.cpp:228] Iteration 20260, loss = 62.799
I0318 09:56:42.373332  3501 solver.cpp:244]     Train net output #0: loss = 62.7989 (* 1 = 62.7989 loss)
I0318 09:56:42.373345  3501 sgd_solver.cpp:106] Iteration 20260, lr = 0.01
I0318 09:56:49.657518  3501 solver.cpp:228] Iteration 20280, loss = 39.35
I0318 09:56:49.657585  3501 solver.cpp:244]     Train net output #0: loss = 39.3498 (* 1 = 39.3498 loss)
I0318 09:56:49.657598  3501 sgd_solver.cpp:106] Iteration 20280, lr = 0.01
I0318 09:56:56.967484  3501 solver.cpp:228] Iteration 20300, loss = 73.4028
I0318 09:56:56.967561  3501 solver.cpp:244]     Train net output #0: loss = 73.4026 (* 1 = 73.4026 loss)
I0318 09:56:56.967576  3501 sgd_solver.cpp:106] Iteration 20300, lr = 0.01
I0318 09:57:04.281792  3501 solver.cpp:228] Iteration 20320, loss = 33.7167
I0318 09:57:04.281857  3501 solver.cpp:244]     Train net output #0: loss = 33.7166 (* 1 = 33.7166 loss)
I0318 09:57:04.281870  3501 sgd_solver.cpp:106] Iteration 20320, lr = 0.01
I0318 09:57:11.601657  3501 solver.cpp:228] Iteration 20340, loss = 34.5988
I0318 09:57:11.601799  3501 solver.cpp:244]     Train net output #0: loss = 34.5987 (* 1 = 34.5987 loss)
I0318 09:57:11.601814  3501 sgd_solver.cpp:106] Iteration 20340, lr = 0.01
I0318 09:57:18.932926  3501 solver.cpp:228] Iteration 20360, loss = 57.0845
I0318 09:57:18.932996  3501 solver.cpp:244]     Train net output #0: loss = 57.0844 (* 1 = 57.0844 loss)
I0318 09:57:18.933010  3501 sgd_solver.cpp:106] Iteration 20360, lr = 0.01
I0318 09:57:26.271344  3501 solver.cpp:228] Iteration 20380, loss = 38.2383
I0318 09:57:26.271409  3501 solver.cpp:244]     Train net output #0: loss = 38.2381 (* 1 = 38.2381 loss)
I0318 09:57:26.271423  3501 sgd_solver.cpp:106] Iteration 20380, lr = 0.01
I0318 09:57:33.606748  3501 solver.cpp:228] Iteration 20400, loss = 26.1667
I0318 09:57:33.606819  3501 solver.cpp:244]     Train net output #0: loss = 26.1666 (* 1 = 26.1666 loss)
I0318 09:57:33.606832  3501 sgd_solver.cpp:106] Iteration 20400, lr = 0.01
I0318 09:57:40.941288  3501 solver.cpp:228] Iteration 20420, loss = 39.095
I0318 09:57:40.941355  3501 solver.cpp:244]     Train net output #0: loss = 39.0948 (* 1 = 39.0948 loss)
I0318 09:57:40.941368  3501 sgd_solver.cpp:106] Iteration 20420, lr = 0.01
I0318 09:57:48.275497  3501 solver.cpp:228] Iteration 20440, loss = 30.0188
I0318 09:57:48.275640  3501 solver.cpp:244]     Train net output #0: loss = 30.0187 (* 1 = 30.0187 loss)
I0318 09:57:48.275656  3501 sgd_solver.cpp:106] Iteration 20440, lr = 0.01
I0318 09:57:55.608244  3501 solver.cpp:228] Iteration 20460, loss = 54.1715
I0318 09:57:55.608321  3501 solver.cpp:244]     Train net output #0: loss = 54.1714 (* 1 = 54.1714 loss)
I0318 09:57:55.608336  3501 sgd_solver.cpp:106] Iteration 20460, lr = 0.01
I0318 09:58:02.943084  3501 solver.cpp:228] Iteration 20480, loss = 41.0941
I0318 09:58:02.943150  3501 solver.cpp:244]     Train net output #0: loss = 41.094 (* 1 = 41.094 loss)
I0318 09:58:02.943163  3501 sgd_solver.cpp:106] Iteration 20480, lr = 0.01
I0318 09:58:09.905922  3501 solver.cpp:337] Iteration 20500, Testing net (#0)
I0318 10:00:04.143244  3501 solver.cpp:404]     Test net output #0: loss = 605.506 (* 1 = 605.506 loss)
I0318 10:00:04.143405  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 10:00:04.480417  3501 solver.cpp:228] Iteration 20500, loss = 51.6311
I0318 10:00:04.480482  3501 solver.cpp:244]     Train net output #0: loss = 51.631 (* 1 = 51.631 loss)
I0318 10:00:04.480496  3501 sgd_solver.cpp:106] Iteration 20500, lr = 0.01
I0318 10:00:11.736611  3501 solver.cpp:228] Iteration 20520, loss = 53.2858
I0318 10:00:11.736680  3501 solver.cpp:244]     Train net output #0: loss = 53.2856 (* 1 = 53.2856 loss)
I0318 10:00:11.736695  3501 sgd_solver.cpp:106] Iteration 20520, lr = 0.01
I0318 10:00:19.035569  3501 solver.cpp:228] Iteration 20540, loss = 31.3597
I0318 10:00:19.035637  3501 solver.cpp:244]     Train net output #0: loss = 31.3595 (* 1 = 31.3595 loss)
I0318 10:00:19.035651  3501 sgd_solver.cpp:106] Iteration 20540, lr = 0.01
I0318 10:00:26.357305  3501 solver.cpp:228] Iteration 20560, loss = 17.622
I0318 10:00:26.357380  3501 solver.cpp:244]     Train net output #0: loss = 17.6218 (* 1 = 17.6218 loss)
I0318 10:00:26.357396  3501 sgd_solver.cpp:106] Iteration 20560, lr = 0.01
I0318 10:00:33.690937  3501 solver.cpp:228] Iteration 20580, loss = 17.601
I0318 10:00:33.691005  3501 solver.cpp:244]     Train net output #0: loss = 17.6008 (* 1 = 17.6008 loss)
I0318 10:00:33.691020  3501 sgd_solver.cpp:106] Iteration 20580, lr = 0.01
I0318 10:00:41.018345  3501 solver.cpp:228] Iteration 20600, loss = 11.5111
I0318 10:00:41.018486  3501 solver.cpp:244]     Train net output #0: loss = 11.511 (* 1 = 11.511 loss)
I0318 10:00:41.018501  3501 sgd_solver.cpp:106] Iteration 20600, lr = 0.01
I0318 10:00:48.342285  3501 solver.cpp:228] Iteration 20620, loss = 59.2854
I0318 10:00:48.342351  3501 solver.cpp:244]     Train net output #0: loss = 59.2853 (* 1 = 59.2853 loss)
I0318 10:00:48.342365  3501 sgd_solver.cpp:106] Iteration 20620, lr = 0.01
I0318 10:00:55.672087  3501 solver.cpp:228] Iteration 20640, loss = 47.0874
I0318 10:00:55.672153  3501 solver.cpp:244]     Train net output #0: loss = 47.0873 (* 1 = 47.0873 loss)
I0318 10:00:55.672166  3501 sgd_solver.cpp:106] Iteration 20640, lr = 0.01
I0318 10:01:03.011968  3501 solver.cpp:228] Iteration 20660, loss = 86.2681
I0318 10:01:03.012032  3501 solver.cpp:244]     Train net output #0: loss = 86.2679 (* 1 = 86.2679 loss)
I0318 10:01:03.012045  3501 sgd_solver.cpp:106] Iteration 20660, lr = 0.01
I0318 10:01:10.352525  3501 solver.cpp:228] Iteration 20680, loss = 67.9801
I0318 10:01:10.352591  3501 solver.cpp:244]     Train net output #0: loss = 67.98 (* 1 = 67.98 loss)
I0318 10:01:10.352605  3501 sgd_solver.cpp:106] Iteration 20680, lr = 0.01
I0318 10:01:17.687885  3501 solver.cpp:228] Iteration 20700, loss = 34.2642
I0318 10:01:17.688032  3501 solver.cpp:244]     Train net output #0: loss = 34.2641 (* 1 = 34.2641 loss)
I0318 10:01:17.688046  3501 sgd_solver.cpp:106] Iteration 20700, lr = 0.01
I0318 10:01:25.019567  3501 solver.cpp:228] Iteration 20720, loss = 47.7091
I0318 10:01:25.019636  3501 solver.cpp:244]     Train net output #0: loss = 47.7089 (* 1 = 47.7089 loss)
I0318 10:01:25.019650  3501 sgd_solver.cpp:106] Iteration 20720, lr = 0.01
I0318 10:01:32.355957  3501 solver.cpp:228] Iteration 20740, loss = 24.4691
I0318 10:01:32.356024  3501 solver.cpp:244]     Train net output #0: loss = 24.4689 (* 1 = 24.4689 loss)
I0318 10:01:32.356037  3501 sgd_solver.cpp:106] Iteration 20740, lr = 0.01
I0318 10:01:35.653903  3501 solver.cpp:337] Iteration 20750, Testing net (#0)
I0318 10:03:29.840189  3501 solver.cpp:404]     Test net output #0: loss = 593.099 (* 1 = 593.099 loss)
I0318 10:03:29.840354  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903799 (* 1 = 0.903799 loss)
I0318 10:03:33.788800  3501 solver.cpp:228] Iteration 20760, loss = 42.5897
I0318 10:03:33.788867  3501 solver.cpp:244]     Train net output #0: loss = 42.5896 (* 1 = 42.5896 loss)
I0318 10:03:33.788882  3501 sgd_solver.cpp:106] Iteration 20760, lr = 0.01
I0318 10:03:41.064229  3501 solver.cpp:228] Iteration 20780, loss = 27.2566
I0318 10:03:41.064316  3501 solver.cpp:244]     Train net output #0: loss = 27.2565 (* 1 = 27.2565 loss)
I0318 10:03:41.064332  3501 sgd_solver.cpp:106] Iteration 20780, lr = 0.01
I0318 10:03:48.378759  3501 solver.cpp:228] Iteration 20800, loss = 80.2146
I0318 10:03:48.378837  3501 solver.cpp:244]     Train net output #0: loss = 80.2144 (* 1 = 80.2144 loss)
I0318 10:03:48.378852  3501 sgd_solver.cpp:106] Iteration 20800, lr = 0.01
I0318 10:03:55.709729  3501 solver.cpp:228] Iteration 20820, loss = 40.7538
I0318 10:03:55.709797  3501 solver.cpp:244]     Train net output #0: loss = 40.7537 (* 1 = 40.7537 loss)
I0318 10:03:55.709810  3501 sgd_solver.cpp:106] Iteration 20820, lr = 0.01
I0318 10:04:03.044872  3501 solver.cpp:228] Iteration 20840, loss = 28.8061
I0318 10:04:03.044997  3501 solver.cpp:244]     Train net output #0: loss = 28.8059 (* 1 = 28.8059 loss)
I0318 10:04:03.045011  3501 sgd_solver.cpp:106] Iteration 20840, lr = 0.01
I0318 10:04:10.383410  3501 solver.cpp:228] Iteration 20860, loss = 59.3176
I0318 10:04:10.383476  3501 solver.cpp:244]     Train net output #0: loss = 59.3175 (* 1 = 59.3175 loss)
I0318 10:04:10.383491  3501 sgd_solver.cpp:106] Iteration 20860, lr = 0.01
I0318 10:04:17.722790  3501 solver.cpp:228] Iteration 20880, loss = 18.477
I0318 10:04:17.722856  3501 solver.cpp:244]     Train net output #0: loss = 18.4768 (* 1 = 18.4768 loss)
I0318 10:04:17.722869  3501 sgd_solver.cpp:106] Iteration 20880, lr = 0.01
I0318 10:04:25.068109  3501 solver.cpp:228] Iteration 20900, loss = 17.9895
I0318 10:04:25.068173  3501 solver.cpp:244]     Train net output #0: loss = 17.9893 (* 1 = 17.9893 loss)
I0318 10:04:25.068187  3501 sgd_solver.cpp:106] Iteration 20900, lr = 0.01
I0318 10:04:32.412307  3501 solver.cpp:228] Iteration 20920, loss = 28.6803
I0318 10:04:32.412376  3501 solver.cpp:244]     Train net output #0: loss = 28.6802 (* 1 = 28.6802 loss)
I0318 10:04:32.412391  3501 sgd_solver.cpp:106] Iteration 20920, lr = 0.01
I0318 10:04:39.755219  3501 solver.cpp:228] Iteration 20940, loss = 19.2991
I0318 10:04:39.755360  3501 solver.cpp:244]     Train net output #0: loss = 19.2989 (* 1 = 19.2989 loss)
I0318 10:04:39.755374  3501 sgd_solver.cpp:106] Iteration 20940, lr = 0.01
I0318 10:04:47.101763  3501 solver.cpp:228] Iteration 20960, loss = 27.9103
I0318 10:04:47.101830  3501 solver.cpp:244]     Train net output #0: loss = 27.9102 (* 1 = 27.9102 loss)
I0318 10:04:47.101843  3501 sgd_solver.cpp:106] Iteration 20960, lr = 0.01
I0318 10:04:54.445125  3501 solver.cpp:228] Iteration 20980, loss = 41.3004
I0318 10:04:54.445190  3501 solver.cpp:244]     Train net output #0: loss = 41.3003 (* 1 = 41.3003 loss)
I0318 10:04:54.445204  3501 sgd_solver.cpp:106] Iteration 20980, lr = 0.01
I0318 10:05:01.413063  3501 solver.cpp:337] Iteration 21000, Testing net (#0)
I0318 10:06:55.636893  3501 solver.cpp:404]     Test net output #0: loss = 602.706 (* 1 = 602.706 loss)
I0318 10:06:55.637009  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903879 (* 1 = 0.903879 loss)
I0318 10:06:55.973675  3501 solver.cpp:228] Iteration 21000, loss = 37.3106
I0318 10:06:55.973742  3501 solver.cpp:244]     Train net output #0: loss = 37.3104 (* 1 = 37.3104 loss)
I0318 10:06:55.973757  3501 sgd_solver.cpp:106] Iteration 21000, lr = 0.01
I0318 10:07:03.228171  3501 solver.cpp:228] Iteration 21020, loss = 52.9578
I0318 10:07:03.228235  3501 solver.cpp:244]     Train net output #0: loss = 52.9576 (* 1 = 52.9576 loss)
I0318 10:07:03.228248  3501 sgd_solver.cpp:106] Iteration 21020, lr = 0.01
I0318 10:07:10.528975  3501 solver.cpp:228] Iteration 21040, loss = 93.1481
I0318 10:07:10.529042  3501 solver.cpp:244]     Train net output #0: loss = 93.148 (* 1 = 93.148 loss)
I0318 10:07:10.529057  3501 sgd_solver.cpp:106] Iteration 21040, lr = 0.01
I0318 10:07:17.844471  3501 solver.cpp:228] Iteration 21060, loss = 44.7649
I0318 10:07:17.844537  3501 solver.cpp:244]     Train net output #0: loss = 44.7647 (* 1 = 44.7647 loss)
I0318 10:07:17.844552  3501 sgd_solver.cpp:106] Iteration 21060, lr = 0.01
I0318 10:07:25.170130  3501 solver.cpp:228] Iteration 21080, loss = 25.9082
I0318 10:07:25.170197  3501 solver.cpp:244]     Train net output #0: loss = 25.908 (* 1 = 25.908 loss)
I0318 10:07:25.170210  3501 sgd_solver.cpp:106] Iteration 21080, lr = 0.01
I0318 10:07:32.504127  3501 solver.cpp:228] Iteration 21100, loss = 29.6568
I0318 10:07:32.504312  3501 solver.cpp:244]     Train net output #0: loss = 29.6567 (* 1 = 29.6567 loss)
I0318 10:07:32.504328  3501 sgd_solver.cpp:106] Iteration 21100, lr = 0.01
I0318 10:07:39.843683  3501 solver.cpp:228] Iteration 21120, loss = 19.6521
I0318 10:07:39.843750  3501 solver.cpp:244]     Train net output #0: loss = 19.652 (* 1 = 19.652 loss)
I0318 10:07:39.843765  3501 sgd_solver.cpp:106] Iteration 21120, lr = 0.01
I0318 10:07:47.187707  3501 solver.cpp:228] Iteration 21140, loss = 23.8026
I0318 10:07:47.187775  3501 solver.cpp:244]     Train net output #0: loss = 23.8024 (* 1 = 23.8024 loss)
I0318 10:07:47.187790  3501 sgd_solver.cpp:106] Iteration 21140, lr = 0.01
I0318 10:07:54.532280  3501 solver.cpp:228] Iteration 21160, loss = 22.934
I0318 10:07:54.532359  3501 solver.cpp:244]     Train net output #0: loss = 22.9338 (* 1 = 22.9338 loss)
I0318 10:07:54.532374  3501 sgd_solver.cpp:106] Iteration 21160, lr = 0.01
I0318 10:08:01.870071  3501 solver.cpp:228] Iteration 21180, loss = 48.4035
I0318 10:08:01.870157  3501 solver.cpp:244]     Train net output #0: loss = 48.4033 (* 1 = 48.4033 loss)
I0318 10:08:01.870178  3501 sgd_solver.cpp:106] Iteration 21180, lr = 0.01
I0318 10:08:09.209316  3501 solver.cpp:228] Iteration 21200, loss = 61.3736
I0318 10:08:09.209457  3501 solver.cpp:244]     Train net output #0: loss = 61.3735 (* 1 = 61.3735 loss)
I0318 10:08:09.209481  3501 sgd_solver.cpp:106] Iteration 21200, lr = 0.01
I0318 10:08:16.546138  3501 solver.cpp:228] Iteration 21220, loss = 50.7332
I0318 10:08:16.546211  3501 solver.cpp:244]     Train net output #0: loss = 50.7331 (* 1 = 50.7331 loss)
I0318 10:08:16.546226  3501 sgd_solver.cpp:106] Iteration 21220, lr = 0.01
I0318 10:08:23.882699  3501 solver.cpp:228] Iteration 21240, loss = 49.7745
I0318 10:08:23.882766  3501 solver.cpp:244]     Train net output #0: loss = 49.7743 (* 1 = 49.7743 loss)
I0318 10:08:23.882781  3501 sgd_solver.cpp:106] Iteration 21240, lr = 0.01
I0318 10:08:27.184545  3501 solver.cpp:337] Iteration 21250, Testing net (#0)
I0318 10:10:21.409040  3501 solver.cpp:404]     Test net output #0: loss = 658.436 (* 1 = 658.436 loss)
I0318 10:10:21.409159  3501 solver.cpp:404]     Test net output #1: siamese_accuracy = 0.903839 (* 1 = 0.903839 loss)
I0318 10:10:25.359076  3501 solver.cpp:228] Iteration 21260, loss = 31.1818
I0318 10:10:25.359143  3501 solver.cpp:244]     Train net output #0: loss = 31.1816 (* 1 = 31.1816 loss)
I0318 10:10:25.359156  3501 sgd_solver.cpp:106] Iteration 21260, lr = 0.01
I0318 10:10:32.643985  3501 solver.cpp:228] Iteration 21280, loss = 12.6247
I0318 10:10:32.644052  3501 solver.cpp:244]     Train net output #0: loss = 12.6245 (* 1 = 12.6245 loss)
I0318 10:10:32.644067  3501 sgd_solver.cpp:106] Iteration 21280, lr = 0.01
